{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM on food review datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing library...\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing library...\")\n",
    "#import pandas as pd\n",
    "import pickle\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "with open('../../data/food_review.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    l = list(reader)\n",
    "\n",
    "print(\"Dataset loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_en_samples:  100000\n",
      "num_de_samples:  100000\n",
      "num_en_words:  59392\n",
      "num_de_words:  15253\n",
      "max_en_words_per_sample:  1805\n",
      "max_de_words_per_sample:  38\n"
     ]
    }
   ],
   "source": [
    "n_samples =100000\n",
    "\n",
    "# running time calculation\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# encoder and decoder\n",
    "# Appending SOS andEOS to target data (decoder)\n",
    "en = [l[i][0] for i in range(1, len(l))][0:n_samples]\n",
    "de = ['SOS_ '+ l[i][1] + ' _EOS' for i in range(1, len(l))][0:n_samples]\n",
    "\n",
    "max_encoder_seq_length = max([len(txt.split(' ')) for txt in en])\n",
    "max_decoder_seq_length = max([len(txt.split(' ')) for txt in de])\n",
    "\n",
    "# Create word dictionaries :\n",
    "en_words=set()\n",
    "for line in en:\n",
    "    for word in line.split():\n",
    "        if word not in en_words:\n",
    "            en_words.add(word)\n",
    "\n",
    "de_words=set()\n",
    "for line in de:\n",
    "    for word in line.split():\n",
    "        if word not in de_words:\n",
    "            de_words.add(word)\n",
    "\n",
    "# get lengths and sizes :\n",
    "num_en_words = len(en_words)\n",
    "num_de_words = len(de_words)\n",
    "\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in en])+5\n",
    "max_de_words_per_sample = max([len(sample.split()) for sample in de])+5\n",
    "\n",
    "num_en_samples = len(en)\n",
    "num_de_samples = len(de)\n",
    "\n",
    "print('num_en_samples: ', num_en_samples)\n",
    "print('num_de_samples: ', num_de_samples)\n",
    "print('num_en_words: ',num_en_words)\n",
    "print('num_de_words: ',num_de_words)\n",
    "print('max_en_words_per_sample: ', max_en_words_per_sample)\n",
    "print('max_de_words_per_sample: ', max_de_words_per_sample)\n",
    "\n",
    "\n",
    "# Tokenize\n",
    "en_tokenizer = Tokenizer(num_words=max_encoder_seq_length, char_level=False)\n",
    "en_tokenizer.fit_on_texts(en)\n",
    "\n",
    "de_tokenizer = Tokenizer(num_words=max_decoder_seq_length, char_level=False)\n",
    "de_tokenizer.fit_on_texts(de)\n",
    "\n",
    "\n",
    "source_token = en_tokenizer.texts_to_sequences(en)\n",
    "target_token = de_tokenizer.texts_to_sequences(de)\n",
    "\n",
    "# padding\n",
    "source_padded = pad_sequences(source_token, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "target_padded = pad_sequences(target_token, maxlen=max_decoder_seq_length, padding = \"post\")\n",
    "#print(len(source_padded[0]))\n",
    "#print(len(target_padded[0]))\n",
    "\n",
    "\n",
    "num_decoder_tokens = num_de_words\n",
    "n_samples = num_en_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "prepare data for the LSTM\n",
    "'''\n",
    "print(\"Preparing data...\")\n",
    "\n",
    "X1, X2, y = list(), list(), list()\n",
    "for i in range(n_samples):\n",
    "    # generate source sequence\n",
    "    target = target_padded[i]\n",
    "    # create padded input target sequence\n",
    "    target_in = np.insert(target[:-1],0,0)\n",
    "    # encode\n",
    "    tar_encoded = utils.to_categorical(target, num_classes=num_decoder_tokens)\n",
    "    # store\n",
    "    X2.append(target_in) #tar2_encoded\n",
    "    y.append(tar_encoded)\n",
    "\n",
    "# X1 = array(X1)\n",
    "# X2 = array(X2)\n",
    "# y = array(y)\n",
    "encoder_input_data = source_padded #np.array(X1)\n",
    "decoder_input_data = np.array(X2)\n",
    "decoder_target_data = np.array(y)\n",
    "\n",
    "print(\"encoder_input_data.shape: \", encoder_input_data.shape)\n",
    "print(\"decoder_input_data.shape: \", decoder_input_data.shape)\n",
    "print(\"decoder_target_data.shape: \", decoder_target_data.shape)\n",
    "\n",
    "# running time check\n",
    "stop = timeit.default_timer()\n",
    "print('Data preparation Runtime: {} s'.format(round(stop - start,2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoding...\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Encoding completed!\n",
      "decoding...\n",
      "Decoding completed!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Embedding\n",
    "'''\n",
    "# Defining some constants:\n",
    "vec_len       = 300 #300   # Length of the vector that we will get from the embedding layer\n",
    "latent_dim    = 1024#1024  # Hidden layers dimension\n",
    "dropout_rate  = 0.5   # Rate of the dropout layers\n",
    "batch_size    = 64 #64    # Batch size\n",
    "epochs        = 100 #30    # Number of epochs\n",
    "\n",
    "###\n",
    "num_en_words = len(en_words)\n",
    "num_de_words = len(de_words)\n",
    "###\n",
    "\n",
    "'''\n",
    "Encoder\n",
    "'''\n",
    "print(\"encoding...\")\n",
    "# Define an input sequence and process it.\n",
    "# Input layer of the encoder :\n",
    "encoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "encoder_embedding = Embedding(input_dim = num_en_words, output_dim = vec_len)(encoder_input)\n",
    "encoder_dropout = (TimeDistributed(Dropout(rate = dropout_rate)))(encoder_embedding)\n",
    "encoder_LSTM = CuDNNLSTM(latent_dim, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# Output layer of the encoder :\n",
    "encoder_LSTM2_layer = CuDNNLSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "print(\"Encoding completed!\")\n",
    "\n",
    "'''\n",
    "Decoder\n",
    "'''\n",
    "print(\"decoding...\")\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# Input layer of the decoder :\n",
    "decoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the decoder :\n",
    "decoder_embedding_layer = Embedding(input_dim = num_de_words, output_dim = vec_len)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "decoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\n",
    "decoder_dropout = decoder_dropout_layer(decoder_embedding)\n",
    "\n",
    "decoder_LSTM_layer = CuDNNLSTM(latent_dim, return_sequences=True)\n",
    "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n",
    "\n",
    "decoder_LSTM_2_layer = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n",
    "\n",
    "# Output layer of the decoder :\n",
    "decoder_dense = Dense(num_de_words, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_LSTM_2)\n",
    "\n",
    "print(\"Decoding completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    5879400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    1373400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          (None, None, 1024)   5431296     time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 300)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 1024), (None 8396800     cu_dnnlstm[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, None, 1024)   5431296     time_distributed_1[0][0]         \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)        [(None, None, 1024), 8396800     cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 4578)   4692450     cu_dnnlstm_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 39,601,442\n",
      "Trainable params: 39,601,442\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Training the model...\n",
      "Train on 9200 samples, validate on 800 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.2912\n",
      "Epoch 00001: val_loss improved from inf to 0.13434, saving model to Weights-001--0.13434.hdf5\n",
      "9200/9200 [==============================] - 463s 50ms/sample - loss: 0.2904 - val_loss: 0.1343\n",
      "Epoch 2/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1366\n",
      "Epoch 00002: val_loss improved from 0.13434 to 0.12871, saving model to Weights-002--0.12871.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1365 - val_loss: 0.1287\n",
      "Epoch 3/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1323\n",
      "Epoch 00003: val_loss improved from 0.12871 to 0.12470, saving model to Weights-003--0.12470.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1324 - val_loss: 0.1247\n",
      "Epoch 4/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1297\n",
      "Epoch 00004: val_loss improved from 0.12470 to 0.12250, saving model to Weights-004--0.12250.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1297 - val_loss: 0.1225\n",
      "Epoch 5/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1276\n",
      "Epoch 00005: val_loss improved from 0.12250 to 0.12092, saving model to Weights-005--0.12092.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1276 - val_loss: 0.1209\n",
      "Epoch 6/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1254\n",
      "Epoch 00006: val_loss improved from 0.12092 to 0.11798, saving model to Weights-006--0.11798.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1255 - val_loss: 0.1180\n",
      "Epoch 7/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1236\n",
      "Epoch 00007: val_loss improved from 0.11798 to 0.11651, saving model to Weights-007--0.11651.hdf5\n",
      "9200/9200 [==============================] - 454s 49ms/sample - loss: 0.1236 - val_loss: 0.1165\n",
      "Epoch 8/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1223\n",
      "Epoch 00008: val_loss improved from 0.11651 to 0.11567, saving model to Weights-008--0.11567.hdf5\n",
      "9200/9200 [==============================] - 454s 49ms/sample - loss: 0.1222 - val_loss: 0.1157\n",
      "Epoch 9/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1211\n",
      "Epoch 00009: val_loss did not improve from 0.11567\n",
      "9200/9200 [==============================] - 426s 46ms/sample - loss: 0.1210 - val_loss: 0.1163\n",
      "Epoch 10/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1199\n",
      "Epoch 00010: val_loss improved from 0.11567 to 0.11385, saving model to Weights-010--0.11385.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1198 - val_loss: 0.1138\n",
      "Epoch 11/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1186\n",
      "Epoch 00011: val_loss improved from 0.11385 to 0.11268, saving model to Weights-011--0.11268.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1186 - val_loss: 0.1127\n",
      "Epoch 12/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1175\n",
      "Epoch 00012: val_loss did not improve from 0.11268\n",
      "9200/9200 [==============================] - 427s 46ms/sample - loss: 0.1174 - val_loss: 0.1129\n",
      "Epoch 13/100\n",
      "9152/9200 [============================>.] - ETA: 2s - loss: 0.1162\n",
      "Epoch 00013: val_loss improved from 0.11268 to 0.11168, saving model to Weights-013--0.11168.hdf5\n",
      "9200/9200 [==============================] - 453s 49ms/sample - loss: 0.1162 - val_loss: 0.1117\n",
      "Epoch 14/100\n",
      "8448/9200 [==========================>...] - ETA: 33s - loss: 0.1147"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f7faa102a2bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.08\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m           callbacks = callbacks_list)\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "Model\n",
    "'''\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define a checkpoint callback :\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "'''\n",
    "Train the Model\n",
    "'''\n",
    "print(\"Training the model...\")\n",
    "#num_train_samples = 100 #9000\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data,\n",
    "               decoder_input_data],\n",
    "               decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.08,\n",
    "          callbacks = callbacks_list)\n",
    "\n",
    "# save model\n",
    "pickle.dump(model, open('seq2seq_model.pkl', 'wb'))\n",
    "\n",
    "print(\"Process completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
