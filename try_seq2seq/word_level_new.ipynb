{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "# import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open('../../data/food_review.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    l = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples =10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder and decoder\n",
    "# Appending SOS andEOS to target data (decoder)\n",
    "enc = [l[i][0] for i in range(1, len(l))] [0:n_samples]\n",
    "dec = ['SOS_ '+ l[i][1] + ' _EOS' for i in range(1, len(l))][0:n_samples]\n",
    "enc_dec = enc + dec\n",
    "\n",
    "num_enc_samples = len(enc)\n",
    "num_dec_samples = len(dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(enc_dec) \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# encoder source data\n",
    "source_token = tokenizer.texts_to_sequences(enc)\n",
    "max_encoder_seq_length = max([len(sentence) for sentence in source_token])\n",
    "source_padded = pad_sequences(source_token, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "# decoder target data\n",
    "target_token = tokenizer.texts_to_sequences(dec)\n",
    "max_decoder_seq_length = max([len(sentence) for sentence in target_token])\n",
    "target_padded = pad_sequences(target_token, maxlen=max_decoder_seq_length, padding = \"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_en_samples:  10000\n",
      "num_de_samples:  10000\n",
      "vocab_size:  22435\n",
      "max_encoder_seq_length:  501\n",
      "max_decoder_seq_length:  28\n"
     ]
    }
   ],
   "source": [
    "print('num_en_samples: ', num_enc_samples)\n",
    "print('num_de_samples: ', num_dec_samples)\n",
    "print('vocab_size: ',vocab_size)\n",
    "print('max_encoder_seq_length: ', max_encoder_seq_length)\n",
    "print('max_decoder_seq_length: ', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 501)\n",
      "(10000, 27)\n",
      "(10000, 27)\n",
      "Time: 0.02 s\n"
     ]
    }
   ],
   "source": [
    "# running time calculation\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "n_samples = num_enc_samples #100\n",
    "\n",
    "# prepare data for the LSTM\n",
    "decoder_input_data, decoder_target_data = list(), list()\n",
    "for i in range(n_samples):\n",
    "    dec_input = target_padded[i][:-1]\n",
    "    target = target_padded[i][1:]     \n",
    "    decoder_input_data.append(dec_input) #tar2_encoded\n",
    "    decoder_target_data.append(target)\n",
    "\n",
    "encoder_input_data = source_padded #np.array(X1)\n",
    "decoder_input_data = np.array(decoder_input_data)\n",
    "decoder_target_data = np.array(decoder_target_data)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} s'.format(round(stop - start,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_17 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_9 (Embedding)         (None, None, 50)     1121750     input_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_18 (InputLayer)           (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_9 (TimeDistrib (None, None, 50)     0           embedding_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, None, 50)     1121750     input_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_18 (CuDNNLSTM)       (None, None, 32)     10752       time_distributed_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_10 (TimeDistri (None, None, 50)     0           embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_19 (CuDNNLSTM)       [(None, 32), (None,  8448        cu_dnnlstm_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_20 (CuDNNLSTM)       (None, None, 32)     10752       time_distributed_10[0][0]        \n",
      "                                                                 cu_dnnlstm_19[0][1]              \n",
      "                                                                 cu_dnnlstm_19[0][2]              \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_21 (CuDNNLSTM)       [(None, None, 32), ( 8448        cu_dnnlstm_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 22435)  740355      cu_dnnlstm_21[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 3,022,255\n",
      "Trainable params: 3,022,255\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Input, Embedding, TimeDistributed,Dropout\n",
    "\n",
    "# Defining some constants: \n",
    "vec_len       = 50#300   # Length of the vector that we willl get from the embedding layer\n",
    "latent_dim    = 32#1024  # Hidden layers dimension \n",
    "dropout_rate  = 0.2   # Rate of the dropout layers\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "# Input layer of the encoder :\n",
    "encoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "encoder_embedding = Embedding(input_dim = vocab_size, output_dim = vec_len)(encoder_input)\n",
    "encoder_dropout   = (TimeDistributed(Dropout(rate = dropout_rate)))(encoder_embedding)\n",
    "encoder_LSTM      = CuDNNLSTM(latent_dim, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# Output layer of the encoder :\n",
    "encoder_LSTM2_layer = CuDNNLSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# Input layer of the decoder :\n",
    "decoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the decoder :\n",
    "decoder_embedding_layer = Embedding(input_dim = vocab_size, output_dim = vec_len)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "decoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\n",
    "decoder_dropout = decoder_dropout_layer(decoder_embedding)\n",
    "\n",
    "decoder_LSTM_layer = CuDNNLSTM(latent_dim, return_sequences=True)\n",
    "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n",
    "\n",
    "decoder_LSTM_2_layer = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n",
    "\n",
    "# Output layer of the decoder :\n",
    "decoder_dense = Dense(vocab_size, activation='linear', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_LSTM_2)\n",
    "\n",
    "\n",
    "# Define encoder model\n",
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Define training model\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "# Define decoder model\n",
    "dec_h = Input(shape=(latent_dim,))\n",
    "dec_c = Input(shape=(latent_dim,))\n",
    "dec_states_inputs = [dec_h, dec_c]\n",
    "decoder_outputs, state_h, state_c = decoder_LSTM_2_layer(decoder_embedding, initial_state=dec_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_input] + dec_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 9200 samples, validate on 800 samples\n",
      "9184/9200 [============================>.] - ETA: 0s - loss: 1.7493\n",
      "Epoch 00001: val_loss improved from inf to 1.21521, saving model to Weights-001--1.21521.hdf5\n",
      "9200/9200 [==============================] - 53s 6ms/sample - loss: 1.7485 - val_loss: 1.2152\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff340555e48>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "learning_rate=0.01    \n",
    "batch_size    = 32#64    # Batch size\n",
    "epochs        = 1#30    # Number of epochs\n",
    "\n",
    "# define loss function: use sparse_softmax_cross_entropy_with_logits\n",
    "def sparse_loss(targets, decoder_outputs):\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=decoder_outputs)\n",
    "\n",
    "# Define a checkpoint callback :\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Run training\n",
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, vocab_size))   \n",
    "\n",
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "                loss=sparse_loss,\n",
    "                target_tensors=[decoder_target])\n",
    "\n",
    "model.fit([encoder_input_data,\n",
    "               decoder_input_data],\n",
    "               decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.08,\n",
    "          callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use sklearn utility to convert label strings to numbered index\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(train_label)\n",
    "# y_train = encoder.transform(train_label)\n",
    "# y_test = encoder.transform(test_label)\n",
    "\n",
    "# # Converts the labels to a one-hot representation\n",
    "# num_classes = np.max(y_train) + 1\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
