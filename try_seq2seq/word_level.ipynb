{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "# import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "with open('../data/food_review.csv') as f:\n",
    "    reader = csv.reader(f)\n",
    "    l = list(reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples =10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder and decoder\n",
    "# Appending SOS andEOS to target data (decoder)\n",
    "en = [l[i][0] for i in range(1, len(l))] [0:n_samples]\n",
    "de = ['SOS_ '+ l[i][1] + ' _EOS' for i in range(1, len(l))][0:n_samples]\n",
    "# en = list()\n",
    "# de = list()\n",
    "# for i in range(1, len(l)):\n",
    "#     en.append(l[i][0])\n",
    "#     de.append('SOS_ ' + l[i][1] + ' _EOS' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "# num_en_words:  19598\n",
    "# num_de_words:  4578\n",
    "# max_en_words_per_sample:  787\n",
    "# max_de_words_per_sample:  37\n",
    "# max([len(txt.split(' ')) for txt in en])\n",
    "\n",
    "max_encoder_seq_length = max([len(txt.split(' ')) for txt in en])\n",
    "max_decoder_seq_length = max([len(txt.split(' ')) for txt in de])\n",
    "print(max_encoder_seq_length)\n",
    "print(max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word dictionaries :\n",
    "en_words=set()\n",
    "for line in en:\n",
    "    for word in line.split():\n",
    "        if word not in en_words:\n",
    "            en_words.add(word)\n",
    "    \n",
    "de_words=set()\n",
    "for line in de:\n",
    "    for word in line.split():\n",
    "        if word not in de_words:\n",
    "            de_words.add(word)\n",
    "            \n",
    "# get lengths and sizes :\n",
    "num_en_words = len(en_words)\n",
    "num_de_words = len(de_words)\n",
    "\n",
    "max_en_words_per_sample = max([len(sample.split()) for sample in en])+5\n",
    "max_de_words_per_sample = max([len(sample.split()) for sample in de])+5\n",
    "\n",
    "num_en_samples = len(en)\n",
    "num_de_samples = len(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_en_samples:  10000\n",
      "num_de_samples:  10000\n",
      "num_en_words:  19598\n",
      "num_de_words:  4578\n",
      "max_en_words_per_sample:  787\n",
      "max_de_words_per_sample:  37\n"
     ]
    }
   ],
   "source": [
    "print('num_en_samples: ', num_en_samples)\n",
    "print('num_de_samples: ', num_de_samples)\n",
    "print('num_en_words: ',num_en_words)\n",
    "print('num_de_words: ',num_de_words)\n",
    "print('max_en_words_per_sample: ', max_en_words_per_sample)\n",
    "print('max_de_words_per_sample: ', max_de_words_per_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get lists of words :\n",
    "# input_words = sorted(list(en_words))\n",
    "# target_words = sorted(list(de_words))\n",
    "\n",
    "# en_token_to_int = dict()\n",
    "# en_int_to_token = dict()\n",
    "\n",
    "# de_token_to_int = dict()\n",
    "# de_int_to_token = dict()\n",
    "\n",
    "# #Tokenizing the words ( Convert them to numbers ) :\n",
    "# for i,token in enumerate(input_words):\n",
    "#     en_token_to_int[token] = i\n",
    "#     en_int_to_token[i]     = token\n",
    "\n",
    "# for i,token in enumerate(target_words):\n",
    "#     de_token_to_int[token] = i\n",
    "#     de_int_to_token[i]     = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = Tokenizer(num_words=max_encoder_seq_length, char_level=False)\n",
    "en_tokenizer.fit_on_texts(en)\n",
    "\n",
    "de_tokenizer = Tokenizer(num_words=max_decoder_seq_length, char_level=False)\n",
    "de_tokenizer.fit_on_texts(de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "65\n"
     ]
    }
   ],
   "source": [
    "source_token = en_tokenizer.texts_to_sequences(en)\n",
    "target_token = de_tokenizer.texts_to_sequences(de)\n",
    "# target_in_token = [0] + target_token[:-1]\n",
    "\n",
    "source_padded = pad_sequences(source_token, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "target_padded = pad_sequences(target_token, maxlen=max_decoder_seq_length, padding = \"post\")\n",
    "print(len(source_padded[0]))\n",
    "print(len(target_padded[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 782)\n",
      "(10000, 65)\n",
      "(10000, 65, 4578)\n",
      "Time: 2.91 s\n"
     ]
    }
   ],
   "source": [
    "# running time calculation\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "# Number of unique input tokens: 84\n",
    "# Number of unique output tokens: 58\n",
    "# num_encoder_tokens = 50\n",
    "num_decoder_tokens = num_de_words #211\n",
    "n_samples = num_en_samples #100\n",
    "# prepare data for the LSTM\n",
    "\n",
    "X1, X2, y = list(), list(), list()\n",
    "for i in range(n_samples):\n",
    "# generate source sequence\n",
    "#     source = source_padded[i] \n",
    "#     print('source',source)\n",
    "    # define target sequence\n",
    "    target = target_padded[i] \n",
    "#     print(target.shape)\n",
    "#     print(type(target))\n",
    "#     print('target',target)\n",
    "    # create padded input target sequence\n",
    "    target_in = np.insert(target[:-1],0,0)\n",
    "#     print(target_in.shape)\n",
    "#     print(type(target_in))\n",
    "#     print('target_in', target_in)\n",
    "    # encode\n",
    "#     src_encoded = utils.to_categorical(source, num_classes=num_encoder_tokens)\n",
    "    tar_encoded = utils.to_categorical(target, num_classes=num_decoder_tokens)\n",
    "#     tar2_encoded = utils.to_categorical(target_in, num_classes=num_decoder_tokens)\n",
    "#     print(src_encoded.shape)\n",
    "#     print(type(src_encoded))\n",
    "#     print('src_encoded',src_encoded)\n",
    "    # store\n",
    "#     X1.append(src_encoded)\n",
    "    X2.append(target_in) #tar2_encoded\n",
    "    y.append(tar_encoded)\n",
    "#     print(array(X1))\n",
    "    \n",
    "# X1 = array(X1)\n",
    "# X2 = array(X2)\n",
    "# y = array(y)\n",
    "encoder_input_data = source_padded #np.array(X1)\n",
    "decoder_input_data = np.array(X2)\n",
    "decoder_target_data = np.array(y)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)\n",
    "\n",
    "# # decode a one hot encoded string\n",
    "# def one_hot_decode(encoded_seq):\n",
    "#     return [argmax(vector) for vector in encoded_seq]\n",
    " \n",
    "\n",
    "# print(X1.shape, X2.shape, y.shape)\n",
    "# print('X1=%s, X2=%s, y=%s' % (one_hot_decode(X1[0]), one_hot_decode(X2[0]), one_hot_decode(y[0])))\n",
    "\n",
    "# running time check\n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} s'.format(round(stop - start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # read file\n",
    "# lines = pd.read_csv('food_review.csv')\n",
    "# # remane col\n",
    "# lines = lines.rename(columns={'text': 'en', 'summary': 'de'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending SOS andEOS to target data\n",
    "\n",
    "# for i in range(lines.shape[0]):\n",
    "#     x = lines.loc[i,'de']\n",
    "#     lines.loc[i,'de']= 'SOS_ '+ x + ' _EOS'\n",
    "\n",
    "# print(lines.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some constants: \n",
    "vec_len       = 50#300   # Length of the vector that we willl get from the embedding layer\n",
    "latent_dim    = 32#1024  # Hidden layers dimension \n",
    "dropout_rate  = 0.2   # Rate of the dropout layers\n",
    "batch_size    = 32#64    # Batch size\n",
    "epochs        = 1#30    # Number of epochs\n",
    "\n",
    "###\n",
    "# num_en_words = 1427\n",
    "# num_de_words = 211\n",
    "num_en_words = len(en_words)\n",
    "num_de_words = len(de_words)\n",
    "###\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "# Input layer of the encoder :\n",
    "encoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "encoder_embedding = Embedding(input_dim = num_en_words, output_dim = vec_len)(encoder_input)\n",
    "encoder_dropout   = (TimeDistributed(Dropout(rate = dropout_rate)))(encoder_embedding)\n",
    "encoder_LSTM      = CuDNNLSTM(latent_dim, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# Output layer of the encoder :\n",
    "encoder_LSTM2_layer = CuDNNLSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "# Input layer of the decoder :\n",
    "decoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the decoder :\n",
    "decoder_embedding_layer = Embedding(input_dim = num_de_words, output_dim = vec_len)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "decoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\n",
    "decoder_dropout = decoder_dropout_layer(decoder_embedding)\n",
    "\n",
    "decoder_LSTM_layer = CuDNNLSTM(latent_dim, return_sequences=True)\n",
    "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n",
    "\n",
    "decoder_LSTM_2_layer = CuDNNLSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n",
    "\n",
    "# Output layer of the decoder :\n",
    "decoder_dense = Dense(num_de_words, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_LSTM_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 50)     979900      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 50)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, None, 50)     228900      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, None, 32)     10752       time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, None, 50)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)        [(None, 32), (None,  8448        cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_4 (CuDNNLSTM)        (None, None, 32)     10752       time_distributed_2[0][0]         \n",
      "                                                                 cu_dnnlstm_3[0][1]               \n",
      "                                                                 cu_dnnlstm_3[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_5 (CuDNNLSTM)        [(None, None, 32), ( 8448        cu_dnnlstm_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 4578)   151074      cu_dnnlstm_5[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,398,274\n",
      "Trainable params: 1,398,274\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 9200 samples, validate on 800 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "9184/9200 [============================>.] - ETA: 0s - loss: 1.7303\n",
      "Epoch 00001: val_loss improved from inf to 0.23110, saving model to Weights-001--0.23110.hdf5\n",
      "9200/9200 [==============================] - 73s 8ms/sample - loss: 1.7276 - val_loss: 0.2311\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe1f1029ac8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Define a checkpoint callback :\n",
    "checkpoint_name = 'Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "\n",
    "\n",
    "num_train_samples = 100 #9000\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data,\n",
    "               decoder_input_data],\n",
    "               decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.08,\n",
    "          callbacks = callbacks_list)\n",
    "\n",
    "# model.fit([encoder_input_data[:num_train_samples,:],\n",
    "#                decoder_input_data[:num_train_samples,:]],\n",
    "#                decoder_target_data[:num_train_samples,:,:],\n",
    "#           batch_size=batch_size,\n",
    "#           epochs=epochs,\n",
    "#           validation_split=0.08,\n",
    "#           callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use sklearn utility to convert label strings to numbered index\n",
    "# encoder = LabelEncoder()\n",
    "# encoder.fit(train_label)\n",
    "# y_train = encoder.transform(train_label)\n",
    "# y_test = encoder.transform(test_label)\n",
    "\n",
    "# # Converts the labels to a one-hot representation\n",
    "# num_classes = np.max(y_train) + 1\n",
    "# y_train = utils.to_categorical(y_train, num_classes)\n",
    "# y_test = utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
