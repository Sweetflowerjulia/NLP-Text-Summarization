{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-bert in /usr/local/lib/python3.5/dist-packages (0.50.0)\n",
      "Requirement already satisfied: Keras in /usr/local/lib/python3.5/dist-packages (from keras-bert) (2.2.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.5/dist-packages (from keras-bert) (1.16.1)\n",
      "Requirement already satisfied: keras-transformer==0.23.0 in /usr/local/lib/python3.5/dist-packages (from keras-bert) (0.23.0)\n",
      "Requirement already satisfied: keras-pos-embd==0.10.0 in /usr/local/lib/python3.5/dist-packages (from keras-bert) (0.10.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.5/dist-packages (from Keras->keras-bert) (1.0.7)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.5/dist-packages (from Keras->keras-bert) (5.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.5/dist-packages (from Keras->keras-bert) (2.9.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.5/dist-packages (from Keras->keras-bert) (1.2.1)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.5/dist-packages (from Keras->keras-bert) (1.0.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from Keras->keras-bert) (1.12.0)\n",
      "Requirement already satisfied: keras-multi-head==0.20.0 in /usr/local/lib/python3.5/dist-packages (from keras-transformer==0.23.0->keras-bert) (0.20.0)\n",
      "Requirement already satisfied: keras-embed-sim==0.4.0 in /usr/local/lib/python3.5/dist-packages (from keras-transformer==0.23.0->keras-bert) (0.4.0)\n",
      "Requirement already satisfied: keras-layer-normalization==0.12.0 in /usr/local/lib/python3.5/dist-packages (from keras-transformer==0.23.0->keras-bert) (0.12.0)\n",
      "Requirement already satisfied: keras-position-wise-feed-forward==0.5.0 in /usr/local/lib/python3.5/dist-packages (from keras-transformer==0.23.0->keras-bert) (0.5.0)\n",
      "Requirement already satisfied: keras-self-attention==0.41.0 in /usr/local/lib/python3.5/dist-packages (from keras-multi-head==0.20.0->keras-transformer==0.23.0->keras-bert) (0.41.0)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-bert\n",
    "# !wget -q https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "# !unzip -o uncased_L-12_H-768_A-12.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "pretrained_path = 'uncased_L-12_H-768_A-12'\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras_bert.backend import keras\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import get_base_dict, get_model, gen_batch_inputs, get_custom_objects\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "\n",
    "seq_len=384 #512-6 #384-12\n",
    "batch_size = 6\n",
    "\n",
    "model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    training=True,\n",
    "    trainable=True,\n",
    "    seq_len=seq_len, \n",
    "    output_layer_num=12,\n",
    ")\n",
    "\n",
    "# Loading takes time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input-Token (InputLayer)        (None, 384)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Segment (InputLayer)      (None, 384)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token (TokenEmbedding [(None, 384, 768), ( 23440896    Input-Token[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Segment (Embedding)   (None, 384, 768)     1536        Input-Segment[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Token-Segment (Add)   (None, 384, 768)     0           Embedding-Token[0][0]            \n",
      "                                                                 Embedding-Segment[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Position (PositionEmb (None, 384, 768)     294912      Embedding-Token-Segment[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Dropout (Dropout)     (None, 384, 768)     0           Embedding-Position[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Embedding-Norm (LayerNormalizat (None, 384, 768)     1536        Embedding-Dropout[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 384, 768)     2362368     Embedding-Norm[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-1-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 384, 768)     0           Embedding-Norm[0][0]             \n",
      "                                                                 Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-1-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-1-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-1-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-1-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-1-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-1-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-1-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-2-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-1-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-2-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-2-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-2-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-2-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-2-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-2-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-2-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-3-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-2-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-3-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-3-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-3-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-3-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-3-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-3-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-3-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-4-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-3-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-4-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-4-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-4-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-4-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-4-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-4-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-4-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-5-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-4-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-5-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-5-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-5-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-5-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-5-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-5-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-5-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-6-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-5-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-6-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-6-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-6-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-6-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-6-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-6-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-6-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-7-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-6-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-7-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-7-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-7-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-7-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-7-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-7-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-7-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-8-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-7-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-8-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-8-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-8-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-8-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-8-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-8-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 384, 768)     2362368     Encoder-8-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-9-MultiHeadSelfAttention[\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 384, 768)     0           Encoder-8-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-MultiHeadSelfAttentio (None, 384, 768)     1536        Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward (FeedForw (None, 384, 768)     4722432     Encoder-9-MultiHeadSelfAttention-\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Dropout ( (None, 384, 768)     0           Encoder-9-FeedForward[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Add (Add) (None, 384, 768)     0           Encoder-9-MultiHeadSelfAttention-\n",
      "                                                                 Encoder-9-FeedForward-Dropout[0][\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-9-FeedForward-Norm (Lay (None, 384, 768)     1536        Encoder-9-FeedForward-Add[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 384, 768)     2362368     Encoder-9-FeedForward-Norm[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 384, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 384, 768)     0           Encoder-9-FeedForward-Norm[0][0] \n",
      "                                                                 Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-MultiHeadSelfAttenti (None, 384, 768)     1536        Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward (FeedFor (None, 384, 768)     4722432     Encoder-10-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Dropout  (None, 384, 768)     0           Encoder-10-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Add (Add (None, 384, 768)     0           Encoder-10-MultiHeadSelfAttention\n",
      "                                                                 Encoder-10-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-10-FeedForward-Norm (La (None, 384, 768)     1536        Encoder-10-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 384, 768)     2362368     Encoder-10-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 384, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 384, 768)     0           Encoder-10-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-MultiHeadSelfAttenti (None, 384, 768)     1536        Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward (FeedFor (None, 384, 768)     4722432     Encoder-11-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Dropout  (None, 384, 768)     0           Encoder-11-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Add (Add (None, 384, 768)     0           Encoder-11-MultiHeadSelfAttention\n",
      "                                                                 Encoder-11-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-11-FeedForward-Norm (La (None, 384, 768)     1536        Encoder-11-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 384, 768)     2362368     Encoder-11-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 384, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 384, 768)     0           Encoder-11-FeedForward-Norm[0][0]\n",
      "                                                                 Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-MultiHeadSelfAttenti (None, 384, 768)     1536        Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward (FeedFor (None, 384, 768)     4722432     Encoder-12-MultiHeadSelfAttention\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Dropout  (None, 384, 768)     0           Encoder-12-FeedForward[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Add (Add (None, 384, 768)     0           Encoder-12-MultiHeadSelfAttention\n",
      "                                                                 Encoder-12-FeedForward-Dropout[0]\n",
      "__________________________________________________________________________________________________\n",
      "Encoder-12-FeedForward-Norm (La (None, 384, 768)     1536        Encoder-12-FeedForward-Add[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "MLM-Dense (Dense)               (None, 384, 768)     590592      Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Norm (LayerNormalization)   (None, 384, 768)     1536        MLM-Dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "Extract (Extract)               (None, 768)          0           Encoder-12-FeedForward-Norm[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "MLM-Sim (EmbeddingSimilarity)   (None, 384, 30522)   30522       MLM-Norm[0][0]                   \n",
      "                                                                 Embedding-Token[0][1]            \n",
      "__________________________________________________________________________________________________\n",
      "Input-Masked (InputLayer)       (None, 384)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "NSP-Dense (Dense)               (None, 768)          590592      Extract[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "MLM (Masked)                    (None, 384, 30522)   0           MLM-Sim[0][0]                    \n",
      "                                                                 Input-Masked[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "NSP (Dense)                     (None, 2)            1538        NSP-Dense[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,008,124\n",
      "Trainable params: 110,008,124\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "import unicodedata\n",
    "\n",
    "# clean vocab\n",
    "token_dict = {} #get_base_dict()\n",
    "\n",
    "with codecs.open(vocab_path, 'rb', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        token_dict[token] = len(token_dict)\n",
    "\n",
    "decoder_dict = {v:k for k, v in token_dict.items()}        \n",
    "token_list = list(token_dict.keys())  # Used for selecting a random word\n",
    "tokenizer = Tokenizer(token_dict)\n",
    "\n",
    "print(len(token_dict))\n",
    "print(len(decoder_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_dict['[PAD]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['switching', 'to', 'a', 'more', 'memory', 'efficient', 'opt', '##imi', '##zer', 'can', 'reduce', 'memory', 'usage', ',', 'but', 'can', 'also', 'affect', 'the', 'results', '.', 'we', 'have', 'not', 'experimented', 'with', 'other', 'opt', '##imi', '##zers', 'for', 'fine', '-', 'tuning']\n"
     ]
    }
   ],
   "source": [
    "# a = 'Switching to a more memory efficient optimizer can reduce memory usage, but can also affect the results. We have not experimented with other optimizers for fine-tuning'\n",
    "# print(tokenizer.tokenize(a)[1:-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset # Generate batches\n",
    "import gzip  \n",
    "\n",
    "dataset = '../../data/reviews_cleaned.json.gz'\n",
    "\n",
    "def create_review_pairs():\n",
    "#     aa=0\n",
    "    with gzip.open(dataset, 'rb') as f:\n",
    "        batch_i=0\n",
    "        review_pairs= []\n",
    "        \n",
    "        for line in f:\n",
    "            data = eval(line)\n",
    "            text = tokenizer.tokenize(data['review'])[1:-1]\n",
    "            summary = tokenizer.tokenize(data['summary'])[1:-1]\n",
    "            review_pairs.append([summary, text])\n",
    "            batch_i+=1\n",
    "            \n",
    "            if batch_i < batch_size:\n",
    "                continue\n",
    "            else:                            \n",
    "                yield {\n",
    "                  \"review_pair\": review_pairs\n",
    "                }\n",
    "\n",
    "                review_pairs= []\n",
    "                batch_i=0\n",
    "\n",
    "#             aa+=1\n",
    "#             if aa >3:\n",
    "#                 break\n",
    "\n",
    "\n",
    "# for pairs in create_review_pairs():\n",
    "#     print(pairs['review_pair'])\n",
    "#     print('---'*10)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def batch_inputs(sentence_pairs,\n",
    "#                     token_dict,\n",
    "#                     token_list,\n",
    "#                     seq_len=seq_len,\n",
    "#                     mask_rate=0.5,\n",
    "#                     train=True):\n",
    "\n",
    "#     \"\"\"Generate a batch of inputs and outputs for training.\n",
    "#     :param sentence_pairs: A list of pairs containing lists of tokens.\n",
    "#     :param token_dict: The dictionary containing special tokens.\n",
    "#     :param token_list: A list containing all tokens.\n",
    "#     :param seq_len: Length of the sequence.\n",
    "#     :param mask_rate: The rate of choosing a token for prediction.\n",
    "#     :return: All the inputs and outputs.\n",
    "#     \"\"\"\n",
    "#     TOKEN_PAD = ''  # Token for padding\n",
    "#     TOKEN_UNK = '[UNK]'  # Token for unknown words\n",
    "#     TOKEN_CLS = '[CLS]'  # Token for classification\n",
    "#     TOKEN_SEP = '[SEP]'  # Token for separation\n",
    "#     TOKEN_MASK = '[MASK]'  # Token for masking\n",
    "\n",
    "#     batch_size = len(sentence_pairs)\n",
    "#     base_dict = get_base_dict()\n",
    "#     unknown_index = token_dict[TOKEN_UNK]\n",
    "#     # Generate sentence swapping mapping\n",
    "#     nsp_outputs = np.zeros((batch_size,))\n",
    "#     mapping = {}\n",
    "\n",
    "#     # Generate MLM\n",
    "#     token_inputs, segment_inputs, masked_inputs = [], [], []\n",
    "#     mlm_outputs = []\n",
    "#     for i in range(batch_size):\n",
    "#         first, second = sentence_pairs[i][0], sentence_pairs[mapping.get(i, i)][1]\n",
    "#         segment_inputs.append(([0] * (len(first) + 2) + [1] * (seq_len - (len(first) + 2)))[:seq_len])\n",
    "#         tokens = [TOKEN_CLS] + first + [TOKEN_SEP] + second + [TOKEN_SEP]\n",
    "#         tokens = tokens[:seq_len]\n",
    "#         tokens += [TOKEN_PAD] * (seq_len - len(tokens))\n",
    "        \n",
    "#         token_input, masked_input, mlm_output = [], [], []\n",
    "#         has_mask = False\n",
    "        \n",
    "#         for token in tokens:\n",
    "#             mlm_output.append(token_dict.get(token, unknown_index))\n",
    "            \n",
    "#             if has_mask: # mask after 'SEP'                \n",
    "#                 if token == TOKEN_SEP or token == TOKEN_PAD:\n",
    "#                     masked_input.append(0)    \n",
    "#                     token_input.append(token_dict.get(token, unknown_index))  \n",
    "#                     has_mask = False  \n",
    "#                 elif train:\n",
    "#                     if np.random.random() < mask_rate:\n",
    "#                         masked_input.append(1)\n",
    "#                         token_input.append(token_dict[TOKEN_MASK])\n",
    "#                     else:\n",
    "#                         masked_input.append(0)\n",
    "#                         token_input.append(token_dict[TOKEN_MASK])\n",
    "#                 else:\n",
    "#                     masked_input.append(1)\n",
    "#                     token_input.append(token_dict[TOKEN_MASK])\n",
    "                    \n",
    "#             else: \n",
    "#                 if token == TOKEN_SEP:\n",
    "#                     has_mask = True\n",
    "                    \n",
    "#                 masked_input.append(0)    \n",
    "#                 token_input.append(token_dict.get(token, unknown_index))    \n",
    "                \n",
    "#         token_inputs.append(token_input)\n",
    "#         masked_inputs.append(masked_input)\n",
    "#         mlm_outputs.append(mlm_output)\n",
    "#     inputs = [np.asarray(x) for x in [token_inputs, segment_inputs, masked_inputs]]\n",
    "#     outputs = [np.asarray(np.expand_dims(x, axis=-1)) for x in [mlm_outputs, nsp_outputs]]\n",
    "#     return inputs, outputs\n",
    "\n",
    "# def _generator(train=True):\n",
    "#     while True:\n",
    "#         for data in create_review_pairs():\n",
    "#             review_pairs = data['review_pair']\n",
    "\n",
    "#             yield batch_inputs(\n",
    "#                 review_pairs,\n",
    "#                 token_dict,\n",
    "#                 token_list,\n",
    "#                 seq_len=seq_len,\n",
    "#                 mask_rate= 0.5, # only used when training\n",
    "#                 train=train\n",
    "#             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_inputs(sentence_pairs,\n",
    "                    token_dict,\n",
    "                    token_list,\n",
    "                    seq_len=seq_len,\n",
    "                    mask_rate=0.5,\n",
    "                    train=True):\n",
    "\n",
    "    \"\"\"Generate a batch of inputs and outputs for training.\n",
    "    :param sentence_pairs: A list of pairs containing lists of tokens.\n",
    "    :param token_dict: The dictionary containing special tokens.\n",
    "    :param token_list: A list containing all tokens.\n",
    "    :param seq_len: Length of the sequence.\n",
    "    :param mask_rate: The rate of choosing a token for prediction.\n",
    "    :return: All the inputs and outputs.\n",
    "    \"\"\"\n",
    "    TOKEN_PAD = ''  # Token for padding\n",
    "    TOKEN_UNK = '[UNK]'  # Token for unknown words\n",
    "    TOKEN_CLS = '[CLS]'  # Token for classification\n",
    "    TOKEN_SEP = '[SEP]'  # Token for separation\n",
    "    TOKEN_MASK = '[MASK]'  # Token for masking\n",
    "\n",
    "    batch_size = len(sentence_pairs)\n",
    "    base_dict = get_base_dict()\n",
    "    unknown_index = token_dict[TOKEN_UNK]\n",
    "    # Generate sentence swapping mapping\n",
    "    nsp_outputs = np.zeros((batch_size,))\n",
    "    mapping = {}\n",
    "\n",
    "    # Generate MLM\n",
    "    token_inputs, segment_inputs, masked_inputs = [], [], []\n",
    "    mlm_outputs = []\n",
    "    for i in range(batch_size):\n",
    "        first, second = sentence_pairs[i][0], sentence_pairs[mapping.get(i, i)][1]\n",
    "        segment_inputs.append(([0] * (len(first) + 2) + [1] * (seq_len - (len(first) + 2)))[:seq_len])\n",
    "        tokens = [TOKEN_CLS] + first + [TOKEN_SEP] + second + [TOKEN_SEP]\n",
    "        tokens = tokens[:seq_len]\n",
    "        tokens += [TOKEN_PAD] * (seq_len - len(tokens))\n",
    "        \n",
    "        token_input, masked_input, mlm_output = [], [], []\n",
    "        has_mask = True\n",
    "        \n",
    "        for token in tokens:\n",
    "            mlm_output.append(token_dict.get(token, unknown_index))\n",
    "            \n",
    "            if has_mask and token != TOKEN_SEP: # mask before 'SEP'                \n",
    "                if token in get_base_dict() or token == '[PAD]':\n",
    "                    masked_input.append(0)    \n",
    "                    token_input.append(token_dict.get(token, unknown_index))  \n",
    " \n",
    "                elif train:\n",
    "                    if np.random.random() < mask_rate:\n",
    "                        masked_input.append(1)\n",
    "                        token_input.append(token_dict[TOKEN_MASK])\n",
    "                    else:\n",
    "                        masked_input.append(0)\n",
    "                        token_input.append(token_dict[TOKEN_MASK])\n",
    "                else:\n",
    "                    masked_input.append(1)\n",
    "                    token_input.append(token_dict[TOKEN_MASK])\n",
    "                    \n",
    "            else: \n",
    "                has_mask = False\n",
    "                    \n",
    "                masked_input.append(0)    \n",
    "                token_input.append(token_dict.get(token, unknown_index))    \n",
    "                \n",
    "        token_inputs.append(token_input)\n",
    "        masked_inputs.append(masked_input)\n",
    "        mlm_outputs.append(mlm_output)\n",
    "    inputs = [np.asarray(x) for x in [token_inputs, segment_inputs, masked_inputs]]\n",
    "    outputs = [np.asarray(np.expand_dims(x, axis=-1)) for x in [mlm_outputs, nsp_outputs]]\n",
    "    return inputs, outputs\n",
    "\n",
    "def _generator(train=True):\n",
    "    while True:\n",
    "        for data in create_review_pairs():\n",
    "            review_pairs = data['review_pair']\n",
    "\n",
    "            yield batch_inputs(\n",
    "                review_pairs,\n",
    "                token_dict,\n",
    "                token_list,\n",
    "                seq_len=seq_len,\n",
    "                mask_rate= 0.3, # only used when training\n",
    "                train=train\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[72,384,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Encoder-10-MultiHeadSelfAttention_4/Encoder-10-MultiHeadSelfAttention-Attention/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_4/MLM_loss/Mean_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-ed6d450895c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     callbacks=[\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     ], #\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[72,384,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node Encoder-10-MultiHeadSelfAttention_4/Encoder-10-MultiHeadSelfAttention-Attention/mul}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_4/MLM_loss/Mean_3}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(\n",
    "    generator=_generator(train=True),\n",
    "    steps_per_epoch=10,\n",
    "    epochs=2,\n",
    "    validation_data=_generator(),\n",
    "    validation_steps=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    ], #\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "model_path = 'fine_tuned_model/keras_bert_%f.h5' % np.random.random()\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load\n",
    "# latest = tf.train.latest_checkpoint(checkpoint_retrain_dir)\n",
    "# print(latest)\n",
    "# model_new = create_model()\n",
    "# model_new.load_weights(latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 3 arrays: [array([[101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-78753645bb7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1147\u001b[0m                              'argument.')\n\u001b[1;32m   1148\u001b[0m         \u001b[0;31m# Validate user data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_standardize_user_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    749\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 2 array(s), but instead got the following list of 3 arrays: [array([[101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ..., 100, 100, 100],\n       [101, 103, 103, ..."
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "for inputs, outputs in _generator(train=False):\n",
    "    predicts = model.predict(inputs)\n",
    "    outputs = list(map(lambda x: np.squeeze(x, axis=-1), outputs))\n",
    "    predicts = list(map(lambda x: np.argmax(x, axis=-1), predicts))\n",
    "    batch_size, seq_len = inputs[-1].shape\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        in_pp = np.where(inputs[0][i] == 102)[0][1] # index of end 'SEP'\n",
    "        out_pp = np.where(inputs[-1][i] == 1)[0] # index of masked\n",
    "\n",
    "        review_text = ' '.join([decoder_dict[inputs[0][i][j]] for j in range(in_pp)])\n",
    "        gold_summary = ' '.join([decoder_dict[outputs[0][i][j]] for j in out_pp]) # print masked\n",
    "        pred_summary = ' '.join([decoder_dict[predicts[0][i][j]] for j in out_pp]) \n",
    "        \n",
    "        print('-REVIEW_TEXT-:\\n', review_text)\n",
    "        print('---'*10)\n",
    "        print('GOLD SUMMARY: ', gold_summary)\n",
    "        print('PRED SUMMARY: ', pred_summary, '\\n')\n",
    "        print('---'*20)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-REVIEW_TEXT-:\n",
      " [CLS] we have many of the old , old issue but the number had depleted there were not enough books to allow us to use them regularly with the additional supply the books will be used more often they ar ##re a good old stand ##by for gospel singing [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  i was disappointed that you would only allow me to purchase 4 when your inventory showed that you had 14 available\n",
      "PRED SUMMARY:  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] i or ##igo ##nally did not get the item i ordered when contact ##ing the company they got back with me quickly it turned out the item was out of stock they sent it once it came in stock this was not a problem for me since i ordered the item way in advance for my daughters birthday i would recommend this selling but ordered ahead of time just inca ##se [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  happy with purchase even though it came a lot later than expected\n",
      "PRED SUMMARY:  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] my daughter wore this every other day for maybe half an hour at most the waistband has completely separated from the elastic after two weeks of this light use [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  this was great until it fell apart two weeks later\n",
      "PRED SUMMARY:  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] unfortunately this skirts elastic is not stretch ##y enough to accommodate the 29 y ##r old range , this is crazy i would say it only stretches through about 1 size i would throw this in a costume play chest with a safety pin handy and call it a day [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  i should have paid more attention to the age range\n",
      "PRED SUMMARY:  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] doubles ##pe ##ak is truly a language unto itself a fair comparison would be the similarities between british english and american english both use the same words which would lead some people to think they were the same language that is until one hears that drivers in england keep their jack ##s in the boot , and take a lift up to their flats ##o does doubles ##pe ##ak use words from english but with a distinct accent that tells you the speaker is not from around here ##some people may not like the tone of the book however when you realize that the author also wrote the cambridge the ##saurus of american english , you will understand why after several decades studying deception in language he used such a sarcastic tone in presenting examples of such ##thi ##s book is essential reading for anyone who wants to understand how sc ##am artists use innocent words for ne ##far ##ious ends perhaps people like bernie mad ##off would have been less successful in running their sc ##ams if more people were aware of and watch ##ful for doubles ##pe ##ak [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  learn to speak doubles ##pe ##ak in under 15 minutes a day\n",
      "PRED SUMMARY:  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] very thorough review of mk ##sa ##p and a great companion to the text book or online , especially if you like to review material in different formats there are a lot references to the tables and figures that require you to go back to the book or the online version , so it is definitely not complete but i like the discussion too many bad jokes , some are border ##line and probably inappropriate , but a few are harmless and amusing a classmate of mine does a great job with one of the sessions [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  very thorough review of mk ##sa ##p and a great companion to the text book or online\n",
      "PRED SUMMARY:  [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] \n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "for inputs, outputs in _generator(train=False):\n",
    "    predicts = model.predict(inputs)\n",
    "    outputs = list(map(lambda x: np.squeeze(x, axis=-1), outputs))\n",
    "    predicts = list(map(lambda x: np.argmax(x, axis=-1), predicts))\n",
    "    batch_size, seq_len = inputs[-1].shape\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        in_pp = np.where(inputs[0][i] == 102)[0][1] # index of end 'SEP'\n",
    "        out_pp = np.where(inputs[-1][i] == 1)[0] # index of masked\n",
    "\n",
    "        review_text = ' '.join([decoder_dict[inputs[0][i][j]] for j in range(in_pp)])\n",
    "        gold_summary = ' '.join([decoder_dict[outputs[0][i][j]] for j in out_pp]) # print masked\n",
    "        pred_summary = ' '.join([decoder_dict[predicts[0][i][j]] for j in out_pp]) \n",
    "        \n",
    "        print('-REVIEW_TEXT-:\\n', review_text)\n",
    "        print('---'*10)\n",
    "        print('GOLD SUMMARY: ', gold_summary)\n",
    "        print('PRED SUMMARY: ', pred_summary, '\\n')\n",
    "        print('---'*20)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-REVIEW_TEXT-:\n",
      " [CLS] we have many of the old , old issue but the number had depleted there were not enough books to allow us to use them regularly with the additional supply the books will be used more often they ar ##re a good old stand ##by for gospel singing [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  i was disappointed that you would only allow me to purchase 4 when your inventory showed that you had 14 available\n",
      "PRED SUMMARY:  the , the the the , , , , , , , , , , , , the the the book \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] i or ##igo ##nally did not get the item i ordered when contact ##ing the company they got back with me quickly it turned out the item was out of stock they sent it once it came in stock this was not a problem for me since i ordered the item way in advance for my daughters birthday i would recommend this selling but ordered ahead of time just inca ##se [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  happy with purchase even though it came a lot later than expected\n",
      "PRED SUMMARY:  i not not i the the the the the the the it \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] my daughter wore this every other day for maybe half an hour at most the waistband has completely separated from the elastic after two weeks of this light use [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  this was great until it fell apart two weeks later\n",
      "PRED SUMMARY:  i i i the the the the the the day \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] unfortunately this skirts elastic is not stretch ##y enough to accommodate the 29 y ##r old range , this is crazy i would say it only stretches through about 1 size i would throw this in a costume play chest with a safety pin handy and call it a day [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  i should have paid more attention to the age range\n",
      "PRED SUMMARY:  i is the the the the the the the it \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] doubles ##pe ##ak is truly a language unto itself a fair comparison would be the similarities between british english and american english both use the same words which would lead some people to think they were the same language that is until one hears that drivers in england keep their jack ##s in the boot , and take a lift up to their flats ##o does doubles ##pe ##ak use words from english but with a distinct accent that tells you the speaker is not from around here ##some people may not like the tone of the book however when you realize that the author also wrote the cambridge the ##saurus of american english , you will understand why after several decades studying deception in language he used such a sarcastic tone in presenting examples of such ##thi ##s book is essential reading for anyone who wants to understand how sc ##am artists use innocent words for ne ##far ##ious ends perhaps people like bernie mad ##off would have been less successful in running their sc ##ams if more people were aware of and watch ##ful for doubles ##pe ##ak [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  learn to speak doubles ##pe ##ak in under 15 minutes a day\n",
      "PRED SUMMARY:  a is the the the the the the the the the book \n",
      "\n",
      "------------------------------------------------------------\n",
      "-REVIEW_TEXT-:\n",
      " [CLS] very thorough review of mk ##sa ##p and a great companion to the text book or online , especially if you like to review material in different formats there are a lot references to the tables and figures that require you to go back to the book or the online version , so it is definitely not complete but i like the discussion too many bad jokes , some are border ##line and probably inappropriate , but a few are harmless and amusing a classmate of mine does a great job with one of the sessions [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK]\n",
      "------------------------------\n",
      "GOLD SUMMARY:  very thorough review of mk ##sa ##p and a great companion to the text book or online\n",
      "PRED SUMMARY:  i good good , , , , , , , , , , the the the book \n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "for inputs, outputs in _generator(train=False):\n",
    "    predicts = model.predict(inputs)\n",
    "    outputs = list(map(lambda x: np.squeeze(x, axis=-1), outputs))\n",
    "    predicts = list(map(lambda x: np.argmax(x, axis=-1), predicts))\n",
    "    batch_size, seq_len = inputs[-1].shape\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "\n",
    "        in_pp = np.where(inputs[0][i] == 102)[0][1] # index of end 'SEP'\n",
    "        out_pp = np.where(inputs[-1][i] == 1)[0] # index of masked\n",
    "\n",
    "        review_text = ' '.join([decoder_dict[inputs[0][i][j]] for j in range(in_pp)])\n",
    "        gold_summary = ' '.join([decoder_dict[outputs[0][i][j]] for j in out_pp]) # print masked\n",
    "        pred_summary = ' '.join([decoder_dict[predicts[0][i][j]] for j in out_pp]) \n",
    "        \n",
    "        print('-REVIEW_TEXT-:\\n', review_text)\n",
    "        print('---'*10)\n",
    "        print('GOLD SUMMARY: ', gold_summary)\n",
    "        print('PRED SUMMARY: ', pred_summary, '\\n')\n",
    "        print('---'*20)\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[['we', 'have', 'many', 'of', 'the', 'old', ',', 'old', 'issue', 'but', 'the', 'number', 'had', 'depleted', 'there', 'were', 'not', 'enough', 'books', 'to', 'allow', 'us', 'to', 'use', 'them', 'regularly', 'with', 'the', 'additional', 'supply', 'the', 'books', 'will', 'be', 'used', 'more', 'often', 'they', 'ar', '##re', 'a', 'good', 'old', 'stand', '##by', 'for', 'gospel', 'singing'], ['i', 'was', 'disappointed', 'that', 'you', 'would', 'only', 'allow', 'me', 'to', 'purchase', '4', 'when', 'your', 'inventory', 'showed', 'that', 'you', 'had', '14', 'available']], [['i', 'or', '##igo', '##nally', 'did', 'not', 'get', 'the', 'item', 'i', 'ordered', 'when', 'contact', '##ing', 'the', 'company', 'they', 'got', 'back', 'with', 'me', 'quickly', 'it', 'turned', 'out', 'the', 'item', 'was', 'out', 'of', 'stock', 'they', 'sent', 'it', 'once', 'it', 'came', 'in', 'stock', 'this', 'was', 'not', 'a', 'problem', 'for', 'me', 'since', 'i', 'ordered', 'the', 'item', 'way', 'in', 'advance', 'for', 'my', 'daughters', 'birthday', 'i', 'would', 'recommend', 'this', 'selling', 'but', 'ordered', 'ahead', 'of', 'time', 'just', 'inca', '##se'], ['happy', 'with', 'purchase', 'even', 'though', 'it', 'came', 'a', 'lot', 'later', 'than', 'expected']], [['my', 'daughter', 'wore', 'this', 'every', 'other', 'day', 'for', 'maybe', 'half', 'an', 'hour', 'at', 'most', 'the', 'waistband', 'has', 'completely', 'separated', 'from', 'the', 'elastic', 'after', 'two', 'weeks', 'of', 'this', 'light', 'use'], ['this', 'was', 'great', 'until', 'it', 'fell', 'apart', 'two', 'weeks', 'later']], [['unfortunately', 'this', 'skirts', 'elastic', 'is', 'not', 'stretch', '##y', 'enough', 'to', 'accommodate', 'the', '29', 'y', '##r', 'old', 'range', ',', 'this', 'is', 'crazy', 'i', 'would', 'say', 'it', 'only', 'stretches', 'through', 'about', '1', 'size', 'i', 'would', 'throw', 'this', 'in', 'a', 'costume', 'play', 'chest', 'with', 'a', 'safety', 'pin', 'handy', 'and', 'call', 'it', 'a', 'day'], ['i', 'should', 'have', 'paid', 'more', 'attention', 'to', 'the', 'age', 'range']], [['doubles', '##pe', '##ak', 'is', 'truly', 'a', 'language', 'unto', 'itself', 'a', 'fair', 'comparison', 'would', 'be', 'the', 'similarities', 'between', 'british', 'english', 'and', 'american', 'english', 'both', 'use', 'the', 'same', 'words', 'which', 'would', 'lead', 'some', 'people', 'to', 'think', 'they', 'were', 'the', 'same', 'language', 'that', 'is', 'until', 'one', 'hears', 'that', 'drivers', 'in', 'england', 'keep', 'their', 'jack', '##s', 'in', 'the', 'boot', ',', 'and', 'take', 'a', 'lift', 'up', 'to', 'their', 'flats', '##o', 'does', 'doubles', '##pe', '##ak', 'use', 'words', 'from', 'english', 'but', 'with', 'a', 'distinct', 'accent', 'that', 'tells', 'you', 'the', 'speaker', 'is', 'not', 'from', 'around', 'here', '##some', 'people', 'may', 'not', 'like', 'the', 'tone', 'of', 'the', 'book', 'however', 'when', 'you', 'realize', 'that', 'the', 'author', 'also', 'wrote', 'the', 'cambridge', 'the', '##saurus', 'of', 'american', 'english', ',', 'you', 'will', 'understand', 'why', 'after', 'several', 'decades', 'studying', 'deception', 'in', 'language', 'he', 'used', 'such', 'a', 'sarcastic', 'tone', 'in', 'presenting', 'examples', 'of', 'such', '##thi', '##s', 'book', 'is', 'essential', 'reading', 'for', 'anyone', 'who', 'wants', 'to', 'understand', 'how', 'sc', '##am', 'artists', 'use', 'innocent', 'words', 'for', 'ne', '##far', '##ious', 'ends', 'perhaps', 'people', 'like', 'bernie', 'mad', '##off', 'would', 'have', 'been', 'less', 'successful', 'in', 'running', 'their', 'sc', '##ams', 'if', 'more', 'people', 'were', 'aware', 'of', 'and', 'watch', '##ful', 'for', 'doubles', '##pe', '##ak'], ['learn', 'to', 'speak', 'doubles', '##pe', '##ak', 'in', 'under', '15', 'minutes', 'a', 'day']], [['very', 'thorough', 'review', 'of', 'mk', '##sa', '##p', 'and', 'a', 'great', 'companion', 'to', 'the', 'text', 'book', 'or', 'online', ',', 'especially', 'if', 'you', 'like', 'to', 'review', 'material', 'in', 'different', 'formats', 'there', 'are', 'a', 'lot', 'references', 'to', 'the', 'tables', 'and', 'figures', 'that', 'require', 'you', 'to', 'go', 'back', 'to', 'the', 'book', 'or', 'the', 'online', 'version', ',', 'so', 'it', 'is', 'definitely', 'not', 'complete', 'but', 'i', 'like', 'the', 'discussion', 'too', 'many', 'bad', 'jokes', ',', 'some', 'are', 'border', '##line', 'and', 'probably', 'inappropriate', ',', 'but', 'a', 'few', 'are', 'harmless', 'and', 'amusing', 'a', 'classmate', 'of', 'mine', 'does', 'a', 'great', 'job', 'with', 'one', 'of', 'the', 'sessions'], ['very', 'thorough', 'review', 'of', 'mk', '##sa', '##p', 'and', 'a', 'great', 'companion', 'to', 'the', 'text', 'book', 'or', 'online']]]\n",
      "------------------------------\n",
      "[[['product', 'was', 'just', 'as', 'described', 'no', 'issues', 'quick', 'shipment', 'not', 'much', 'more', 'to', 'say', 'a'], ['product', 'was', 'just', 'as', 'described', 'no', 'issues', 'quick', 'shipment', 'not', 'much', 'more', 'to', 'say', 'a']], [['i', 'first', 'read', 'the', 'prophet', 'in', 'college', 'back', 'in', 'the', '60s', 'the', 'book', 'had', 'a', 'revival', 'as', 'did', 'anything', 'metaphysical', 'in', 'the', 'turbulent', '60s', 'it', 'had', 'a', 'profound', 'effect', 'on', 'me', 'and', 'became', 'a', 'book', 'i', 'always', 'took', 'with', 'me', 'after', 'graduation', 'i', 'joined', 'the', 'peace', 'corps', 'and', 'during', 'stress', '##ful', 'training', 'in', 'country', 'liberia', 'at', 'times', 'of', 'illness', 'and', 'the', 'night', 'before', 'i', 'left', ',', 'this', 'book', 'gave', 'me', 'great', 'comfort', 'i', 'read', 'it', 'before', 'i', 'married', ',', 'just', 'before', 'and', 'again', 'after', 'my', 'children', 'were', 'born', 'and', 'again', 'after', 'two', 'near', 'fatal', 'illnesses', 'i', 'am', 'always', 'amazed', 'that', 'there', 'is', 'a', 'chapter', 'that', 'reaches', 'out', 'to', 'you', ',', 'grabs', 'you', 'and', 'offers', 'both', 'comfort', 'and', 'hope', 'for', 'the', 'future', '##gi', '##bra', '##n', 'offers', 'timeless', 'insights', 'and', 'love', 'with', 'each', 'word', 'i', 'think', 'that', 'we', 'as', 'a', 'nation', 'should', 'read', 'and', 'learn', 'the', 'lessons', 'here', 'it', 'is', 'definitely', 'a', 'time', 'for', 'thought', 'and', 'reflection', 'this', 'book', 'could', 'guide', 'us', 'through'], ['timeless', 'for', 'every', 'good', 'and', 'bad', 'time', 'in', 'your', 'life']], [['i', 'have', 'always', 'loved', 'the', 'prophet', 'and', 'i', 'have', 'read', 'it', 'many', 'times', 'i', 'gave', 'away', 'my', 'last', 'copy', 'and', 'was', 'looking', 'for', 'a', 'replacement', 'i', 'purchased', 'this', 'copy', 'only', 'to', 'find', 'that', 'it', 'was', 'a', 'tiny', 'novelty', 'version', 'of', 'the', 'book', 'if', 'you', 'are', 'looking', 'for', 'something', 'tiny', 'to', 'keep', 'in', 'your', 'purse', 'then', 'this', 'is', 'for', 'you', 'if', 'you', 'are', 'looking', 'for', 'a', 'real', 'book', 'then', 'this', 'is', 'a', 'joke'], ['if', 'you', 'are', 'looking', 'for', 'a', 'tiny', 'printed', 'novelty', 'you', 'will', 'love', 'this']], [['this', 'book', 'red', '##ef', '##ines', 'poetry', 'and', 're', 'introduces', 'a', 'long', 'forgotten', 'era', 'where', 'people', 'once', 'spoke', 'with', 'meaning', 'and', 'passion', 'instead', 'of', 'politics', ',', 'money', 'and', 'regulations', ',', 'the', 'prophet', 'reminds', 'you', 'of', 'wisdom', 'we', 'as', 'people', 'once', 'had', 'embraced', 'but', 'now', 'over', 'look', 'and', 'under', 'estimate', 'the', 'value', 'of', 'what', 'real', 'knowledge', 'is', 'worth', 'this', 'book', 'gives', 'u', 'all', 'of', 'this', 'in', 'less', 'than', '100', 'pages', 'i', 'am', 'not', 'getting', 'paid', 'to', 'write', 'this', 'review', 'so', 'this', 'is', 'a', 'real', 'review', 'from', 'a', 'real', 'person', 'and', 'not', 'a', 'salesman', 'if', 'this', 'was', 'helpful', 'then', 'send', 'me', '5', 'bucks', 'and', 'we', 'will', 'call', 'it', 'even', 'pay', '##pal', 'accepted'], ['this', 'is', 'a', 'really', 'really', 'good', 'book', '5', 'stars', 'all', 'the', 'way', 'and', 'ship', '##per', 'sent', 'item', 'as', 'described']], [['this', 'powerful', 'work', 'has', 'been', 'with', 'me', 'most', 'of', 'my', 'life', ',', 'cal', '##ib', '##rating', 'my', 'thinking', 'and', 'perspective', 'of', 'everyday', 'life', 'i', 'rate', 'it', 'along', 'side', 'of', 'ki', '##pling', '##s', 'poem', '34', '##if', '##34', ',', 'an', 'ideal', 'template', 'for', 'man', '##hood'], ['after', 'the', 'holy', 'scriptures', 'this', 'is', 'next', 'on', 'my', 'list']], [['i', 'used', 'to', 'have', 'the', 'one', 'pictured', ',', 'the', 'one', 'i', 'got', 'was', 'different', 'artist', 'it', 'was', 'for', 'reference', 'use', 'so', 'i', 'can', 'live', 'with', 'it'], ['the', 'album', 'cover', 'shown', 'is', 'from', 'a', 'different', 'artist', ',', 'than', 'the', 'one', 'sent']]]\n",
      "------------------------------\n",
      "[[['_', 'the', 'prophet', '_', 'is', 'a', 'short', 'read', 'my', 'copy', 'checks', 'in', 'at', 'just', 'under', '100', 'pages', ',', 'but', 'its', 'be', '##re', '##vity', 'bel', '##ies', 'both', 'the', 'power', 'and', 'beauty', 'of', 'gi', '##bra', '##ns', 'words', 'at', 'its', 'simplest', ',', 'it', 'is', 'a', 'discourse', 'on', 'the', 'human', 'condition', 'love', ',', 'work', ',', 'joy', 'and', 'sorrow', ',', 'crime', 'and', 'punishment', ',', 'reason', 'and', 'passion', ',', 'gi', '##bra', '##n', 'runs', 'the', 'ga', '##mut', 'of', 'emotion', 'and', 'being', ',', 'laying', 'bare', 'the', 'paradox', 'of', 'who', 'we', 'are', 'as', 'human', 'beings', 'while', 'the', 'tone', 'is', 'somewhat', 'mystical', 'which', 'i', 'did', 'not', 'really', 'care', 'for', ',', 'the', 'sheer', 'poetic', 'beauty', 'of', 'his', 'writing', 'moved', 'me', '##for', 'example', ',', 'in', 'the', 'chapter', 'love', ',', 'gi', '##bra', '##n', 'writes', ',', 'when', 'love', 'beck', '##ons', 'to', 'you', ',', 'follow', 'him', ',', 'though', 'his', 'ways', 'are', 'hard', 'and', 'steep', ',', 'and', 'when', 'his', 'wings', 'en', '##fold', 'you', 'yield', 'to', 'him', ',', 'though', 'the', 'sword', 'hidden', 'among', 'his', 'pin', '##ions', 'may', 'wound', 'you', 'and', 'when', 'he', 'speaks', 'to', 'you', 'believe', 'in', 'him', ',', 'for', 'even', 'as', 'love', 'crowns', 'you', 'so', 'shall', 'he', 'cr', '##uc', '##ify', 'you', 'even', 'as', 'he', 'is', 'for', 'you', 'growth', 'so', 'is', 'he', 'for', 'your', 'pr', '##uni', '##ng', 'the', 'contrast', ',', 'vivid', 'metaphor', 'and', 'beautiful', 'images', 'were', 'stunning', 'and', 'left', 'me', 'with', 'much', 'to', 'think', 'and', 'reflect', 'on', 'about', 'my', 'own', 'life', ',', 'and', 'the', 'choices', 'i', 'have', 'made', 'it', 'would', 'be', 'going', 'too', 'far', 'to', 'say', 'that', ',', 'as', 'a', 'result', 'of', 'reading', 'the', 'prophet', 'i', 'have', 'had', 'an', 'ep', '##ip', '##han', '##y', 'or', 'to', 'take', 'it', 'to', 'a', 'ridiculous', 'conclusion', 'some', 'sort', 'of', 'a', 'conversion', 'rather', 'it', 'has', 'caused', 'me', 'to', 'consider', 'on', 'a', 'philosophical', 'level', 'what', 'it', 'is', 'to', 'be', 'human', 'means', 'to', 'me', ',', 'and', 'how', 'i', 'have', 'demonstrated', 'my', 'human', '##ness', 'in', 'my', 'life', '##par', '##t', 'poetry', ',', 'part', 'philosophy', ',', 'it', 'is', 'simultaneously', 'thought', '##pro', '##voking', 'and', 'emotional', 'und', '##enia', '##bly', 'readers', 'will', 'have', 'a', 'vis', '##cera', '##l', 'reaction', 'although', ',', 'apparently', 'given', 'the', 'reviews', 'of', 'some', ',', 'not', 'all', 'reactions', 'are', 'positive', 'highly', 'recommended', ',', 'if', 'only', 'to', 'cause', 'one', 'to', 'examine', 'their', 'own', 'life', 'after', 'all', ',', 'the', 'une', '##xa', '##mined', 'life', 'is', 'not', 'worth', 'living'], ['a', 'beautiful', 'poetic', 'commentary', 'on', 'what', 'it', 'is', 'to', 'be', 'human']], [['writing', 'this', 'review', 'due', 'to', 'how', 'interesting', 'the', 'book', 'was', '##sol', '##ita', '##ry', 'life', 'of', 'a', 'profit', 'going', 'from', 'place', 'to', 'place', 'educating', 'the', 'world', 'on', 'what', 'he', '##s', 'learned', 'and', 'needed', 'to', 'tell', 'them', 'up', 'till', 'i', 'read', 'this', 'book', 'i', 'had', 'an', 'entirely', 'different', 'image', 'of', 'what', 'i', 'profit', 'even', 'is', '##i', 'like', 'the', 'perspective', 'this', 'book', 'gave', 'mei', 'also', 'very', 'much', 'enjoyed', 'the', 'flow', 'of', 'the', 'story', 'well', 'worth', 'the', 'read'], ['this', 'is', 'one', 'of', '4', 'books', 'i', 'recently', 'read', 'that', 'i', 'loved']], [['the', 'cover', 'was', 'good', 'and', 'sturdy', 'as', 'well', 'as', 'the', 'binding', 'price', 'was', 'very', 'reasonable', 'given', 'that', 'it', 'is', 'an', 'old', 'book', ',', 'the', 'pages', 'were', 'in', 'very', 'good', 'condition', ',', 'no', 'tears', 'and', 'folds', 'delivery', 'was', 'prompt', '##i', 'am', 'glad', 'to', 'have', 'made', 'this', 'purchase'], ['item', 'as', 'described', ',', 'for', 'it', 'is', 'age', 'it', 'is', 'in', 'good', 'condition']], [['the', 'language', 'is', 'poetic', ',', 'but', 'it', 'is', 'not', 'poetry', 'it', 'works', 'as', 'prose', 'poetry', ',', 'in', 'fact', 'some', 'of', 'the', 'best', 'in', 'english', 'that', 'is', 'probably', 'because', 'gi', '##bra', '##n', 'somewhat', 'mimic', '##s', 'the', 'poetry', 'of', 'the', 'king', 'james', 'version', 'some', 'phrases', 'and', 'turns', 'and', 'images', 'are', 'brilliant', '##tha', '##t', 'said', ',', 'what', 'is', 'the', 'point', 'of', 'this', 'book', 'there', 'are', 'some', 'nice', 'things', 'is', 'not', 'religion', 'all', 'deeds', 'and', 'all', 'reflections', 'who', 'can', 'separate', 'his', 'faith', 'from', 'his', 'actions', 'who', 'can', 'spread', 'his', 'hours', 'before', 'him', ',', 'saying', ',', 'this', 'is', 'for', 'god', 'and', 'this', 'is', 'for', 'myself', 'this', 'for', 'my', 'soul', ',', 'and', 'this', 'other', 'for', 'my', 'body', '##good', 'point', 'there', 'are', 'several', 'such', 'nu', '##gg', '##ets', 'but', ',', 'you', 'can', 'tell', 'why', 'the', 'hip', '##pies', '##et', 'loved', 'this', 'book', 'it', 'is', 'spirituality', 'devoid', 'of', 'religion', 'namely', ',', 'this', 'book', 'can', 'make', 'you', 'feel', 'all', 'cosmic', 'without', 'all', 'that', 'pe', '##sky', 'christian', 'morality', 'that', 'bit', 'quoted', 'above', ',', 'it', 'is', 'followed', 'by', 'he', 'who', 'wears', 'his', 'morality', 'but', 'as', 'his', 'best', 'garment', 'were', 'better', 'naked', '##ah', 'see', 'what', 'morals', 'get', 'you', 'on', 'crime', 'and', 'punishment', ',', 'we', 'get', 'this', 'it', 'takes', 'a', 'village', 'gem', 'and', 'as', 'a', 'single', 'leaf', 'turns', 'not', 'yellow', 'but', 'with', 'the', 'silent', 'knowledge', 'of', 'the', 'whole', 'tree', ',', 'so', 'the', 'wrong', '##do', '##er', 'cannot', 'do', 'wrong', 'without', 'the', 'hidden', 'will', 'of', 'you', 'all', 'see', ',', 'crime', 'is', 'not', 'the', 'fault', 'of', 'the', 'criminal', ',', 'it', 'is', 'the', 'fault', 'of', 'society', 'the', 'righteous', 'is', 'not', 'innocent', 'of', 'the', 'deeds', 'of', 'the', 'wicked', 'ye', '##a', ',', 'the', 'guilty', 'is', 'often', '##time', '##s', 'the', 'victim', 'of', 'the', 'injured', 'see', ',', 'if', 'you', 'self', '##right', '##eous', 'rich', 'folk', 'did', 'not', 'op', '##press', 'the', 'meek', 'poor', 'folk', ',', 'they', 'would', 'not', 'have', 'to', 'commit', 'crime', '##i', 'wonder', 'how', 'the', 'injured', 'rape', 'victim', 'victim', '##ized', 'her', 'guilty', 'rap', '##ist', '##gen', '##eral', '##ly', ',', 'the', 'twin', 'cr', '##edo', '##s', 'of', 'the', 'prophet', 'are', '1', 'any', 'path', 'to', 'god', 'is', 'the', 'right', 'path', ',', 'and', '2', 'if', 'it', 'feels', 'good', 'do', 'it', '##1', 'thus', ',', 'the', 'plural', '##istic', 're', '##lat', '##ivism', 'of', 'lines', 'likes', '##ay', 'not', ',', 'i', 'have', 'found', 'the', 'truth', ',', 'but', 'rather', ',', 'i', 'have', 'found', 'a', 'truth', 'say', 'not', ',', 'i', 'have', 'found', 'the', 'path', 'of', 'the', 'soul', 'say', 'rather', ',', 'i', 'have', 'met', 'the', 'soul', 'walking', 'upon', 'my', 'path', '##for', '##get', 'that', 'jesus', 'said', ',', 'i', 'am', 'the', 'way', ',', 'the', 'truth', ',', 'and', 'the', 'life', 'no', 'man', 'comet', '##h', 'unto', 'the', 'father', ',', 'but', 'by', 'me', 'john', '146', ',', 'by', 'the', 'way', 'no', ',', 'no', ',', 'no', 'many', 'will', '##fully', 'ignore', 'that', 'jesus', 'said', 'such', 'une', '##cum', '##eni', '##cal', 'things', 'like', 'this', 'the', 'hip', '##pie', '##je', '##sus', 'thinks', 'that', 'there', 'are', 'many', 'paths', 'to', 'god', 'the', 'whole', 'one', 'source', ',', 'many', 'wells', 'philosophy', 'do', 'not', 'believe', 'me', 'why', 'do', 'you', 'think', 'the', 'folks', 'at', 'the', 'jesus', 'seminar', 'black', '##ball', '##ed', 'this', 'very', 'verse', 'john', '146', 'our', 'great', ',', 'loving', ',', 'hip', '##pie', '##je', '##sus', 'believes', 'that', 'everyone', 'comes', 'to', 'god', 'in', 'his', 'own', 'way', 'he', 'would', 'never', 'say', 'that', 'black', '##ball', 'that', 'last', 'quota', '##tion', 'was', 'not', 'an', 'actual', 'quote', ',', 'by', 'the', 'way', '##2', 'and', ',', 'though', 'the', 'nature', 'of', 'the', 'good', 'and', 'evil', 'chapter', 'is', 'tempered', 'by', 'a', 'sort', '##of', 'love', 'thy', 'neighbor', 'philosophy', ',', 'it', 'still', 'defines', 'good', 'not', 'as', 'following', 'ju', '##dae', '##och', '##rist', '##ian', 'morals', ',', 'or', 'even', 'a', 'do', 'unto', 'others', 'prescription', ',', 'but', 'you', 'are', 'good', 'when', 'you', 'are', 'one', 'with', 'yourself', '##if', 'it', 'feels', 'good', 'to', 'you', ',', 'do', 'it', 'sure', ',', 'gi', '##bra', '##n', 'later', 'seems', 'to', 'warn', 'against', 'un', '##bri', '##dled', 'pleasures', '##ee', '##king', ',', 'pleasure', 'is', 'a', 'freedoms', '##ong', ',', 'but', 'it', 'is', 'not', 'freedom', 'it', 'is', 'the', 'blossom', '##ing', 'of', 'your', 'desires', ',', 'but', 'it', 'is', 'not', 'their', 'fruit', 'later', ',', 'however', ',', 'such', 'he', '##don', '##ism', 'is', 'considered', 'a', 'path', 'a', 'truth', 'to', 'gods', '##ome', 'of', 'your', 'youth', 'seek', 'pleasure', 'as', 'if', 'it', 'were', 'all', ',', 'and', 'they', 'are', 'judged', 'and', 're', '##bu', '##ked', 'i', 'would', 'not', 'judge', 'or', 're', '##bu', '##ke', 'them', 'i', 'would', 'have', 'them', 'seek', '##thus', 'the', 'paradox', 'of', 'liberalism', 'the', 'prophet', 're', '##bu', '##kes', 'you', 'for', 're', '##bu', '##king', 'those', 'he', '##don', '##s', 'amongst', 'you', 'they', 'are', 'only', 'on', 'their', 'path', 'you', 'are', 'so', 'wrong', 'to', 'tell', 'them', 'that', 'they', 'are', 'wrong', 'ah', ',', 're', '##lat', '##ivism', 'the', 'problem', 'with', 'un', '##che', '##cked', 'he', '##don', '##ism', 'you', 'are', 'good', 'when', 'you', 'are', 'one', 'with', 'yourself', ',', 'if', 'you', 'prefer', 'is', 'that', 'mankind', 'always', 'finds', 'some', 'inner', 'reasoning', 'to', 'justify', 'their', 'evil', 'actions', 'man', 'always', 'finds', 'a', 'way', 'to', 'make', 'pleasure', '##ata', '##ny', '##cos', '##t', ',', 'devoid', 'of', 'anything', 'else', ',', 'his', 'god', '##i', 'do', 'not', 'want', 'to', 'seem', 'as', 'if', 'i', 'am', 'some', 'stick', 'in', 'the', 'mud', ',', 'a', 'pr', '##ude', ',', 'or', 'an', 'over', '##moral', '##izing', 'and', 'self', '##right', '##eous', 'christian', 'christian', 'i', 'am', ',', 'and', 'he', '##don', 'i', 'sometimes', 'am', 'i', 'can', 'get', 'wheat', 'from', 'the', 'cha', '##ff', 'out', 'of', 'the', 'prophet', 'there', 'is', 'much', 'here', 'that', 'is', 'good', ',', 'and', 'some', 'of', 'it', 'is', 'art', '##fully', 'written', 'some', 'of', 'gi', '##bra', '##ns', 'thoughts', 'are', 'even', 'profound', 'still', ',', 'there', 'is', 'much', 'here', 'that', 'is', 'bad', ',', 'there', 'is', 'still', 'some', 'shu', '##ck', 'on', 'the', 'ear', ',', 'so', 'to', 'speak', 'sometimes', 'you', 'must', 'take', 'the', 'wrong', 'path', 'which', 'leads', 'you', 'to', 'the', 'right', 'path', ',', 'but', 'there', 'is', 'just', 'one', 'right', 'path', ',', 'not', 'several', 'gi', '##bra', '##n', 'never', 'seems', 'to', 'say', 'that', ',', 'and', ',', 'since', 'he', 'was', 'a', 'lebanese', 'christian', ',', 'i', 'find', 'that', 'sad', '##ther', '##e', 'is', 'much', 'here', 'to', 'lead', 'you', 'as', '##tray', '##i', 'will', 'not', 'hold', 'it', 'against', 'gi', '##bra', '##n', 'that', 'the', 'prophet', 'has', 'inspired', 'numerous', 'imitation', '##s', 'of', 'lesser', '##po', '##etic', 'talent', ',', 'sad', 'pu', '##r', '##vey', '##ances', 'of', 'even', 'fewer', 'truths', 'yes', ',', 'i', 'am', 'speaking', 'of', 'amongst', 'others', 'the', 'tri', '##te', ',', 'ina', '##ne', 'ca', '##co', '##phon', '##ies', 'of', 'paulo', 'coe', '##lho', '##and', ',', 'for', 'those', 'reviewers', 'and', 'readers', 'who', 'think', 'that', 'the', 'prophet', 'is', 'the', 'most', 'poetic', ',', 'most', 'en', '##light', '##ening', ',', 'most', 'cosmic', '##ally', 'spiritual', 'tome', 'of', 'all', 'time', 'try', 'reading', 'the', 'psalms', 'in', 'the', 'k', '##j', '##v', 'with', 'an', 'open', 'mind', '##yes', 'i', 'invented', 'the', 'word', 'he', '##don', 'he', '##don', '##ist', 'sounds', 'like', 'a', 'philosophical', 'school', ',', 'as', 'does', 'epic', '##ure', '##an', 'the', 'latter', ',', 'too', ',', 'is', 'for', 'an', 'ae', '##st', '##het', '##e', 'let', 'us', 'say', 'a', 'he', '##don', 'is', 'a', 'pleasures', '##ee', '##ker', 'without', 'the', 'philosophy', 'a', 'he', '##don', 'is', 'an', 'un', '##sop', '##his', '##tica', '##te', 'i', 'probably', 'made', 'up', 'that', 'word', 'too'], ['unfortunately', ',', 'the', 'twin', 'cr', '##edo', '##s', 'of', 'the', 'prophet', 'are', 'any', 'path', 'to', 'god', 'is', 'the', 'right', 'path', 'and', 'if', 'it', 'feels', 'good', 'do', 'it']], [['this', 'book', 'is', 'almost', 'as', 'if', 'ka', '##hli', '##l', 'gi', '##bra', '##n', 'took', 'the', 'pains', 'and', 'joy', '##s', 'of', 'millions', 'of', 'people', 'and', 'somehow', 'managed', 'to', 'en', '##cap', '##sul', '##ate', 'all', 'these', 'wonders', 'in', 'this', 'one', 'book', 'in', 'his', 'own', 'unique', 'manner', ',', 'gi', '##bra', '##n', 'has', 'explored', 'every', 'face', '##t', 'of', 'human', 'existence', 'although', 'the', 'book', 'has', 'a', 'sacred', 'charm', 'to', 'it', ',', 'there', 'is', 'a', 'universal', 'appeal', 'that', 'should', 'allow', 'anyone', 'of', 'any', 'religious', 'per', '##su', '##ass', '##ion', 'enjoy', 'this', 'gem', 'of', 'a', 'book', 'i', 'would', 'not', 'be', 'surprised', 'if', 'a', 'few', 'hundred', 'years', 'from', 'now', 'some', 'theologians', 'une', '##arth', 'this', 'book', 'and', 'conclude', 'that', 'it', 'was', 'a', 'bible', 'of', 'our', 'times'], ['a', 'few', 'hundred', 'years', 'from', 'now', 'this', 'book', 'will', 'be', 'a', 'bible']], [['this', 'is', 'a', 'classic', 'of', 'world', 'literature', 'everyone', 'with', 'a', 'poets', 'heart', ',', 'a', 'lovers', 'heart', ',', 'a', 'philosophers', 'heart', ',', 'a', 'dancers', 'heart', ',', 'a', 'parents', 'heart', ',', 'a', 'leaders', 'heart', ',', 'or', 'simply', 'who', 'enjoys', 'beauty', 'found', 'in', 'life', '##s', 'joy', '##s', 'and', 'sorrow', '##s', ',', 'this', 'book', 'is', 'simply', 'the', 'best'], ['a', 'must', 'read', 'for', 'every', 'young', 'person', ',', 'a', 'joy', 'and', 'delight', 'for', 'every', 'mature', 'one', 'more', 'truth', 'per', 'page', 'than', '999', '##9', '##of', 'what', 'is', 'out', 'there']]]\n",
      "------------------------------\n",
      "[[['what', 'is', 'there', 'not', 'to', 'love', 'about', 'the', 'writing', 'of', 'k', '##hal', '##il', 'gi', '##bra', '##n', 'i', 'finally', 'took', 'the', 'time', ',', 'and', 'not', 'much', 'is', 'needed', ',', 'to', 'read', 'the', 'prophet', '##ever', '##y', 'page', 'flows', 'seam', '##lessly', 'from', 'one', 'to', 'the', 'other', 'and', 'his', 'narrative', 'writing', 'style', 'is', 'infused', 'with', 'lessons', 'of', 'love', ',', 'life', ',', 'and', 'more', '##thi', '##s', 'is', 'a', 'book', 'that', 'i', 'will', 'read', 'time', 'and', 'time', 'again', 'like', 'many', 'other', 'of', 'the', 'great', '##s', 'i', 'keep', 'on', 'my', 'book', 'shelf'], ['i', 'finally', 'read', 'it', 'and', 'it', 'was', 'worth', 'the', 'wait']], [['really', 'criticism', 'of', 'this', 'work', 'is', 'impossible', 'the', 'prophet', 'is', 'shear', 'genius', 'i', 'first', 'read', 'it', 'when', 'i', 'was', 'maybe', '12', 'and', 'my', 'dad', 'just', 'had', 'it', 'laying', 'around', 'i', 'am', 'sure', 'i', 'did', 'not', '34', '##get', 'it', '##34', 'at', 'that', 'age', 'but', 'something', 'about', 'it', 'res', '##onate', '##d', 'with', 'me'], ['really', 'criticism', 'of', 'this', 'work', 'is', 'impossible', 'the', 'prophet', 'is', 'shear', 'genius', 'i', 'first', 'read', 'it', 'when', 'i', 'was', 'maybe', '12', 'and', 'my', 'dad', 'had', 'it']], [['such', 'a', 'classic', 'book', 'deep', 'wisdom', 'in', 'a', 'very', 'read', '##able', 'container', 'the', 'prophet', 'has', 'always', 'been', 'a', 'favorite', ',', 'and', 'i', 'wanted', 'to', 'give', 'a', 'copy', 'to', 'my', 'brother', 'the', 'book', 'is', 'great', 'for', 'just', 'picking', 'up', 'and', 'reading', 'a', 'page', 'or', 'two', 'i', 'had', 'the', 'opportunity', 'to', 'read', 'a', 'passage', 'at', 'my', 'wedding', 'ceremony'], ['such', 'a', 'classic', 'book', 'deep', 'wisdom', 'in', 'a', 'very', 'read', '##able', 'container']], [['i', 'have', 'never', 'read', 'any', 'other', 'book', 'that', 'has', 'repeatedly', 'made', 'me', 'smile', 'throughout', 'the', 'entire', 'literary', 'journey', 'each', 'chapter', 'addresses', 'a', 'common', 'human', 'experience', 'that', 'i', 'think', 'we', 'can', 'all', 'relate', 'to', 'on', 'some', 'level', 'i', 'will', 'never', 'get', 'tired', 'of', 're', '##rea', '##ding', 'this'], ['if', 'i', 'had', 'the', 'funds', ',', 'i', 'would', 'buy', 'this', 'for', 'all', 'of', 'my', 'loved', 'ones']], [['the', 'kind', 'of', 'book', 'you', 'can', 'read', 'over', 'and', 'over', 'again', 'in', 'it', 'is', 'entirety', 'or', 'choose', 'an', 'individual', 'lesson', 'from', 'love', 'to', 'friendship', 'to', 'children', ',', 'work', 'and', 'everything', 'in', 'between', ',', 'gi', '##bra', '##n', 'has', 'a', 'way', 'showing', 'you', 'how', 'to', 'think', 'about', 'the', 'things', 'we', 'deal', 'with', 'everyday', 'one', 'will', 'become', 'a', 'better', 'more', 'en', '##light', '##ened', 'person', 'through', 'the', 'teachings', 'of', '34th', '##e', 'prophet', '##34'], ['a', 'must', 'read', 'for', 'anyone', 'with', 'any', 'interest', 'in', 'understanding', 'their', 'world']], [['i', 'am', 'almost', '51', 'years', 'old', 'when', 'i', 'was', '18', 'and', 'deeply', 'conflict', '##ed', ',', 'weighted', 'down', 'with', 'the', 'confusion', '##s', 'that', 'can', 'only', 'be', 'so', 'imp', '##oss', '##ibly', 'painful', 'at', 'that', 'young', 'age', ',', 'my', 'mother', 'gave', 'me', 'a', 'hard', '##back', 'copy', 'of', 'the', 'prophet', 'i', 'have', 'carried', 'it', 'with', 'through', 'all', 'these', 'years', 'as', 'a', 'treasure', '##d', 'companion', 'it', 'helped', 'me', 'make', 'sense', 'of', 'the', 'nonsense', 'my', 'life', 'was', 'back', 'there', 'in', '1974', ',', 'and', 'has', 'comfort', '##ed', 'me', 'more', 'then', 'a', 'couple', 'times', 'in', 'the', 'years', 'since', 'then', ',', 'as', 'well', '##to', '##day', 'i', 'am', 'here', 'at', 'amazon', 'to', 'buy', 'the', 'same', 'book', 'for', 'my', '19', '##year', '##old', 'daughter', 'she', 'is', 'a', 'bright', ',', 'warm', ',', 'funny', ',', 'sweet', 'girl', 'who', 'is', 'undergoing', 'no', 'small', 'amount', 'of', 'ang', '##st', 'as', 'her', 'com', '##fy', 'girl', '##hood', 'world', 'slips', 'away', 'her', 'friends', 'have', 'begun', 'to', 'sc', '##atter', 'to', 'the', 'winds', '##some', 'to', 'jobs', 'and', 'some', 'to', 'college', '##and', 'her', 'own', 'adult', 'responsibilities', 'lo', '##om', 'ever', 'taller', '##w', '##hy', ',', 'she', 'has', 'asked', 'me', ',', 'why', 'do', 'i', 'feel', 'so', 'sad', 'i', 'have', 'no', 'reason', 'to', 'be', 'so', 'sad', '##bu', '##t', 'i', 'am', 'why', '##and', 'my', 'mind', 'has', 'flashed', 'toy', '##our', 'joy', 'is', 'your', 'sorrow', 'un', '##mas', '##ked', '##and', 'the', 'self', '##sam', '##e', 'well', 'from', 'which', 'your', 'laughter', 'rises', 'was', 'often', '##time', '##s', 'filled', 'with', 'your', 'tears', '##and', 'how', 'else', 'can', 'it', 'be', '##so', 'i', 'want', 'this', 'book', 'for', 'my', 'wonderful', 'daughter', 'i', 'think', 'she', 'is', 'ready', 'for', 'it', 'and', ',', 'better', ',', 'i', 'think', 'it', 'will', 'help', 'the', 'turmoil', 'in', 'her', 'soul', 'as', 'it', 'did', 'for', 'me', 'when', 'i', 'was', 'her', 'age', ',', 'and', 'for', 'my', 'mother', 'before', 'me'], ['from', 'my', 'mother', 'to', 'me', ',', 'and', 'now', 'to', 'my', 'daughter', 'words', 'that', 'help']]]\n",
      "------------------------------\n",
      "[[['i', 'love', 'love', 'love', 'love', 'love', 'this', 'book', 'i', 'received', 'it', 'with', 'in', 'a', 'week', 'of', 'ordering', 'in', 'awesome', 'condition', 'i', 'am', 'now', 'sending', 'it', 'to', 'a', 'loved', 'one', 'in', 'canada', 'as', 'a', 'graduation', 'gift', 'thank', 'you', 'send', '##er'], ['if', 'you', 'have', 'not', 'read', 'the', 'prophet', ',', 'but', 'it', 'on', 'amazon', 'and', 'read', 'it']], [['seldom', 'has', 'there', 'been', 'a', 'book', 'that', 'i', 'have', 'read', 'time', 'and', 'time', 'again', 'as', 'much', 'as', 'this', 'one', 'i', 'have', 'given', 'this', 'book', 'numerous', 'times', 'to', 'people', 'as', 'a', 'small', 'gift', 'i', 'have', 'used', 'this', 'book', 'in', 'my', 'sons', 'christ', '##ening', 'when', 'he', 'was', 'a', 'baby', 'and', 'on', 'numerous', 'holidays', 'these', 'words', 'are', 'every', 'bit', 'as', 'relevant', 'when', 'they', 'were', 'written', 'for', 'all', 'time', ',', 'this', 'book', 'still', 'ranks', 'in', 'my', 'top', '5'], ['one', 'of', 'the', 'most', 'classic', ',', 'profound', ',', 'timeless', ',', 'el', '##o', '##quent', 'pieces', 'of', 'literature', 'of', 'all', 'time']], [['gi', '##bra', '##n', 'may', 'be', 'a', 'poet', ',', 'rather', 'than', 'a', 'true', 'prophet', 'but', 'the', 'simple', 'truths', 'in', 'this', 'book', 'seem', 'applicable', 'to', 'all', 'religions', 'the', 'short', 'passages', 'on', 'love', ',', 'children', ',', 'pain', ',', 'beauty', ',', 'death', 'and', 'other', 'timeless', 'topics', 'are', 'full', 'of', 'insight', 'and', 'inspiration', ',', 'but', 'completely', 'lacking', 'in', 'the', 'politics', 'and', 'self', '##right', '##eous', '##ness', 'that', 'per', '##va', '##de', 'so', 'many', 'religions', 'this', 'book', 'is', 'goodness', ',', 'plain', 'and', 'simple', ',', 'and', 'it', 'changed', 'my', 'life', 'more', 'than', 'any', 'other', 'book', 'i', 'can', 'recall', 'reading'], ['prophet', 'you', 'will', 'come', 'back', 'to', 'its', 'wisdom', 'again', 'and', 'again']], [['you', 'can', 'get', 'them', 'online', 'for', '15', 'not', '200', '##it', 'even', 'has', 'a', 'place', 'in', 'the', 'back', 'for', 'you', 'to', 'track', 'the', 'people', 'you', 'have', 'talked', 'to', 'and', 'follow', 'them', 'up'], ['if', 'you', 'are', 'a', 'real', 'christian', ',', 'you', 'have', 'to', 'have', 'one']], [['written', 'as', 'a', 'play', 'for', 'voices', 'for', 'the', 'bbc', ',', 'this', 'historic', 'audio', '##ta', '##pe', 'features', 'the', 'all', '##we', '##ls', '##h', 'cast', 'of', 'the', 'original', 'bbc', 'production', 'from', '1954', 'richard', 'burton', 'is', 'the', 'first', 'voice', ',', 'which', 'connects', 'all', 'the', 'characters', ',', 'played', 'by', 'twenty', '##ei', '##ght', 'men', ',', 'women', ',', 'and', 'children', 'with', 'perfect', 'di', '##ction', 'and', 'the', 'sense', 'of', 'character', 'which', 'only', 'a', 'great', 'actor', 'can', 'convey', ',', 'burton', 'rolls', 'his', 'rs', ',', 'mod', '##ulates', 'his', 'voice', 'in', 'pitch', 'and', 'intensity', ',', 'and', 'makes', 'thomas', '##s', 'poetry', 'come', 'fully', 'alive', '##ful', '##l', 'of', 'all', '##iter', '##ation', 'and', 'various', 'kinds', 'of', 'rhyme', ',', 'with', 'nouns', 'and', 'adjective', '##s', 'used', 'as', 'verbs', 'to', 'convey', 'action', 'and', 'sense', 'impressions', 'simultaneously', ',', 'and', 'always', 'a', 'wry', 'humor', 'and', 'honesty', 'of', 'feeling', '##de', '##pic', '##ting', 'one', 'full', 'day', 'in', 'the', 'life', 'of', 'a', 'small', 'town', 'in', 'wales', ',', 'thomas', 'shows', 'its', 'mo', '##tley', 'residents', 'as', 'they', 'awake', '##n', ',', 'perform', 'their', 'daily', 'tasks', ',', 'social', '##ize', ',', 'gossip', ',', 'and', 'day', '##dre', '##am', 'about', 'the', 'past', 'that', 'might', 'have', 'been', 'and', 'the', 'future', 'that', 'may', 'yet', 'hold', 'hope', 'when', 'night', 'falls', 'and', 'the', 'residents', 'retire', ',', 'their', 'losses', 'and', 'disappointment', '##s', ',', 'along', 'with', 'their', 'escapes', 'into', 'dreams', ',', 'are', 'given', 'voice', 'and', 'po', '##ign', '##ancy', 'polly', 'ga', '##rter', ',', 'with', 'her', 'numerous', 'children', 'by', 'numerous', 'fathers', ',', 'dreams', 'of', 'willie', ',', 'a', 'very', 'small', 'man', 'who', 'was', 'the', 'love', 'of', 'her', 'life', 'captain', 'cat', ',', 'the', 'blind', 'bell', '##ring', '##er', ',', 'thinks', 'of', 'all', 'the', 'sailors', 'he', 'knew', 'who', 'died', 'at', 'sea', 'mr', 'pu', '##gh', 'dreams', 'of', 'poisoning', 'his', 'wife', ',', 'and', 'young', 'gwen', '##ny', ',', 'who', 'has', 'ex', '##tort', '##ed', 'penn', '##ies', 'from', 'the', 'little', 'boys', 'who', 'do', 'not', 'want', 'to', 'kiss', 'her', ',', 'plans', 'for', 'the', 'next', 'day', 'and', 'more', 'penn', '##iest', '##he', 'sound', 'effects', 'provide', 'context', 'for', 'the', 'drama', 'without', 'over', '##powering', 'the', 'narrative', '##a', 'cock', '##s', 'crow', ',', 'the', 'clip', '##cl', '##op', 'of', 'horses', ',', 'the', 'bark', 'of', 'dogs', ',', 'footsteps', ',', 'the', 'sea', ',', 'bell', 'bu', '##oys', '##and', 'simple', 'songs', 'add', 'to', 'the', 'realism', 'and', 'the', 'sense', 'of', 'character', 'and', 'place', 'a', 'mo', '##urn', '##ful', 'tune', 'performed', 'by', 'polly', 'ga', '##rter', 'in', 'a', 'minor', 'key', ',', 'as', 'she', 'remembers', 'willie', 'and', 'compares', 'him', 'to', 'her', 'other', 'lovers', ',', 'is', 'beautifully', 'sung', 'by', 'diana', 'maddox', ',', 'her', 'clear', ',', 'bell', '##like', 'voice', 'and', 'almost', 'pal', '##pa', '##ble', 'sadness', 'making', 'her', 'one', 'of', 'the', 'most', 'memorable', 'of', 'the', 'characters', 'a', 'humorous', 'children', '##s', 'singing', 'game', ',', 'sung', 'by', 'local', 'school', 'children', ',', 'gives', 'added', 'realism', ',', 'and', 'little', 'gwen', '##ny', '##s', 'song', 'to', 'three', 'very', 'young', 'boys', 'is', 'delightful', '##ly', 'cheek', '##y', 'both', 'en', '##chan', '##ting', 'and', 'historically', 'important', ',', 'this', 'memorable', 'recording', 'is', 'worth', 'seeking', 'through', 'used', 'sites', 'or', 'through', 'amazon', '##co', '##uk', '##the', 'best', 'recording', 'ever', 'made', 'of', 'this', 'wonderful', 'play', 'for', 'voices', 'mary', 'whip', '##ple'], ['original', 'recording', 'from', '1954', 'with', 'richard', 'burton', 'and', 'all', '##we', '##ls', '##h', 'cast']], [['this', 'is', 'a', 'review', 'for', 'both', 'the', 'hard', 'cover', 'book', 'and', 'the', 'audio', 'case', '##tte', '##i', 'read', 'this', '10', 'years', 'ago', ',', 'as', 'the', 'second', 'book', 'following', 'the', 'za', '##hn', 'trilogy', 'by', 'comparison', 'it', 'was', 'natural', 'to', 'feel', 'disappointed', 'but', 'over', 'the', 'years', ',', 'listening', 'to', 'it', 'several', 'times', 'on', 'audio', 'cassette', 'it', 'has', 'grown', 'on', 'me', 'the', 'question', 'you', 'are', 'asking', ',', 'with', 'so', 'many', 'sw', 'books', 'to', 'select', 'from', 'should', 'you', 'read', 'this', 'one', 'yes', '##lu', '##ke', 'falls', 'in', 'love', 'the', 'characters', 'here', 'appear', 'in', 'the', 'core', '##lli', '##an', 'trilogy', 'baku', '##ra', 'and', 'biotechnology', 'used', 'by', 'the', 'ss', '##i', 'ru', '##uk', 'is', 'revisited', 'in', 'the', 'ill', 'intention', '##ed', 'nj', '##o', 'series', 'though', 'it', 'did', 'not', 'seem', 'to', 'fit', 'when', 'i', 'first', 'read', 'the', 'book', ',', 'the', 'story', 'here', 'does', 'fit', 'nicely', 'with', 'the', 'entire', 'expanded', 'universe', '##on', 'the', 'timeline', 'i', 'have', 'this', 'one', 'at', 'year', '45', 'an', '##h', 'the', 'story', 'takes', 'place', 'right', 'after', 'the', 'battle', 'at', 'end', '##or', 'in', 'rot', '##j', 'ben', 'appears', 'and', 'tells', 'luke', 'to', 'go', 'to', 'baku', '##ra', 'luke', ',', 'at', 'age', '20', 'is', 'in', 'command', 'and', 'can', 'only', 'assemble', 'a', 'small', 'force', 'after', 'the', 'devastation', 'of', 'the', 'battle', 'they', 'have', 'just', 'fought', 'naturally', 'the', 'falcon', ',', 'han', ',', 'lei', '##a', 'and', 'chew', '##y', 'are', 'available', 'to', 'assist', '##the', 'point', 'of', 'the', 'plot', 'is', 'for', 'the', 'rebels', 'to', 'help', 'the', 'imperial', 'garrison', 'on', 'baku', '##ra', 'defend', 'against', 'an', 'invasion', 'that', 'is', 'coming', 'from', 'the', 'unknown', 'regions', 'the', 'invaders', 'are', 'large', 'rep', '##ti', '##lian', 'creatures', 'that', ',', 'when', 'they', 'speak', 'sound', 'like', 'flutes', 'playing', 'they', 'use', 'en', '##tech', '##ment', 'to', 'take', 'the', 'essence', 'of', 'sent', '##ient', 'beings', 'and', 'place', 'that', 'essence', 'inside', 'their', 'machines', 'to', 'run', 'those', 'machines', '##thi', '##s', 'is', 'a', 'book', 'i', 'recommend', 'reading', 'if', 'you', 'are', 'now', 'deciding', 'to', 'read', 'the', 'stories', 'written', 'for', 'the', 'classic', 'era', 'which', 'starts', 'with', 'the', 'han', 'solo', 'trilogy', 'and', 'ends', 'with', 'the', 'za', '##hn', 'duo', '##logy', 'and', 'the', 'dark', 'horse', 'comic', 'union', 'fortunately', ',', 'at', 'this', 'writing', ',', 'amazon', '##com', 'had', 'both', 'new', 'and', 'used', 'audio', 'cassette', '##s', 'and', 'paperback', '##s', 'for', 'sale', '##aud', '##io', 'case', '##tte', 'this', 'is', 'an', 'ab', '##ridge', '##d', 'version', 'that', 'can', 'be', 'listened', 'to', 'in', 'about', '2', 'hours', 'anthony', 'heal', '##d', 'is', 'the', 'reader', 'and', 'he', 'is', 'the', 'best', 'i', 'recommend', 'all', 'the', 'ban', '##tam', 'audio', 'books', 'for', 'star', 'wars', 'they', 'do', 'a', 'great', 'job', 'presenting', 'the', 'books', 'with', 'music', 'and', 'sound', 'effects'], ['luke', 'you', 'must', 'go', 'to', 'baku', '##ra', 'need', 'we', 'read', 'this', 'book', 'tho']]]\n",
      "------------------------------\n",
      "[[['in', 'the', 'second', 'half', 'of', 'the', 'book', ',', 'the', 'characters', 'relocate', 'to', 'silicon', 'valley', 'they', 'make', 'many', 'comments', 'about', 'it', 'sadly', ',', 'i', 'was', 'unable', 'to', 'listen', 'to', 'this', 'because', 'matthew', 'perry', 'consistently', 'says', 'silicon', '##e', 'valley', 'instead', ',', 'which', 'is', 'something', 'else', 'entirely', 'as', 'a', 'consequence', ',', 'i', 'abandoned', 'the', 'book'], ['like', 'computer', 'software', 'one', 'bug', 'in', 'this', 'recording', 'spoiled', 'the', 'whole', 'thing']], [['this', 'was', 'to', 'be', 'quite', 'frank', 'the', 'most', 'de', '##pressing', 'autobiography', 'i', 'have', 'ever', 'read', 'we', 'read', 'of', 'a', 'so', '##rdi', '##d', 'life', 'full', 'of', 'self', '##lo', '##athing', 'and', 'bum', '##bling', 'from', 'one', 'insanity', 'to', 'another', 'no', 'vision', 'the', 'attempts', 'at', 'humor', 'are', 'so', 'dark', 'to', 'be', 'totally', 'un', '##fu', '##nn', '##y', 'i', 'get', 'the', 'feeling', 'tom', 'wrote', 'this', 'to', 'valid', '##ate', 'his', 'own', 'self', '##lo', '##athing', 'and', 'in', 'my', 'case', 'it', 'su', '##ce', '##eded', 'as', 'i', 'end', 'up', 'di', '##sl', '##iki', '##ng', 'this', 'person', 'intensely', ',', 'not', 'just', 'for', 'his', 'morals', 'and', 'lack', 'of', 'inner', 'strength', 'but', 'for', 'removing', 'my', 'vision', 'of', 'him', 'as', 'a', 'funny', 'talented', 'at', 'times', 'actor', '##not', 'one', 'for', 'faint', 'hearts', 'and', 'not', 'a', 'book', 'to', 'enjoy', 'neither', 'is', 'their', 'any', 'depth', ',', 'what', 'are', 'described', 'are', 'a', 'series', 'of', 'be', '##fu', '##ddled', 'mixed', 'up', 'scenes', 'which', 'go', 'nowhere', 'to', 'a', 'answer', 'the', 'question', 'of', 'the', 'title', '##w', '##ho', 'is', 'tom', 'baker', 'answer', 'from', 'this', 'book', 'is', 'an', 'almost', 'con', '##tino', '##usly', 'depressed', 'social', 'inadequate', ',', 'of', 'ne', '##gli', '##gible', 'talent', 'who', 'somehow', 'has', 'survived', 'despite', 'being', 'totally', 'screwed', 'up', 'by', 'events', 'in', 'his', 'life', 'not', 'what', 'i', 'wanted', 'to', 'read'], ['having', 'tried', 'to', 'find', 'out', 'who', 'tom', 'baker', 'is', 'i', 'wish', 'i', 'had', 'not']], [['this', 'is', 'the', 'greatest', 'children', '##s', 'chess', 'book', 'ever', 'it', 'introduces', 'children', 'and', 'adults', 'to', 'the', 'pieces', ',', 'how', 'they', 'move', ',', 'their', 'value', ',', 'and', 'lays', 'a', 'great', 'foundation', 'for', 'further', 'skills', 'development'], ['an', 'excellent', 'book', 'for', 'children', 'and', 'adults', 'it', 'provides', 'a', 'child', 'friendly', 'and', 'sound', 'foundation', 'for', 'further', 'skills', 'development']], [['great', 'book', 'and', 'i', 'have', 'got', 'it', 'in', 'good', 'shape', 'and', 'in', 'better', 'time', 'love', 'the', 'dust', 'jacket', 'and', 'the', 'illustration', 'this', 'book', 'comes', 'handy', 'even', 'for', 'adults', 'who', 'have', 'not', 'play', 'chess', 'for', 'decades'], ['nothing', 'like', 'spending', 'a', 'good', 'time', 'with', 'a', 'friend', 'in', 'a', 'rainy', 'day', ',', 'and', 'why', 'not', ',', 'at', 'any', 'time']], [['unfortunately', 'this', 'book', 'is', 'out', 'of', 'print', ',', 'but', 'it', 'is', 'by', 'far', 'the', 'best', 'book', 'which', 'helped', 'get', 'my', 'son', 'interested', 'the', 'drawings', 'are', 'delightful', 'and', 'full', 'of', 'expression', 'for', 'example', ',', 'and', 'i', 'am', 'para', '##ph', '##ras', '##ing', 'the', 'pawn', '##s', 'are', 'foot', 'soldiers', ',', 'marching', 'up', 'one', 'by', 'one', ',', 'and', 'can', 'stubborn', '##ly', 'block', 'others', 'from', 'advancing', '##in', 'short', ',', 'the', 'analogy', 'to', 'soldiers', 'in', 'war', 'is', 'what', 'my', 'son', 'likes', 'so', 'much', 'about', 'the', 'book', 'he', 'gets', 'to', 'attack', 'me', 'and', 'st', '##rate', '##gi', '##ze', 'about', 'it', 'he', 'is', 'given', 'freedom', 'among', 'a', 'few', 'rules', 'you', 'can', 'move', 'where', '##ever', 'you', 'want', 'but', 'only', 'diagonal', '##ly', ',', 'in', 'the', 'case', 'of', 'the', 'bishop', 'i', 'was', 'amazed', 'at', 'how', 'fast', 'he', 'was', 'st', '##rate', '##gizing', 'several', 'moves', 'ahead', 'the', 'final', 'pages', 'are', 'full', 'of', 'advanced', 'terms', 'and', 'all', 'are', 'explained', 'well', ',', 'with', 'pictures', 'to', 'help', 'explain', 'the', 'moves', 'and', 'practice', 'games', '##i', 'wish', 'the', 'uk', 'would', 'reprint', 'this', 'book', ',', 'in', 'hardcover', 'the', 'paperback', 'we', 'got', 'only', 'thing', 'left', 'will', 'not', 'last', 'long'], ['the', 'only', 'chess', 'book', 'to', 'buy', 'for', 'kids', 'and', 'adults', 'alike']], [['this', 'was', 'a', 'favourite', 'of', 'a', 'daughter', 'who', 'was', 'given', 'it', 'for', 'her', 'third', 'birthday', 'and', 'it', 'fast', 'became', 'a', 'loved', 'book', 'by', 'our', 'four', 'children', ',', 'and', 'then', 'the', 'next', 'generation', '##a', 'wonderful', 'collection', 'for', 'ages', 'up', 'to', '12', 'or', 'so', 'so', 'much', 'in', 'it', 'from', 'the', 'ju', '##mb', '##lies', 'on', 'long', 'poems', ',', 'short', 'ones', ',', 'happy', 'and', 'sad', 'charming', 'illustrations', 'a', 'wonderful', 'present', 'for', 'boys', 'and', 'girls'], ['this', 'was', 'a', 'favourite', 'of', 'a', 'daughter', 'who', 'was', 'given', 'it', 'for', 'her']]]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# for pairs in create_review_pairs(6):\n",
    "#     print(pairs['review_pair'])\n",
    "#     print('---'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[6,512,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_4/AdamWarmup/gradients/zeros_34}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_5/add}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-417-b2baeed99a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     callbacks=[\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     ], #\n\u001b[1;32m     23\u001b[0m )\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1416\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    215\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    216\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1217\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[6,512,768] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node training_4/AdamWarmup/gradients/zeros_34}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_5/add}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "# Train model -512\n",
    "\n",
    "# Create checkpoint callback\n",
    "checkpoint_retrain = \"training_2/cp-{epoch:04d}.ckpt\"\n",
    "checkpoint_retrain_dir = os.path.dirname(checkpoint_retrain)\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    checkpoint_retrain, verbose=1, save_weights_only=True,\n",
    "    # Save weights, every 5-epochs.\n",
    "    period=5)\n",
    "# pass callback to training\n",
    "\n",
    "\n",
    "model.fit_generator(\n",
    "    generator=_generator(train=True),\n",
    "    steps_per_epoch=10,\n",
    "    epochs=2,\n",
    "    validation_data=_generator(),\n",
    "    validation_steps=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
    "    ], #\n",
    ")\n",
    "#cp_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sentence_pairs = [\n",
    "#     [['all', 'work', 'and', 'no', 'play'], ['makes', 'jack', 'a', 'dull', 'boy']],\n",
    "#     [['from', 'the', 'day', 'forth'], ['my', 'arm', 'changed']],\n",
    "#     [['and', 'a', 'voice', 'echoed'], ['power', 'give', 'me', 'more', 'power']],\n",
    "# ]\n",
    "\n",
    "# def _generator():\n",
    "#     while True:\n",
    "#         yield gen_batch_inputs(\n",
    "#             sentence_pairs,\n",
    "#             token_dict,\n",
    "#             token_list,\n",
    "#             seq_len=384)\n",
    "# #                 mask_rate= #0.3,#default 0.15\n",
    "# #                 swap_sentence_rate=0,\n",
    "# #             ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict\n",
    "# for inputs, outputs in _generator():\n",
    "#     predicts = model.predict(inputs)\n",
    "#     outputs = list(map(lambda x: np.squeeze(x, axis=-1), outputs))\n",
    "#     predicts = list(map(lambda x: np.argmax(x, axis=-1), predicts))\n",
    "#     batch_size, seq_len = inputs[-1].shape\n",
    "#     for i in range(batch_size):\n",
    "#         for j in range(seq_len):\n",
    "#             if inputs[-1][i][j]:\n",
    "#                 print(outputs[0][i][j], predicts[0][i][j])\n",
    "#                 print(decoder_dict[int(outputs[0][i][j])], decoder_dict[int(predicts[0][i][j])])\n",
    "#     print(np.allclose(outputs[1], predicts[1]))\n",
    "# #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = keras.models.load_model(\n",
    "    model_path,\n",
    "    custom_objects=get_custom_objects(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# # Build the model\n",
    "# model = get_model(\n",
    "#     token_num=len(token_dict),\n",
    "#     head_num=5,\n",
    "#     transformer_num=12,\n",
    "#     embed_dim=25,\n",
    "#     feed_forward_dim=100,\n",
    "#     seq_len=20,\n",
    "#     pos_num=20,\n",
    "#     dropout_rate=0.05,\n",
    "# #     training=False,\n",
    "# #     trainable=False,\n",
    "#     output_layer_num=4,\n",
    "# )\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data, model_name='trs', ty='valid', verbose=True):\n",
    "    hyp_g, ref, r1, r2, rl, r_avg = [],[],[],[],[],[]\n",
    "    t = Translator(model)\n",
    "    rouge = Rouge()\n",
    "\n",
    "    l, loss = [], None\n",
    "    pbar = tqdm(enumerate(data),total=len(data))\n",
    "    for j, batch in pbar:\n",
    "        if ty!=\"test\":\n",
    "            loss = model.train_one_batch(batch, train=False)\n",
    "            l.append(loss.item())\n",
    "            \n",
    "        if((j<=1 and ty != \"test\") or ty ==\"test\"): \n",
    "            if ty!='test':\n",
    "                sent_g = model.decoder_greedy(batch) # 1-decoder generation. for testing\n",
    "            else:\n",
    "                sent_g = model.eval_one_batch(batch) # 2-decoder generation.\n",
    "            # sent_b, _ = t.translate_batch(batch) # beam search\n",
    "\n",
    "            for i, sent in enumerate(sent_g):\n",
    "                hyp_g.append(sent) \n",
    "                ref.append(batch[\"target_txt\"][i])\n",
    "                rouges = rouge.get_scores(sent,batch[\"target_txt\"][i])[0] # (hyp, ref)\n",
    "\n",
    "                r1_val,r2_val,rl_val = rouges['rouge-1'][\"f\"], rouges['rouge-2'][\"f\"], rouges['rouge-l'][\"f\"]\n",
    "                r1.append(r1_val)\n",
    "                r2.append(r2_val)\n",
    "                rl.append(rl_val)\n",
    "                r_avg.append(np.mean([r1_val,r2_val,rl_val]))\n",
    "        pbar.set_description(\"EVAL loss:{:.4f} r_avg:{:.2f}\".format(np.mean(l),np.mean(r_avg)))\n",
    "        if(j>1 and ty==\"train\"): break\n",
    "    if l: loss = np.mean(l)\n",
    "    r_avg = np.mean(r_avg)\n",
    "    r1 = np.mean(r1)\n",
    "    r2 = np.mean(r2)\n",
    "    rl = np.mean(rl)\n",
    "\n",
    "    if(verbose):\n",
    "        print(\"\\nEVAL loss: {:.4f} r_avg: [{:.2f}] r1: {:.2f} r2: {:.2f} rl: {:.2f}\".format(loss, r_avg, r1, r2, rl))\n",
    "        for hyp, gold in zip(hyp_g, ref):\n",
    "            print(\"HYP: \")\n",
    "            print(hyp)\n",
    "            print(\"GOLD: \")\n",
    "            print(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy input example\n",
    "sentence_pairs = [\n",
    "    [['all', 'work', 'and', 'no', 'play'], ['makes', 'jack', 'a', 'dull', 'boy']],\n",
    "    [['from', 'the', 'day', 'forth'], ['my', 'arm', 'changed']],\n",
    "    [['and', 'a', 'voice', 'echoed'], ['power', 'give', 'me', 'more', 'power']],\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
