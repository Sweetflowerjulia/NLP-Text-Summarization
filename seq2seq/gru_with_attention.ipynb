{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow \n",
    "tensorflow.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.5/dist-packages (0.21.2)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.2.1)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_en_samples:  500000\n",
      "num_de_samples:  500000\n"
     ]
    }
   ],
   "source": [
    "# load dataset       \n",
    "import gzip    \n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "# path = '../../data/reviews_cleaned.json.gz' \n",
    "path = '../../data/reviews.json.gz' \n",
    "\n",
    "# n=1\n",
    "# encoder and decoder\n",
    "reviews, summaries = list(), list()\n",
    "for data in parse(path):\n",
    "    try:        \n",
    "        if data['summary'] in data['review']:\n",
    "            continue\n",
    "        if ''.join(data['review'].strip().split()[:3]) == ''.join(data['summary'].strip().split()[:3]):\n",
    "            continue        \n",
    "        \n",
    "        reviews.append(data['review'])\n",
    "        # Appending SOS and EOS to target data (decoder)\n",
    "        summaries.append('sos' + data['summary'] + 'eos')\n",
    "#     n+=1\n",
    "#     if n>2000:\n",
    "#         break\n",
    "    except:\n",
    "        print(data)\n",
    "\n",
    "reviews = reviews[:500000]\n",
    "summaries = summaries[:500000]\n",
    "all_data = reviews + summaries\n",
    "\n",
    "num_enc_samples = len(reviews)\n",
    "num_dec_samples = len(summaries)\n",
    "print('num_en_samples: ', num_enc_samples)\n",
    "print('num_de_samples: ', num_dec_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test data split\n",
    "# train_X, test_X, train_Y, test_Y = train_test_split(source_padded, target_padded, test_size=0.01)\n",
    "train_reviews, test_reviews, train_summaries, test_summaries = train_test_split(reviews, summaries, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 116.15 s\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils\n",
    "# running time check\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=16384) #16384#2**14 #32768# 2**15, same as t2t model\n",
    "tokenizer.fit_on_texts(all_data) \n",
    "vocab_size = 16384 #len(tokenizer.word_index) #min(10000, len(tokenizer.word_index) + 1)\n",
    "\n",
    "# encoder source data\n",
    "source_token = tokenizer.texts_to_sequences(train_reviews)\n",
    "max_encoder_seq_length = 512 #max([len(sentence) for sentence in source_token])\n",
    "source_padded = pad_sequences(source_token, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "# decoder target data\n",
    "target_token = tokenizer.texts_to_sequences(train_summaries)\n",
    "max_decoder_seq_length = max([len(sentence) for sentence in target_token])\n",
    "target_padded = pad_sequences(target_token, maxlen=max_decoder_seq_length, padding = \"post\")\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} s'.format(round(stop - start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import CuDNNGRU, Dense, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_X = source_padded\n",
    "train_Y = target_padded\n",
    "\n",
    "BUFFER_SIZE = len(train_X)\n",
    "BATCH_SIZE = 32#64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 100 #256\n",
    "units = 512 #1024\n",
    "# vocab_size = 16384\n",
    "vocab_inp_size = vocab_size\n",
    "vocab_tar_size = vocab_size \n",
    "enc_units = train_X.shape[1]\n",
    "dec_units = train_Y.shape[1]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)   \n",
    "        encoder_dropout   = (TimeDistributed(Dropout(rate = 0.2)))(x) #dropout_rate\n",
    "        gru_layer_1 = CuDNNGRU(self.enc_units, return_sequences=True) \\\n",
    "                            (encoder_dropout, initial_state = hidden)\n",
    "        gru_layer_2 = CuDNNGRU(self.enc_units, return_state=True)\n",
    "        output, state = gru_layer_2(gru_layer_1)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)      \n",
    "        \n",
    "#         self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        gru_dec_layer_1 = CuDNNGRU(self.dec_units,return_sequences=True)(x)     \n",
    "        gru_dec_layer_2 = CuDNNGRU(self.dec_units,return_sequences=True, return_state=True)\n",
    "        output, state = gru_dec_layer_2(gru_dec_layer_1)   \n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints_v3/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1 Batch 0 Loss 2.0343\n",
      "Epoch 1 Batch 100 Loss 2.0385\n",
      "Epoch 1 Batch 200 Loss 2.0033\n",
      "Epoch 1 Batch 300 Loss 2.0532\n",
      "Epoch 1 Batch 400 Loss 2.0754\n",
      "Epoch 1 Batch 500 Loss 2.1590\n",
      "Epoch 1 Batch 600 Loss 1.9388\n",
      "Epoch 1 Batch 700 Loss 1.8120\n",
      "Epoch 1 Batch 800 Loss 1.9068\n",
      "Epoch 1 Batch 900 Loss 1.9893\n",
      "Epoch 1 Batch 1000 Loss 1.9213\n",
      "Epoch 1 Batch 1100 Loss 1.9306\n",
      "Epoch 1 Batch 1200 Loss 1.6726\n",
      "Epoch 1 Batch 1300 Loss 1.8163\n",
      "Epoch 1 Batch 1400 Loss 1.9242\n",
      "Epoch 1 Batch 1500 Loss 1.9221\n",
      "Epoch 1 Batch 1600 Loss 1.7161\n",
      "Epoch 1 Batch 1700 Loss 1.6606\n",
      "Epoch 1 Batch 1800 Loss 1.8692\n",
      "Epoch 1 Batch 1900 Loss 1.7607\n",
      "Epoch 1 Batch 2000 Loss 1.7601\n",
      "Epoch 1 Batch 2100 Loss 1.8265\n",
      "Epoch 1 Batch 2200 Loss 1.7360\n",
      "Epoch 1 Batch 2300 Loss 1.7665\n",
      "Epoch 1 Batch 2400 Loss 1.8079\n",
      "Epoch 1 Batch 2500 Loss 1.8795\n",
      "Epoch 1 Batch 2600 Loss 1.4789\n",
      "Epoch 1 Batch 2700 Loss 1.7097\n",
      "Epoch 1 Batch 2800 Loss 1.7184\n",
      "Epoch 1 Batch 2900 Loss 1.6121\n",
      "Epoch 1 Batch 3000 Loss 1.6750\n",
      "Epoch 1 Batch 3100 Loss 1.6971\n",
      "Epoch 1 Batch 3200 Loss 1.4706\n",
      "Epoch 1 Batch 3300 Loss 1.6249\n",
      "Epoch 1 Batch 3400 Loss 1.5746\n",
      "Epoch 1 Batch 3500 Loss 1.5708\n",
      "Epoch 1 Batch 3600 Loss 1.6173\n",
      "Epoch 1 Batch 3700 Loss 1.3529\n",
      "Epoch 1 Batch 3800 Loss 1.5600\n",
      "Epoch 1 Batch 3900 Loss 1.6032\n",
      "Epoch 1 Batch 4000 Loss 1.5925\n",
      "Epoch 1 Batch 4100 Loss 1.6338\n",
      "Epoch 1 Batch 4200 Loss 1.5449\n",
      "Epoch 1 Batch 4300 Loss 1.4522\n",
      "Epoch 1 Batch 4400 Loss 1.5494\n",
      "Epoch 1 Batch 4500 Loss 1.4862\n",
      "Epoch 1 Batch 4600 Loss 1.4234\n",
      "Epoch 1 Batch 4700 Loss 1.4032\n",
      "Epoch 1 Batch 4800 Loss 1.3691\n",
      "Epoch 1 Batch 4900 Loss 1.4802\n",
      "Epoch 1 Batch 5000 Loss 1.6227\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ef9f2cc79c7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-467429e9ce95>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mencoder_dropout\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dropout_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mgru_layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCuDNNGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m                             \u001b[0;34m(\u001b[0m\u001b[0mencoder_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mgru_layer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCuDNNGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_layer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_layer_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'constants'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m   def call(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \u001b[0;31m# Only call `build` if the user has manually overridden the build method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/cudnn_recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         constraint=self.recurrent_constraint)\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     self.bias = self.add_weight(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    605\u001b[0m     new_variable = getter(\n\u001b[1;32m    606\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m    146\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                         aggregation=VariableAggregation.NONE):\n\u001b[1;32m    154\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2488\u001b[0;31m         import_scope=import_scope)\n\u001b[0m\u001b[1;32m   2489\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)\u001b[0m\n\u001b[1;32m    292\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m   \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m    408\u001b[0m           self._handle = eager_safe_variable_handle(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m       init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n\u001b[0;32m--> 127\u001b[0;31m           shape, dtype=dtype, partition_info=partition_info)\n\u001b[0m\u001b[1;32m    128\u001b[0m       \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, partition_info)\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;31m# Compute the qr factorization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_linalg_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_matrices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m     \u001b[0;31m# Make Q uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_linalg_ops.py\u001b[0m in \u001b[0;36mqr\u001b[0;34m(input, full_matrices, name)\u001b[0m\n\u001b[1;32m   1823\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[1;32m   1824\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Qr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1825\u001b[0;31m         _ctx._post_execution_callbacks, input, \"full_matrices\", full_matrices)\n\u001b[0m\u001b[1;32m   1826\u001b[0m       \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_QrOutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# running time check\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([tokenizer.word_index['sos']] * BATCH_SIZE, 1)\n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "        # saving (checkpoint) the model every 500 batch\n",
    "        if batch % 500 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} s'.format(round(stop - start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: word for word, idx in tokenizer.word_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./checkpoints_v3/ckpt-11'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('./checkpoints_v3/') #model.ckpt\n",
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize(text, encoder, decoder, tokenizer, max_encoder_seq_length, max_decoder_seq_length):\n",
    "def summarize(text):\n",
    "    attention_plot = np.zeros((max_decoder_seq_length, max_encoder_seq_length))\n",
    "    \n",
    "#     text = preprocess(text)\n",
    "    inputs = tokenizer.texts_to_sequences([text])\n",
    "    inputs = pad_sequences(inputs, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "#     print(inputs)\n",
    "#     print(hidden)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['sos']], 0)\n",
    "\n",
    "    for t in range(max_decoder_seq_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2word[predicted_id] + ' '\n",
    "\n",
    "        if idx2word[predicted_id] == 'eos':\n",
    "            return result, text, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, text, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File ./checkpoints_v3/ckpt-11 does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-24912fa150aa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetection_graph\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Load the graph with the trained states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mimport_meta_graph\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, **kwargs)\u001b[0m\n\u001b[1;32m   1433\u001b[0m   \"\"\"  # pylint: disable=g-doc-exception\n\u001b[1;32m   1434\u001b[0m   return _import_meta_graph_with_return_elements(\n\u001b[0;32m-> 1435\u001b[0;31m       meta_graph_or_file, clear_devices, import_scope, **kwargs)[0]\n\u001b[0m\u001b[1;32m   1436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_import_meta_graph_with_return_elements\u001b[0;34m(meta_graph_or_file, clear_devices, import_scope, return_elements, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m                        \"execution is enabled.\")\n\u001b[1;32m   1446\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1447\u001b[0;31m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_meta_graph_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1448\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1449\u001b[0m     \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_or_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/meta_graph.py\u001b[0m in \u001b[0;36mread_meta_graph_file\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    631\u001b[0m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMetaGraphDef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File %s does not exist.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m   \u001b[0;31m# First try to read it as a binary file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m   \u001b[0mfile_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: File ./checkpoints_v3/ckpt-11 does not exist."
     ]
    }
   ],
   "source": [
    "checkpoint = tf.train.latest_checkpoint('./checkpoints_v3/') #model.ckpt\n",
    "checkpoint\n",
    "# model_path = \"model.ckpt\"\n",
    "detection_graph = tf.Graph()\n",
    "with tf.Session(graph=detection_graph) as sess:\n",
    "    # Load the graph with the trained states\n",
    "    loader = tf.train.import_meta_graph(checkpoint)\n",
    "    loader.restore(sess, checkpoint)\n",
    "\n",
    "    # Get the tensors by their variable name\n",
    "    encoder = detection_graph.get_tensor_by_name('encoder:0')\n",
    "    decoder = detection_graph.get_tensor_by_name('decoder:0')\n",
    "#     scores = detection_graph.get_tensor_by_name('detection_scores:0')\n",
    "    pred_summaries = []\n",
    "    for review in test_reviews[0:10]:\n",
    "        summary, review, attention_plot = summarize(review)\n",
    "        pred_summaries.append(summary)\n",
    "        print(\"summary: \", summary, \"\\n\")\n",
    "        print(\"review: \", review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  eos  \n",
      "\n",
      "review:  one of the best books written about what survivors go through from day one of tragic loss to day  whatever  when you pick up this book.  for me, it was one month after the loss of my beloved brother and my mind was in chaos and my emotions were jumping all over the place.  i was scared, sad, and felt completely lost.  this book allowed me to accept that i was not a victim but a 34;survivor34;  a concept foreign to me.  this book does not clinically put you to sleep with facts, statistics, lists, or frustrate you because the reader cannot relate.  it covers every aspect of loss  be it child, sibling, mother, father, friend, and extended family members.  i highly recommend this book as a first read after suffering such a devastating loss.  the book is intelligent, emotive, and written with a compassion for anyone who has lost a loved one or friend to suicide.  the book explores taboos and debunks them, and offers the reader avenues to pursue to avoid the archaic preconceived notions held by an alarming number of people in this day and age.  i know this reader will pick up this book again and again as i travel down the road of surviving the sudden, unexpected loss of my brother, michael.\n",
      "summary:  the the eos  \n",
      "\n",
      "review:  published six years before i was born, different seasons is kings first novella collection, compiling four longerthanashortstorybutshorterthananovel stories. three out of the four stories in this collection have been turned into hollywood films the exception being the breathing method, two of the films to great success and acclaim.in a way, king was breaking new ground in the literary world of his time by being allowed to publish this collection, which he talks about in the books afterword. publishers were not interested in this silly, obscure little thing called a novella. it was too short to publish as a novel and expect people to pay money for, and story collections in general have never brought in the same revenue as a novel in any genre or market. but thankfully, by 1982, stephen king had some clout with the publishers and a morethandedicated fan base of readers. so, long story short, he brought the idea before his agent, and after some convincing, different seasons was introduced to the world. there is even a little tease in the title; this work is compiled of shorter stories, which is different than what king had normally published before if that sounds like conjecture, read the afterword. i am paraphrasing kings own words, here.rita hayworth and shawshank redemption: how much honestly needs to be said about such a wellknown story? the movie is very well known and, honestly, far more famous than the story, at least to the general public. i have found in my own circles that nonking fans are rarely aware that stephen king even wrote the story. anyway, this is one of kings best works, and it has a certain timelessness about it that is rarely found in contemporary literature. it is full of life in every sentence, made up of just the right balance of humor, real pain, and airtight cleverness so that it concludes with a neat little bow that leaves you walking away with a smile. the movie, thankfully, is very true to the book and is equally well done.apt pupil: here is where things turn darkest in this collection it is appropriately subtitled summer of corruption. the story begins in 1974 and introduces todd bowden, a seemingly kind and innocent boy of fourteen who becomes obsessed with 1940s war magazines that he finds amongst the comic books at a friends house. by way of his poring over these gruesome magazines, todd discovers that one of his own neighbors, an elderly german man named arthur denker, is in fact a wanted nazi war criminal who had changed his name and fled his previous life. todd confronts the old man early in the story where he also finds out denkers true name; kurt dussander to denkers horror, as his secret had lain hidden for twentyfive years or more. what follows is a remarkably twisted game of for lack of a better term catandmouse in which todd blackmails denker and threatens to reveal his secret if denker does not recount to todd all of the wretched, torturous things he performed on jews during the war. it is slowly revealed that todd is not so innocent as we were led to think, and his morbid fascination with concentration camp killing methods manifests itself in increasingly darker forms.while reading this, i kept thinking to myself, early on, that i did not know how it could keep going for as many pages as i saw were left. the story is deceivingly straightforward at the start, to the point that i thought i had the end figured out. well, leave it to king to take things in an unexpected direction and where he took the story was, obviously, much better than what i had in mind. this is a chilling tale, though very wellcrafted, and involves one of the most complicated and messy friendshipsturnedrivalry i have ever encountered in literature.the body: again, a wellknown story, primarily for the success of the film adaptation stand by me. it is pleasantly straightforward; four young boys in the 1960s go for a long journey to find something mysterious, and learn about their friendships and themselves along the way. for as reminiscent of a tale as this is, with many humorous moments of colorful adolescent dialogue and smiling glimpses of americapast, there is a stark human sadness, even darkness, present as well. none of the boys home lives are very pleasant, and the confrontations with the older kids are downright scary. king can make a violent, unpredictable highschooler more unsettling than the strangest of his otherworldly creatures.king narrates brilliantly in the adult voice of gordie lachance, probably the most levelheaded and mature of the boys in the story of their youth. his friendship with chris chambers is quiet and profound, while his connection to the other two boys is more surfacelevel. in all, this is not necessarily a happy story, but like many coming of age pieces, this too has both moments of joy and of tough, grownup realizations.the breathing method: the final tale in the collection is the shortest, as well as the most odd. it is narrated by a middleaged manhattan lawyer named david, though the focus is an abstract mens club that meets in the parlor of an old hotel. david attends numerous times over a series of months, only to find that most of the time is spent by the men ambling about on their own, sipping fine whiskies, reading books by the fire, playing pool, and closing each gathering with a story. these stories are bizarre and often macabre, as they all end with some sort of death.the breathing method is a story within the story that an elderly physician tells the men of the club on the last meeting before christmas. to tell it here would ruin the point of you reading it, but i will at least reveal that it begins very sweet and sentimental, and ends very strange.while the breathing method piece of the narrative itself is interesting, the story as a whole seems somewhat misguided. there is an odd hint at things supernatural within the club, with suggestion that the gathering takes place in what might be a portal with doors to other worlds king fans may even take this further to suggest it is loosely connected to the dark tower, though this is only briefly passed over in the closing dialogue and never expanded upon. there is a sinister lovecraftian undertone to the atmosphere of the club which is fun, but again, only hinted at.different seasons as a whole is a wonderfully diverse and multilayered read, and displays king in top form, what is more is to see him excel at writing something other than horror. though he has pursued other more regular genres in the years since, surely in 82 this book had to have helped solidify him as not only a master of horror, but rather a master of writing in general. it seems there is no medium or genre he cannot do, and do quite well.\n",
      "summary:  the the the the eos  \n",
      "\n",
      "review:  this book is very well laid out.  i am a teacher and would absolutely recommend this as an intro textbook to marine science.  it contains all that you would need to know.  great as a first year colledge text or even an advanced high school textbook.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  eos  \n",
      "\n",
      "review:  oh my gosh, oh my gosh! quintana not queentana...let us start with that i love all characters in the books i love love the passion of the law the passion of the people, and it is a shame that this remarkable...author died...i know where you are dear author i also know that heaven has an incredible library, all truths, all motivations revealed, personally it is the first place i would go...you must be so happy thanks for all the great books you wrote\n",
      "summary:  the the the the the the the the the eos  \n",
      "\n",
      "review:  as an experienced programmer this book really helped teach me the basics behind ios dev. it also provided good code to help me understand some of the more specific topics that were necessary for the project that i was given. i think that issues such as memory management can be confusing to newcomers, but this book did a decent job at tackling the subject.\n",
      "summary:  the the the eos  \n",
      "\n",
      "review:  callie parrish works at middletons as a mortuary cosmetologist in south carolina.  otis and odel middleton are real characters, but she enjoys her job.callie discovers a needle broken off in bobby saxons neck while she is applying makeup and styling his hair.  bobby, a local car salesman and womanizer, was found floating face down in a swimming pool.  since he was a heavy drinker, the coroner accepted drowning as the cause of death.although bobby was the age of callies older brothers, his current wife was a school friend of callies.  callie calls her bouncy betty behind her back.  she is also quite brash.when bobby is sent for an autopsy, his fancy, empty casket is stolen and callie is hit on the head.when she comes to in the er, she finds dr. don walters to be quite a dish.  after she is released with stitches, he asks her out on a date.  she accepts.  callies best friend, jane is blind but assists callie in determining motives and solving crimes.june bug corley, a local bar owner, is found shot in his parking lot.  soon callie becomes a target, receiving nasty phone calls that are threatening and then someone breaking into her house.  does she know more than she realizes?  can she solve these crimes before she is the next victim?callie is a great character.  i look forward to reading many more books in this series.  having an amateur sleuth in a mortuary really has a lot of possibilities.  gives her good reason to be sleuthing as well.i love her friend jane.  it is always amazing to me all the things she can do.  but yet the author has made it very believable.the south carolina setting is great as well.  i highly recommend this book.\n",
      "summary:  the the the the the the the the the the the eos  \n",
      "\n",
      "review:  this book came highly recommended but i was disappointed.chapter one was great, but the rest was disappointing.when dealing with the story of job, he makes 3 assertions and states that only 2 can be true. a, god is all powerful and causes everyting. b, god is just and fair. c, job is a good person.. his conclution here is that god is not all powerful, otherwise he would have helped job.the devil causing evil is not discussed as a choice.pg67 says god does not cause bad things to happen and also, he is unable to stop bad things from happening.he states that all the supernatural events and miracles of the bible are just stories to make god look good chapter 3.he also states that god cannot defy the laws of nature.i stopped reading at this point.\n",
      "summary:  eos  \n",
      "\n",
      "review:  as much as i try to make his books last, i continually find cobens books impossible to put down once you start.  missing you was no exception.\n",
      "summary:  the the the the the the eos  \n",
      "\n",
      "review:  i enjoy graftons books and have read aq so far.  i originally read o is for outlaw and then decided to start with a.  i consider these books to be 34;light34; reading.  even though they are mystery novels and mildly thought provoking, they are more like a miniseries with the same main characters and do not take big brain effort.  i believe this was the first one that she based on a real unsolved cold case at least she has not pointed it out before and it is good that it brought renewed interest in pursuing it.  i especially applaud the addition of the photos of actual facial recreations at the end of the book.\n",
      "summary:  the the the the the eos  \n",
      "\n",
      "review:  memoirs of a geisha is a novel and the character of the geishas is only arthur goldens inventions. but the novel presents a seamless authenticity of a hidden world full of complex rituals, ruts, regulations, and machinations. interwoven between first and third person narratives, the novel is a geishas reflection of her life that began in early twentieth century, in a remote fishing village toroido. a little girl was sold to an okiya in kyoto where she worked as a maid. upon separation from her older sister and after the death of her parents, she lost all hope and her dream of seeing her family shattered. all that was left in her were confusion and confusion.since the outsiders have limited knowledge of a geishas profession, even though the geisha might be in a less favored position to observe incidents around her, sayuri might have left a record of herself that is far more complete, more accurate, and more compelling than any previous account. arthur golden has presented a character, a geisha from the 1920s, to deliver a powerful account that reconstructs nuances of a mysterious profession.the incessant chores, the acrimonious okiya mother headmistress, the constant bullying from the jealous geisha in the house made the little girls life more difficult. it strengthened her determination to run away. her futile attempts to escape enraged the mistress and forfeited her opportunity to get the training to become a geisha. chiyo sayuri was her geishas name later ineluctably faced a bleak prospect as her debts to the okiya stacked high. since nobody made a decision to become a geisha, her only choice was to at least complete apprenticeshipand hopefully made enough money as a mediocre geisha to pay back for her living expenses and lessons. one can see that being a geisha is no more than a total surrender of self and will.chiyos road to become a geisha was thorny. she was constant the target of bullying from a senior geisha who hated anyone more successful than she was and who thrived to rid of all prospective rivals. even though the senior geisha had falsely accused her, faulted her, and rendered her debut a standstill, her determination to become a geisha did not spring from the inventive to revenge on her enemy. the driving force was to attract a man who was as gentlemanly as the one who gave her a coin when she first arrived in kyoto. this is the heart of the novel: in the geishas world where appearances are paramount and where love is scorned as illusion, our little heroine has steered her whole life toward winning the affection of one man whom she admired.the geisha world was about putting on the most impeccable appearance in order to attract a longterm patron danna who would sustain a relationship like a business deal. in other word, a true geisha would never risk to blemish her reputation by making herself available to men on a nightly basis. even though she would not pretend she never gave in to a man she found attractive, she had to be extremely careful and discreet about any serious romantic relationship that would jeopardize her relationship with beneficiaries. a geisha, like our heroine, was advised against any circumstances that would diminish the chance of anchoring to a powerful danna. a geisha was to put on the best show to fish for a sponsor. therefore, on the account of the exquisite fabrics that draped a geisha and the strict ceremonial measures, the most severe rebuke a young geisha was likely to receive probably would not be for performing poorly, but rather for having dirty fingernails, tousled hair or having poor manner. every aspect of a geishas life is used to secure an affluent tutelage, is programmed to success, which is gauged by money. behind the impeccable beauty was painful melancholy.in a world where a girls virginity was auctioned to the highest bidder, memoirs of a geisha illuminates how inexorably a geisha must comply to the complicated ruts in order to sustain popularity. it beguiles the reader as much as the geishas beguiled the most powerful men; and immerses the reader in an exotic territory with its nuanced portraits of lives in the okiya and the gion geisha district in kyoto. the novel spans over sixty years encompassing the great depression and the second world war, following which sees the downhill of the geisha industry.\n"
     ]
    }
   ],
   "source": [
    "pred_summaries = []\n",
    "for review in test_reviews[0:10]:\n",
    "    summary, review, attention_plot = summarize(review)\n",
    "    pred_summaries.append(summary)\n",
    "    print(\"summary: \", summary, \"\\n\")\n",
    "    print(\"review: \", review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  and and and and and and and and of and and and and and to and and the and the and and and and and and and and  \n",
      "\n",
      "review:  the archetypical american novel features a solitary protagonist undertaking an odyssey in which heshe confronts both physical and moral challenges and through which heshe emerges with a renewed spirit, transformed by the crucible of confrontations with adversaries, both real and imagined  sara gruens engaging water for elephants is an eminently american work, set in the depths of the depression and featuring a brokenhearted young man whose unplanned existential leap of faith catapults him into a chaotic and unknown way of life  jacob jankowski discovers his untapped reservoir of courage, conviction and compassion, and in so doing, he, in every manner of the american definition of the word, emerges as a hero  water for elephants is a triumph    of a writer fully in control of her craft, of a character resolute in his determination to discover life and of a depiction of ideas that truly embody our national characterthe innocent jacob, on the threshold of graduating from cornell veterinary school and returning to the midwest to practice medicine with his father, is shattered by the sudden accidental deaths of his parents  unmoored from his past and unable to focus on his final examination, he abandons his former life the moment he jumps on board a passing train  to his astonishment, he has leaped into the midst of a traveling circus, and, thanks to the flinty care of several roustabouts, manages to gain employment with the tawdry benzini brothers most spectacular show on earth, a circus dominated by a sadistic ringmaster and populated by a cast of characters, admirable and evil in equal proportions  in a manner of weeks, jacob immerses himself as the circus vet, forms abiding friendships, falls is love and faces moral dilemmas that tie his conscience into gordian knotssome critics have likened jacob to his biblical counterpart and praised the novel for its allusions to the bible  in my opinion, jacob mirrors a less sanctified model, huckleberry finn  like huck, jacob flees familiar surroundings, flaunts social norms, befriends outcasts and risks his personal safety in order to adhere to a hardearned sense of conscience and social responsibility  both huck and jacob confront mendacious adversaries, rejoice in life on the lam and reinvent themselves  like twain, sarah gruen writes with inordinate respect about the dispossessed and with painful awareness about the inexplicable capacity for the human heart to do well in the midst of spiritual darkness  as twain has written    and as gruen affirms    the source of humor is paineven the title bespeaks our capacity for misrepresentation  the elderly jacob, shunted away by his family in a assistedliving nursing home, excoriates a new resident who has claimed to have carried water in buckets for elephants when he was a youngster  aware of an elephants enormous drinking capacity, jacob knows a lie when he hears one  and well he should  the entire purpose of the traveling circus is deception, an enterprise where rubes are separated from their money by gaping at mangy animals, staring at sideshow freaks many of whom are frauds or being serviced by exotic dancers  falsehoods, illusions and duplicity are the moral big tent under which circus performers enchant depression men and women hungry for escape  the central conflict of the novel rotates around jacobs involvement with a married woman and the peculiarly disturbing aspects of her husbands mental state  this conflict is rife with secrecy, duplicity and tormented twisting of ideas of right and wrongit is no surprise that this novel is a bestseller  written with extraordinary narrative drive, populated by characters who truly represent our national character and animated by a young man whose journey for selfdiscovery is our own, water for elephants intimately involves us in jacob jankowskis search for meaning\n",
      "summary:  and and this and this and and and the of the and and and and and the of to and and and the and to and and the  \n",
      "\n",
      "review:  so tommy and tuppence were married, she chanted, and lived happily ever afterwards and six years later they were still living together happily ever afterwardssome stories were fairly straightforward and i was able to guess the end which turned out right like the unbreakable alibi  yippeeee the bantering between tommy and tuppence is amusing they are two people who complement each other while tommy is serious and cautious, tuppence is impulsive and hopelessly optimistici can rub my hands together when i am pleased for polton,tommy and tuppence, who were first introduced in the book the secret adversary,are now happily married but tuppence is restless and bored she craves for some adventurei wish something would happenas if her wish is granted, they are visited by mr carter who works for some unnamed secret government agency and is their old friend he proposes that they take over the international detective agency and tommy pose as its owner mr theodore blunt they are to intercept secret messages from russia meanwhile they are to run the detective agency as their own and solve any cases that come their waythe book contains short accounts of different cases that come their way with the secret russian angle running in the background in all the storiesagatha christie wrote these stories as parodies on the famous detectives of 1920s like sherlock holmes, john thorndyke, father brown, tommy mccarty and even poirot the stories present some much unexpected turns like in the case of the missing lady, i could never have guessed the end solution it will be too marvelouswe will hunt down murderers, and discover the missing family jewels, and find people who have disappeared and detect embezzlersi always like your cheery optimism, tuppence you seem to have no doubt whatever that you have talent to exercisewell i have read every detective novel that has been published in the last ten yearsthey are a fun loving couple who enjoy their life as well as the cases leaving the reader feeling giddy and delightedi especially liked their efforts in imitating their favorite detective inspirations likea nice little syringe and a bottle labelled cocaine for sherlock holmesand it was hilarious when they sort of overdid the sherlock holmes or polton or father brown mannerism copyinghe picked up a violin which lay on the table, and drew the bow once or twice across the strings tuppence ground her teeth and even the explorer blanched  p dalbert, first introduced in the secret adversary, is another great character who apart from being a domestic help at home is also appointed by tommy and tuppence beresford as their assistant at their agency  he is efficient, yet eccentric and likes to change his behavior, mannerisms to suit a situation based on movies he has watched likea tall lad of fifteen who seemed undecided as to whether he was a footman or a page boy inquired in a truly magnificent mannerare you at home, madam the front door bell has just rungi wish albert would not go to the pictureshe is copying a long island butler now thank goodness i have cured him of asking for peoples cards and bringing them to me on a salver or albert relinquished the role of a long island butler, and took  up that of office boy, a part which he played to perfection a paper bag of sweets, inky hands, and a tousled head was his conception of the charactermy words are inconsequential when it comes to commenting on agatha christie she is aptly described as the queen of crime and she does not let us down in this book toothe book seems like a casual, nostress venture from her and therefore is a perfect light, entertaining readthe short stories though dealing with different cases are connected and should be read in the order they are presented in the bookalthough not the best from tommy and tuppence, the stories are simple in style but very expertly written and exercise our little grey cells vigorously i give it a 38 out of 5 and urge all to read and fall in love with tommy and tuppence, a great pair of detectives who are courageous, impulsive, loyal and not afraid to risk their lives following the slogan any case solved in 24 hours they will and can solve any case no matter how bizarre that comes their wayprepare to test all your mental faculties in this beautiful short story collection with these great partners in crime this review is also available on njkinnyblogspotin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  and to this of of the and the the of and and and the the the and of and to and and and and of and and the  \n",
      "\n",
      "review:  flash jumps around a bib in this adventure, but in the west his manner improved form deplorable to tolerably despicablei do not think it would be amused with the oh show tune , i want to be an indian too , would not you  like a sioux but he honest shows a slice of what the encroaching society effect were on the fictional charterers and there action and reaction to the new world  as far as the locals, heck it was their street corner so i will not make any judgment\n",
      "summary:  and and the and and and and the and and the and of and the this the and and and and and the and and the and and  \n",
      "\n",
      "review:  a song of fire and ice is one of the few fantasy series which becomes more enjoyable with each novel released  a dance with dragons was a long time coming but the wait was worth itabout half of the main characters were missing from a feast for crows, so i thought that dance would show the same period of time from different viewpoints instead of advancing the story this is not the caseabout 14th of dance overlaps with the previous novel  after that the overall plot advances and takes some interesting turns characters absent from feast such as jon snow and tyrion tell the majority of dance, but jaime, arya, and others also have a few chaptersmartin has not dropped the ball a dance with dragons is just as good as game of thrones, and even more fun to read\n",
      "summary:  and and the of of and and and and and the and and this to and and and and of the and and of and and and and  \n",
      "\n",
      "review:  this collection of quality photos taken during the christmas season shows americans celebrating in many different ways santa claus appears in many places and in many contexts, emerging from the hawaiian surf to the traditional handing off of a wrapped gift there are also photos of christmas performances, from concerts and ballets to nativity scenes most striking are the beautiful photos of natures winter splendor winter weather can be horrific, but there is nothing prettier that a fresh snowfall in cold crisp weathera celebration of the christmas season with all the splendor and shopping stress, this collection of photos truly captures americans in all aspects of the christmas spirit\n",
      "summary:  and and the and and the and this and and the and and and the and and and and of and the and the to and and the  \n",
      "\n",
      "review:  i have been reading clive cussler novels for many years and i can call myself a true fan of cusslers pop fiction of course, cussler does not aspire to the status of belles lettres highclass literature worthy of study for its aesthetic value, but this author excels in his chosen genre of adventure action novel and of books with plots that relate at least in part to seafaring exploits or maritime curiosities sahara is among the best of cusslers novels, among my very favourite of his exciting epics only the mediterranean caper, iceberg, and raise the titantic are as thrilling as sahara is there are few films based on cussler novels, the only two of which i know being cinematic treatments of sahara, a great boxoffice success, and raise the titanic a novel, hence the film too, whose plausibility suffers in retrospect only due to the discovery of the titanic wreck well after cussler had written his novel and after the film industry made a cinematic treatment of it i read sahara many years before the film came out both the novel and the film are supercussler researches his subjects exceedingly well the tuaregs in sahara are true to the life, religious beliefs and practices, and lore of this peculiar muslim sect in mali eg, whose men, rather than their women, wear an allencompassing veil cusslers experience at sea, especially in exploring wrecks and naval mysteries, shows in all of his novels having been in the us navy myself even having consorted for a few months with the navy seals during the kennedy presidency in the early 1960s, i can appreciate the authenticity of cusslers naval and maritime lore as he depicts it in sahara it takes the form of subplots that entail some expert manoeuvering under dangerous conditions of a small river craft vessel as well as the discovery of a marooned confederacy warshipthe reader cannot go wrong with most of cusslers novels especially those which were published before the present 21st century began sahara makes a good point of departure in exploring the dirk pitt pop classics of cusslers famously enjoyable 20th century output if you liked the film treatment, you probably will enjoy the novel even more, if you are an avid reader of action fiction\n",
      "summary:  and and the and the and the and and and the and and the and and to of and and and and and and and and and and  \n",
      "\n",
      "review:  i love books like this i saw this one, browsed through it a bit and bought it right away because it looked very good  it was i can confidently say that this is one of the best books of its type the author explains, in very clear language, the nature of probability and its use in understanding some of the many areas in everyday life that could otherwise remain very obscure or misunderstood the topics covered include gambling games and methods, the ways casinos operate this may be very surprising to many, card games and strategies, preelection polls, certain game shows, the war against spam, weather prediction, and many other areas where probability plays a key role the author, an expert in the field, writes very well and in such an engaging and often humorous style that the book is almost impossible to put down those who read this book will understand a bit more about how the world that we live in actually works i highly recommended this book to everyone\n",
      "summary:  the and the and and the and the the the and the the the and and this and and the and and and and the the and and  \n",
      "\n",
      "review:  i started reading this series about a year ago i was enraptured by the first three books especially a storm of swords, and i came off a bit disappointed by a feast for crows, not just because it had a slow pace, but because my favorite characters were left outbut this only got me more excited for a dance with dragons i thought it could be like the movie twins, where grrm would put all of the recessive genes into feast while putting the dominant genes into dance the book started off well, with some interesting revelations taking place early on, but it quickly returned to the pace of a feast for crowssomething that disappointed me was how few bran chapters were included i felt he had the most interesting chapters in the entire book and not to spoil anything, but it actually gets somewhere even if the rest of the book were just segments of bran training that did not go anywhere, the skinchanging segments are some of the most fascinating passages ever written that it would still be satisfyingbut no, instead of bran, we get nameless character after nameless character that does not do anything only a few of these characters i felt were absolutely necessary, and the ones that were had enough chapters to warrant getting an actual name, instead of being giving 4 or 5 names throughout the bookthe book came off as 1000 pages of buildup to the winds of winter, with events happening at the end of the book that get you really excited for what is to come, only to be cut short i honestly cannot wait for twow, but not because i was so enthralled by this book it is because this book plodded along, introduced some exciting things, but cut us shortto me, george r r martin has become like another george, the one named lucas both creating some of the best pieces of fiction ever george lucas obviously created star wars, and grrm created a song of ice and fire when lucas introduced some questionable ideas with the prequels, who at that point would dare question the man who made star wars it is the same with grrm now his series is commonly referred to as one of the greatest fantasy series of all time what editor would have the guts to tell him that his novel was sloppy, meandering, and pointless up till the last few chapters, especially when under a tight deadline the editor could force him to refine the novel, but push back the ridiculously close july 12th deadline, or the editor could put a first draft into print and make a bunch of moneyso, in conclusion, a dance with dragons was disappointing but hopefully, grrm has purged himself of all the tedious buildup and will deliver us an actionpacked, eventful winds of winter of course, i thought a feast of crows was going to be a purge, too well, i guess i will have to wait and see, and there will be a lot of waiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  and and the and the the of this and the and to and to of the and to and and the this to and the and to the  \n",
      "\n",
      "review:  this is a remarkable book that will introduce you to the process of science and a fascinating aspect of the emergence of life  trilobites are among the best fossils for children to get to know because they are very distinct the tri lobed shells and very different from anything currently living the horseshoe crab on american atlantic beaches is comparable in unique appearance and attracts children with similar fascinationfor those who want a better system of american science education, fortey gives some powerful hints  consider his language quotthe fever of discovery was upon me i found a trilobitethe textbook came alivethis was my first discovery of the animals that would change my life p18quot  he continues, i knew, by some principle which i could not articulate, that the wider end was the head of the animal  and of course upon the head there were the eyes  despite the unfamiliar conformation of the fossil i knew that eyes must always belong on heads  so despite the exoticism of the fossil there was already a common bond between me and the trilobite  we both had our heads screwed on the right wayquotp19again and again fortey reminds us that scientists grow from discovery, mystery, romance, intrigue, while the memorization comes later  he reminds us that there is an enormous amount we still do not know and in the process introduces us to a world we have never considered  quoti want to invest the trilobite with all the glamour of the dinosaur and twice its endurance  i want you to see the world through the eyes of trilobites, to help you make a journey back through hundreds of millions of yearsthis will be an unabashedly trilobitecentric view of the world,quotp19for anyone who wants to take a few hours to think different thoughts, to consider how brief our history has been and how successful other organisms have been, to contemplate the various catastrophic extinctions and their dramatic impact on life, to ask about life beyond the rush hour traffic and the monthly report, forteys work is a little gem of an introduction to a fascinating part of the world  i highly recommend it\n",
      "summary:  the and and and of and of the and the and to the and and the of and and and and and and and and the the and  \n",
      "\n",
      "review:  this was a very well written book  i also appreciated the comments made by the author on her research and the way she brought the stories she uncovered into the book\n"
     ]
    }
   ],
   "source": [
    "pred_summaries = []\n",
    "for review in test_reviews[0:10]:\n",
    "    summary, review, attention_plot = summarize(review)\n",
    "    pred_summaries.append(summary)\n",
    "    print(\"summary: \", summary, \"\\n\")\n",
    "    print(\"review: \", review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/56/90178929712ce427ebad179f8dc46c8deef4e89d4c853092bee1efd57d05/nltk-3.4.1.zip (3.1MB)\n",
      "\u001b[K    100% || 3.1MB 24.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/97/8a/10/d646015f33c525688e91986c4544c68019b19a473cb33d3b55\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-rouge in /usr/local/lib/python3.5/dist-packages (1.1)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install py-rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with Avg\n",
      "\trouge-1:\tP:  3.93\tR:  8.60\tF1:  5.30\n",
      "\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-l:\tP:  6.17\tR: 11.81\tF1:  8.00\n",
      "\trouge-w:\tP:  3.54\tR:  4.71\tF1:  3.94\n",
      "\n",
      "Evaluation with Best\n",
      "\trouge-1:\tP:  3.93\tR:  8.60\tF1:  5.30\n",
      "\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-l:\tP:  6.17\tR: 11.81\tF1:  8.00\n",
      "\trouge-w:\tP:  3.54\tR:  4.71\tF1:  3.94\n",
      "\n",
      "Evaluation with Individual\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 16.67\tF1: 10.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 20.00\tF1: 10.53\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 16.67\tF1: 10.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR:  8.33\tF1:  7.69\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-1:\tP:  3.57\tR: 10.00\tF1:  5.26\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 14.29\tF1:  9.52\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 22.47\tF1: 14.85\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 26.15\tF1: 15.57\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 22.47\tF1: 14.85\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 12.61\tF1: 11.80\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-l:\tP:  6.22\tR: 14.68\tF1:  8.74\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 19.76\tF1: 14.21\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  9.03\tF1:  7.47\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR: 11.24\tF1:  8.13\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  9.03\tF1:  7.47\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  3.93\tF1:  4.86\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-w:\tP:  3.57\tR:  6.31\tF1:  4.56\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  7.51\tF1:  6.89\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install py-rouge\n",
    "import rouge\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "for aggregator in ['Avg', 'Best']: #, 'Individual'\n",
    "    print('Evaluation with {}'.format(aggregator))\n",
    "    apply_avg = aggregator == 'Avg'\n",
    "    apply_best = aggregator == 'Best'\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n",
    "    prediction = pred_summaries\n",
    "    gold = test_summaries[0:10]\n",
    "\n",
    "    scores = evaluator.get_scores(prediction, gold)\n",
    "\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "#         if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "#             for hypothesis_id, results_per_ref in enumerate(results):\n",
    "#                 nb_references = len(results_per_ref['p'])\n",
    "#                 for reference_id in range(nb_references):\n",
    "#                     print('\\Pred_summary #{} & Gold_summary #{}: '.format(hypothesis_id, reference_id))\n",
    "#                     print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "#             print()\n",
    "        if apply_avg or apply_best:\n",
    "            print(prepare_results(results['p'], results['r'], results['f']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, text, predicted_summary):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + text, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_summary, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_result(text, encoder, decoder, tokenizer, max_encoder_seq_length, max_decoder_seq_length):\n",
    "def print_result(tex):\n",
    "    result, text, attention_plot = summarize(text, encoder, decoder, tokenizer, max_encoder_seq_length, max_decoder_seq_length)\n",
    "        \n",
    "    print('Input: {}'.format(text))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(text.split(' '))]\n",
    "    plot_attention(attention_plot, text.split(' '), result.split(' '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f5dc88ecef0>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaa = '''\n",
    "we have many of the old, old issue but the number had depleted there were not  enough books to allow us to use them regularly with the additional supply the books will be used more often  they arre a good old standby for gospel singing\n",
    "'''\n",
    "print_result(aaaa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = model.history['loss']\n",
    "# epoch = [i for i in range(epochs)]\n",
    "\n",
    "# plt.plot(epoch, loss) #, label=str(batch_size)\n",
    "    \n",
    "# plt.legend()\n",
    "# # plt.title('different batch size');\n",
    "# plt.xlabel('epoch'); \n",
    "# plt.ylabel('loss')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_6B_100d_file_path_name = \"../glove.6B/glove.6B.100d.txt\"\n",
    "\n",
    "# embeddings_index = dict()\n",
    "\n",
    "# f = open(glove_6B_100d_file_path_name)\n",
    "\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "    \n",
    "# f.close()\n",
    "# print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# # Tokenize\n",
    "# vocabulary_size = len(all_glove_words)\n",
    "# tokenizer = Tokenizer() #num_words= vocabulary_size\n",
    "# tokenizer.fit_on_texts(all_glove_words) \n",
    "\n",
    "# # create a weight matrix for words in training docs\n",
    "# embedding_matrix = np.zeros((vocabulary_size, 100)) \n",
    "# for word, index in tokenizer.word_index.items():\n",
    "#     if index > vocabulary_size - 1:\n",
    "#         break\n",
    "#     else:\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(data_path + \"\\model-40.hdf5\")\n",
    "# dummy_iters = 40\n",
    "# example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "#                                                      skip_step=1)\n",
    "# print(\"Training data:\")\n",
    "# for i in range(dummy_iters):\n",
    "#     dummy = next(example_training_generator.generate())\n",
    "# num_predict = 10\n",
    "# true_print_out = \"Actual words: \"\n",
    "# pred_print_out = \"Predicted words: \"\n",
    "# for i in range(num_predict):\n",
    "#     data = next(example_training_generator.generate())\n",
    "#     prediction = model.predict(data[0])\n",
    "#     predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "#     true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "#     pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "# print(true_print_out)\n",
    "# print(pred_print_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_summary_generation(reviews):\n",
    " \n",
    "#      # clean inputs\n",
    "#     cleaned = cleaning_data(reviews) \n",
    "#     # tokenize\n",
    "#     tokenized = tokenizer.texts_to_sequences([cleaned]) \n",
    "#     # padding\n",
    "#     sequence = pad_sequences(tokenized, maxlen = maxlen)  \n",
    "   \n",
    "#     # encode\n",
    "#     state = encoder_model.predict(sequence)\n",
    "\n",
    "\n",
    "#     # collect predictions\n",
    "#     output = list()\n",
    "#     for t in [answer_word2index['_B_'], answer_word2index['_U_']]:\n",
    "#         # predict next sequence\n",
    "#         target_seq = np.eye(n_class)[[t]]\n",
    "#         target_seq = target_seq[newaxis,:, : ]\n",
    "#         yhat, h, c = decoder_model.predict([target_seq] + state)\n",
    "#         # save first prediction\n",
    "#         output.append(yhat[0,0,:])\n",
    "#         # update state\n",
    "#         state = [h, c]\n",
    "#         # update target sequence\n",
    "#         target_seq = yhat\n",
    "    \n",
    "#     # select max probability words and decode\n",
    "#     output_sequence = [np.argmax(vector) for vector in np.array(output)]\n",
    "#     decoded = [answer_index2word[i] for i in output_sequence]\n",
    "\n",
    "#     # Remove anything after '_E_'        \n",
    "#     if \"_E_\" in decoded:\n",
    "#         end = decoded.index('_E_')\n",
    "#         answer = ' '.join(decoded[:end])\n",
    "#     else :\n",
    "#         answer = ' '.join(decoded[:])    \n",
    "#     # if no answer return choose random answer    \n",
    "#     if answer:\n",
    "#         result = answer\n",
    "#     else: \n",
    "#         result = np.random.random_integers(100)\n",
    "#     return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
