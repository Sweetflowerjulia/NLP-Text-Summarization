{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_en_samples:  4032305\n",
      "num_de_samples:  4032305\n"
     ]
    }
   ],
   "source": [
    "# load dataset       \n",
    "import gzip    \n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "path = '../../data/reviews_cleaned.json.gz'        \n",
    "\n",
    "# encoder and decoder\n",
    "reviews, summaries = list(), list()\n",
    "for data in parse(path):\n",
    "    reviews.append(data['review'])\n",
    "    # Appending SOS and EOS to target data (decoder)\n",
    "    summaries.append('SOS_ ' + data['summary'] + ' _EOS')\n",
    "\n",
    "all_data = reviews + summaries\n",
    "\n",
    "num_enc_samples = len(summaries)\n",
    "num_dec_samples = len(reviews)\n",
    "print('num_en_samples: ', num_enc_samples)\n",
    "print('num_de_samples: ', num_dec_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_samples =10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 755.06 s\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils\n",
    "# running time calculation\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=32768) # 2**15, same as t2t model\n",
    "tokenizer.fit_on_texts(all_data) \n",
    "vocab_size = min(32768, len(tokenizer.word_index) + 1)\n",
    "\n",
    "# encoder source data\n",
    "source_token = tokenizer.texts_to_sequences(reviews)\n",
    "max_encoder_seq_length = 525 #max([len(sentence) for sentence in source_token])\n",
    "source_padded = pad_sequences(source_token, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "# decoder target data\n",
    "target_token = tokenizer.texts_to_sequences(summaries)\n",
    "# max_decoder_seq_length = max([len(sentence) for sentence in target_token])\n",
    "# target_padded = pad_sequences(target_token, maxlen=max_decoder_seq_length, padding = \"post\")\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} s'.format(round(stop - start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  32768\n",
      "max_encoder_seq_length:  525\n"
     ]
    }
   ],
   "source": [
    "print('vocab_size: ',vocab_size)\n",
    "print('max_encoder_seq_length: ', max_encoder_seq_length)\n",
    "# print('max_decoder_seq_length: ', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4032305, 525)\n",
      "(4032305,)\n",
      "(4032305,)\n"
     ]
    }
   ],
   "source": [
    "n_samples = num_enc_samples #100\n",
    "\n",
    "# prepare data for the LSTM\n",
    "decoder_input_data, decoder_target_data = list(), list()\n",
    "for i in range(n_samples):\n",
    "    dec_input = target_token[i][:-1] # dec_input = target_padded[i][:-1]\n",
    "    target = target_token[i][1:] # target = target_padded[i][1:]\n",
    "    decoder_input_data.append(dec_input) #tar2_encoded\n",
    "    decoder_target_data.append(target)\n",
    "\n",
    "encoder_input_data = source_padded #np.array(X1)\n",
    "decoder_input_data = np.array(decoder_input_data)\n",
    "decoder_target_data = np.array(decoder_target_data)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, None, 300)    9830400     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, None, 300)    0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, None, 300)    9830400     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm (CuDNNLSTM)          (None, None, 128)    220160      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 300)    0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_1 (CuDNNLSTM)        [(None, 128), (None, 132096      cu_dnnlstm[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)        (None, None, 128)    220160      time_distributed_1[0][0]         \n",
      "                                                                 cu_dnnlstm_1[0][1]               \n",
      "                                                                 cu_dnnlstm_1[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnnlstm_3 (CuDNNLSTM)        [(None, None, 128),  132096      cu_dnnlstm_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 32768)  4227072     cu_dnnlstm_3[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 24,592,384\n",
      "Trainable params: 24,592,384\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, CuDNNLSTM, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "vec_len = 300\n",
    "n_units = 128\n",
    "dropout_rate = 0.3   \n",
    "\n",
    "# Input layer of the encoder :\n",
    "encoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the encoder :\n",
    "encoder_embedding = Embedding(input_dim = vocab_size, output_dim = vec_len)(encoder_input)\n",
    "encoder_dropout   = (TimeDistributed(Dropout(rate = dropout_rate)))(encoder_embedding)\n",
    "encoder_LSTM      = CuDNNLSTM(n_units, return_sequences=True)(encoder_dropout)\n",
    "\n",
    "# Output layer of the encoder \n",
    "encoder_LSTM2_layer = CuDNNLSTM(n_units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_LSTM2_layer(encoder_LSTM)\n",
    "\n",
    "# encoder states\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# decoder\n",
    "# Input layer of the decoder :\n",
    "decoder_input = Input(shape=(None,))\n",
    "\n",
    "# Hidden layers of the decoder :\n",
    "decoder_embedding_layer = Embedding(input_dim = vocab_size, output_dim = vec_len)\n",
    "decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "decoder_dropout_layer = (TimeDistributed(Dropout(rate = dropout_rate)))\n",
    "decoder_dropout = decoder_dropout_layer(decoder_embedding)\n",
    "\n",
    "decoder_LSTM_layer = CuDNNLSTM(n_units, return_sequences=True)\n",
    "decoder_LSTM = decoder_LSTM_layer(decoder_dropout, initial_state = encoder_states)\n",
    "\n",
    "decoder_LSTM_2_layer = CuDNNLSTM(n_units, return_sequences=True, return_state=True)\n",
    "decoder_LSTM_2,_,_ = decoder_LSTM_2_layer(decoder_LSTM)\n",
    "\n",
    "# Output layer of the decoder :\n",
    "decoder_dense = Dense(vocab_size, activation='linear', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_LSTM_2)\n",
    "\n",
    "\n",
    "# Define encoder model\n",
    "encoder_model = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Define training model\n",
    "model = Model([encoder_input, decoder_input], decoder_outputs)\n",
    "\n",
    "# Define decoder model\n",
    "dec_h = Input(shape=(n_units,))\n",
    "dec_c = Input(shape=(n_units,))\n",
    "dec_states_inputs = [dec_h, dec_c]\n",
    "decoder_outputs, state_h, state_c = decoder_LSTM_2_layer(decoder_embedding, initial_state=dec_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model([decoder_input] + dec_states_inputs, [decoder_outputs] + decoder_states)\n",
    "\n",
    "model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3629074 samples, validate on 403231 samples\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-38e9b88296f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m           \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m           callbacks = callbacks_list)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3059\u001b[0m         \u001b[0mtensor_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3060\u001b[0m         array_vals.append(np.asarray(value,\n\u001b[0;32m-> 3061\u001b[0;31m                                      dtype=tensor_type.as_numpy_dtype))\n\u001b[0m\u001b[1;32m   3062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3063\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \"\"\"\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "learning_rate=0.01    \n",
    "batch_size    = 32#64   \n",
    "epochs        = 1#30  \n",
    "\n",
    "# define loss function: use sparse_softmax_cross_entropy_with_logits\n",
    "def sparse_loss(targets, decoder_outputs):\n",
    "    return tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=decoder_outputs)\n",
    "\n",
    "# Define a checkpoint callback \n",
    "checkpoint_name = './checkpoint/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# Run training\n",
    "decoder_target = tf.placeholder(dtype='int32', shape=(None, vocab_size))   \n",
    "\n",
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "                loss=sparse_loss,\n",
    "                target_tensors=[decoder_target])\n",
    "\n",
    "model.fit([encoder_input_data,\n",
    "               decoder_input_data],\n",
    "               decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.1,\n",
    "          callbacks = callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = model.history['loss']\n",
    "# epoch = [i for i in range(epochs)]\n",
    "\n",
    "# plt.plot(epoch, loss) #, label=str(batch_size)\n",
    "    \n",
    "# plt.legend()\n",
    "# # plt.title('different batch size');\n",
    "# plt.xlabel('epoch'); \n",
    "# plt.ylabel('loss')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(data_path + \"\\model-40.hdf5\")\n",
    "# dummy_iters = 40\n",
    "# example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "#                                                      skip_step=1)\n",
    "# print(\"Training data:\")\n",
    "# for i in range(dummy_iters):\n",
    "#     dummy = next(example_training_generator.generate())\n",
    "# num_predict = 10\n",
    "# true_print_out = \"Actual words: \"\n",
    "# pred_print_out = \"Predicted words: \"\n",
    "# for i in range(num_predict):\n",
    "#     data = next(example_training_generator.generate())\n",
    "#     prediction = model.predict(data[0])\n",
    "#     predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "#     true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "#     pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "# print(true_print_out)\n",
    "# print(pred_print_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_summary_generation(reviews):\n",
    " \n",
    "#      # clean inputs\n",
    "#     cleaned = cleaning_data(reviews) \n",
    "#     # tokenize\n",
    "#     tokenized = tokenizer.texts_to_sequences([cleaned]) \n",
    "#     # padding\n",
    "#     sequence = pad_sequences(tokenized, maxlen = maxlen)  \n",
    "   \n",
    "#     # encode\n",
    "#     state = encoder_model.predict(sequence)\n",
    "\n",
    "\n",
    "#     # collect predictions\n",
    "#     output = list()\n",
    "#     for t in [answer_word2index['_B_'], answer_word2index['_U_']]:\n",
    "#         # predict next sequence\n",
    "#         target_seq = np.eye(n_class)[[t]]\n",
    "#         target_seq = target_seq[newaxis,:, : ]\n",
    "#         yhat, h, c = decoder_model.predict([target_seq] + state)\n",
    "#         # save first prediction\n",
    "#         output.append(yhat[0,0,:])\n",
    "#         # update state\n",
    "#         state = [h, c]\n",
    "#         # update target sequence\n",
    "#         target_seq = yhat\n",
    "    \n",
    "#     # select max probability words and decode\n",
    "#     output_sequence = [np.argmax(vector) for vector in np.array(output)]\n",
    "#     decoded = [answer_index2word[i] for i in output_sequence]\n",
    "\n",
    "#     # Remove anything after '_E_'        \n",
    "#     if \"_E_\" in decoded:\n",
    "#         end = decoded.index('_E_')\n",
    "#         answer = ' '.join(decoded[:end])\n",
    "#     else :\n",
    "#         answer = ' '.join(decoded[:])    \n",
    "#     # if no answer return choose random answer    \n",
    "#     if answer:\n",
    "#         result = answer\n",
    "#     else: \n",
    "#         result = np.random.random_integers(100)\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install py-rouge\n",
    "\n",
    "# import rouge\n",
    "\n",
    "# def prepare_results(p, r, f):\n",
    "#     return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "# for aggregator in ['Avg', 'Best', 'Individual']:\n",
    "#     print('Evaluation with {}'.format(aggregator))\n",
    "#     apply_avg = aggregator == 'Avg'\n",
    "#     apply_best = aggregator == 'Best'\n",
    "\n",
    "#     evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "#                            max_n=4,\n",
    "#                            limit_length=True,\n",
    "#                            length_limit=100,\n",
    "#                            length_limit_type='words',\n",
    "#                            apply_avg=apply_avg,\n",
    "#                            apply_best=apply_best,\n",
    "#                            alpha=0.5, # Default F1_score\n",
    "#                            weight_factor=1.2,\n",
    "#                            stemming=True)\n",
    "\n",
    "#     hypothesis_1 = \"King Norodom Sihanouk has declined requests to chair a summit of Cambodia 's top political leaders , saying the meeting would not bring any progress in deadlocked negotiations to form a government .\\nGovernment and opposition parties have asked King Norodom Sihanouk to host a summit meeting after a series of post-election negotiations between the two opposition groups and Hun Sen 's party to form a new government failed .\\nHun Sen 's ruling party narrowly won a majority in elections in July , but the opposition _ claiming widespread intimidation and fraud _ has denied Hun Sen the two-thirds vote in parliament required to approve the next government .\\n\"\n",
    "#     references_1 = [\"Prospects were dim for resolution of the political crisis in Cambodia in October 1998.\\nPrime Minister Hun Sen insisted that talks take place in Cambodia while opposition leaders Ranariddh and Sam Rainsy, fearing arrest at home, wanted them abroad.\\nKing Sihanouk declined to chair talks in either place.\\nA U.S. House resolution criticized Hun Sen's regime while the opposition tried to cut off his access to loans.\\nBut in November the King announced a coalition government with Hun Sen heading the executive and Ranariddh leading the parliament.\\nLeft out, Sam Rainsy sought the King's assurance of Hun Sen's promise of safety and freedom for all politicians.\",\n",
    "#                     \"Cambodian prime minister Hun Sen rejects demands of 2 opposition parties for talks in Beijing after failing to win a 2/3 majority in recent elections.\\nSihanouk refuses to host talks in Beijing.\\nOpposition parties ask the Asian Development Bank to stop loans to Hun Sen's government.\\nCCP defends Hun Sen to the US Senate.\\nFUNCINPEC refuses to share the presidency.\\nHun Sen and Ranariddh eventually form a coalition at summit convened by Sihanouk.\\nHun Sen remains prime minister, Ranariddh is president of the national assembly, and a new senate will be formed.\\nOpposition leader Rainsy left out.\\nHe seeks strong assurance of safety should he return to Cambodia.\\n\",\n",
    "#                     ]\n",
    "\n",
    "#     hypothesis_2 = \"China 's government said Thursday that two prominent dissidents arrested this week are suspected of endangering national security _ the clearest sign yet Chinese leaders plan to quash a would-be opposition party .\\nOne leader of a suppressed new political party will be tried on Dec. 17 on a charge of colluding with foreign enemies of China '' to incite the subversion of state power , '' according to court documents given to his wife on Monday .\\nWith attorneys locked up , harassed or plain scared , two prominent dissidents will defend themselves against charges of subversion Thursday in China 's highest-profile dissident trials in two years .\\n\"\n",
    "#     references_2 = \"Hurricane Mitch, category 5 hurricane, brought widespread death and destruction to Central American.\\nEspecially hard hit was Honduras where an estimated 6,076 people lost their lives.\\nThe hurricane, which lingered off the coast of Honduras for 3 days before moving off, flooded large areas, destroying crops and property.\\nThe U.S. and European Union were joined by Pope John Paul II in a call for money and workers to help the stricken area.\\nPresident Clinton sent Tipper Gore, wife of Vice President Gore to the area to deliver much needed supplies to the area, demonstrating U.S. commitment to the recovery of the region.\\n\"\n",
    "\n",
    "#     all_hypothesis = [hypothesis_1, hypothesis_2]\n",
    "#     all_references = [references_1, references_2]\n",
    "\n",
    "#     scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "\n",
    "#     for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "#         if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "#             for hypothesis_id, results_per_ref in enumerate(results):\n",
    "#                 nb_references = len(results_per_ref['p'])\n",
    "#                 for reference_id in range(nb_references):\n",
    "#                     print('\\tHypothesis #{} & Reference #{}: '.format(hypothesis_id, reference_id))\n",
    "#                     print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "#             print()\n",
    "#         else:\n",
    "#             print(prepare_results(results['p'], results['r'], results['f']))\n",
    "#     print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
