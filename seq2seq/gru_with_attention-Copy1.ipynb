{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRU with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import tensorflow \n",
    "tensorflow.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/6c/bbbf3452cd5c8ed8e6cb51d37e06ebea3113d347085a59a21f19ee76c8eb/scikit_learn-0.21.2-cp35-cp35m-manylinux1_x86_64.whl (6.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.6MB 13.8MB/s ta 0:00:011    66% |█████████████████████▎          | 4.4MB 44.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.16.1)\n",
      "Collecting joblib>=0.11 (from scikit-learn)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/c1/50a758e8247561e58cb87305b1e90b171b8c767b15b12a1734001f41d356/joblib-0.13.2-py2.py3-none-any.whl (278kB)\n",
      "\u001b[K    100% |████████████████████████████████| 286kB 58.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.5/dist-packages (from scikit-learn) (1.2.1)\n",
      "Installing collected packages: joblib, scikit-learn\n",
      "Successfully installed joblib-0.13.2 scikit-learn-0.21.2\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_en_samples:  4032305\n",
      "num_de_samples:  4032305\n"
     ]
    }
   ],
   "source": [
    "# load dataset       \n",
    "import gzip    \n",
    "\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'rb')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "# path = '../../data/reviews_cleaned.json.gz' \n",
    "path = '../../data/reviews.json.gz' \n",
    "\n",
    "# n=1\n",
    "# encoder and decoder\n",
    "reviews, summaries = list(), list()\n",
    "for data in parse(path):\n",
    "    try:        \n",
    "        if data['summary'] in data['review']:\n",
    "            continue\n",
    "        if ''.join(data['summary'].strip().split()[:3]) ==  ''.join(data['summary'].strip().split()[:3]):\n",
    "            continue        \n",
    "        \n",
    "        reviews.append(data['review'])\n",
    "        # Appending SOS and EOS to target data (decoder)\n",
    "        summaries.append('sos' + data['summary'] + 'eos')\n",
    "#     n+=1\n",
    "#     if n>2000:\n",
    "#         break\n",
    "    except:\n",
    "        print(data)\n",
    "all_data = reviews + summaries\n",
    "\n",
    "num_enc_samples = len(summaries)\n",
    "num_dec_samples = len(reviews)\n",
    "print('num_en_samples: ', num_enc_samples)\n",
    "print('num_de_samples: ', num_dec_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# tokenizer = pickle.load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test data split\n",
    "# train_X, test_X, train_Y, test_Y = train_test_split(source_padded, target_padded, test_size=0.01)\n",
    "train_reviews, test_reviews, train_summaries, test_summaries = train_test_split(reviews, summaries, test_size=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 824.66 s\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import utils\n",
    "# running time check\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "tokenizer = Tokenizer(num_words=16384) #16384#2**14 #32768# 2**15, same as t2t model\n",
    "tokenizer.fit_on_texts(all_data) \n",
    "vocab_size = 16384 #len(tokenizer.word_index) #min(10000, len(tokenizer.word_index) + 1)\n",
    "\n",
    "# encoder source data\n",
    "source_token = tokenizer.texts_to_sequences(train_reviews)\n",
    "max_encoder_seq_length = 525 #max([len(sentence) for sentence in source_token])\n",
    "source_padded = pad_sequences(source_token, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "# decoder target data\n",
    "target_token = tokenizer.texts_to_sequences(train_summaries)\n",
    "max_decoder_seq_length = max([len(sentence) for sentence in target_token])\n",
    "target_padded = pad_sequences(target_token, maxlen=max_decoder_seq_length, padding = \"post\")\n",
    "\n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} s'.format(round(stop - start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16384"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRU with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import CuDNNGRU, Dense, Input, Embedding, TimeDistributed, Flatten, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = source_padded\n",
    "train_Y = target_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(train_X)\n",
    "BATCH_SIZE = 32#64\n",
    "N_BATCH = BUFFER_SIZE//BATCH_SIZE\n",
    "embedding_dim = 100 #256\n",
    "units = 512 #1024\n",
    "vocab_size = 16384\n",
    "vocab_inp_size = vocab_size \n",
    "vocab_tar_size = vocab_size \n",
    "dec_units = train_Y.shape[1]\n",
    "enc_units = train_X.shape[1]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((train_X, train_Y)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)   \n",
    "        encoder_dropout   = (TimeDistributed(Dropout(rate = 0.2)))(x) #dropout_rate\n",
    "        gru_layer_1 = CuDNNGRU(self.enc_units, return_sequences=True) \\\n",
    "                            (encoder_dropout, initial_state = hidden)\n",
    "        gru_layer_2 = CuDNNGRU(self.enc_units, return_state=True)\n",
    "        output, state = gru_layer_2(gru_layer_1)        \n",
    "        return output, state\n",
    "    \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)      \n",
    "        \n",
    "#         self.gru = gru(self.dec_units)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "        # used for attention\n",
    "        self.W1 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.W2 = tf.keras.layers.Dense(self.dec_units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        \n",
    "        # hidden shape == (batch_size, hidden size)\n",
    "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # perform addition to calculate the score\n",
    "        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n",
    "        \n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n",
    "        \n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        \n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * enc_output\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        \n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        \n",
    "        # passing the concatenated vector to the GRU\n",
    "        gru_dec_layer_1 = CuDNNGRU(self.dec_units,return_sequences=True)(x)     \n",
    "        gru_dec_layer_2 = CuDNNGRU(self.dec_units,return_sequences=True, return_state=True)\n",
    "        output, state = gru_dec_layer_2(gru_dec_layer_1)   \n",
    "        \n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        \n",
    "        # output shape == (batch_size * 1, vocab)\n",
    "        x = self.fc(output)\n",
    "        \n",
    "        return x, state, attention_weights\n",
    "        \n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.dec_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './checkpoints_v3/'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/data/ops/iterator_ops.py:532: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1 Batch 0 Loss 1.6192\n",
      "Epoch 1 Batch 100 Loss 1.7391\n",
      "Epoch 1 Batch 200 Loss 1.8297\n",
      "Epoch 1 Batch 300 Loss 1.6525\n",
      "Epoch 1 Batch 400 Loss 1.6939\n",
      "Epoch 1 Batch 500 Loss 1.7272\n",
      "Epoch 1 Batch 600 Loss 1.5917\n",
      "Epoch 1 Batch 700 Loss 1.5265\n",
      "Epoch 1 Batch 800 Loss 1.5971\n",
      "Epoch 1 Batch 900 Loss 1.5877\n",
      "Epoch 1 Batch 1000 Loss 1.4587\n",
      "Epoch 1 Batch 1100 Loss 1.5660\n",
      "Epoch 1 Batch 1200 Loss 1.5420\n",
      "Epoch 1 Batch 1300 Loss 1.6159\n",
      "Epoch 1 Batch 1400 Loss 1.5123\n",
      "Epoch 1 Batch 1500 Loss 1.5129\n",
      "Epoch 1 Batch 1600 Loss 1.5976\n",
      "Epoch 1 Batch 1700 Loss 1.6570\n",
      "Epoch 1 Batch 1800 Loss 1.5752\n",
      "Epoch 1 Batch 1900 Loss 1.3501\n",
      "Epoch 1 Batch 2000 Loss 1.4758\n",
      "Epoch 1 Batch 2100 Loss 1.5511\n",
      "Epoch 1 Batch 2200 Loss 1.3209\n",
      "Epoch 1 Batch 2300 Loss 1.3935\n",
      "Epoch 1 Batch 2400 Loss 1.5293\n",
      "Epoch 1 Batch 2500 Loss 1.4261\n",
      "Epoch 1 Batch 2600 Loss 1.5182\n",
      "Epoch 1 Batch 2700 Loss 1.3979\n",
      "Epoch 1 Batch 2800 Loss 1.4831\n",
      "Epoch 1 Batch 2900 Loss 1.2923\n",
      "Epoch 1 Batch 3000 Loss 1.3580\n",
      "Epoch 1 Batch 3100 Loss 1.3256\n",
      "Epoch 1 Batch 3200 Loss 1.2145\n",
      "Epoch 1 Batch 3300 Loss 1.3345\n",
      "Epoch 1 Batch 3400 Loss 1.3692\n",
      "Epoch 1 Batch 3500 Loss 1.3126\n",
      "Epoch 1 Batch 3600 Loss 1.2642\n",
      "Epoch 1 Batch 3700 Loss 1.2396\n",
      "Epoch 1 Batch 3800 Loss 1.2101\n",
      "Epoch 1 Batch 3900 Loss 1.3574\n",
      "Epoch 1 Batch 4000 Loss 1.3903\n",
      "Epoch 1 Batch 4100 Loss 1.3216\n",
      "Epoch 1 Batch 4200 Loss 1.2766\n",
      "Epoch 1 Batch 4300 Loss 1.4021\n",
      "Epoch 1 Batch 4400 Loss 1.2213\n",
      "Epoch 1 Batch 4500 Loss 1.3713\n",
      "Epoch 1 Batch 4600 Loss 1.3915\n",
      "Epoch 1 Batch 4700 Loss 1.3256\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6d0128e755ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m# passing enc_output to the decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdec_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-0629e28c1618>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden, enc_output)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mgru_dec_layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCuDNNGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mgru_dec_layer_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCuDNNGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgru_dec_layer_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgru_dec_layer_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;31m# output shape == (batch_size * 1, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0;31m# We must set self.built since user defined build functions are not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;31m# constrained to set self.built.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1601\u001b[0m     \u001b[0;31m# Only call `build` if the user has manually overridden the build method.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1602\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_is_default'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1603\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1604\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1605\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/layers/cudnn_recurrent.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_initializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mregularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurrent_regularizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         constraint=self.recurrent_constraint)\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     self.bias = self.add_weight(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/training/checkpointable/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    605\u001b[0m     new_variable = getter(\n\u001b[1;32m    606\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, partition_info, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       aggregation=aggregation)\n\u001b[0m\u001b[1;32m    146\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         aggregation=aggregation)\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m   def _variable_v2_call(cls,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                         aggregation=VariableAggregation.NONE):\n\u001b[1;32m    154\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2486\u001b[0m         \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariable_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2488\u001b[0;31m         import_scope=import_scope)\n\u001b[0m\u001b[1;32m   2489\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m     return variables.RefVariable(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    215\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint)\u001b[0m\n\u001b[1;32m    292\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m   \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint)\u001b[0m\n\u001b[1;32m    404\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m    408\u001b[0m           self._handle = eager_safe_variable_handle(\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m       init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n\u001b[0;32m--> 127\u001b[0;31m           shape, dtype=dtype, partition_info=partition_info)\n\u001b[0m\u001b[1;32m    128\u001b[0m       \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0muse_resource\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/init_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype, partition_info)\u001b[0m\n\u001b[1;32m    585\u001b[0m     \u001b[0;31m# Make Q uniform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m     \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 587\u001b[0;31m     \u001b[0mq\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    588\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_cols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m       \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix_transpose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mbinary_op_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 810\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    811\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6068\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_with_trailing_slash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m               if self._old_name else name_with_trailing_slash)\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mname_scope_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# running time check\n",
    "import timeit\n",
    "start = timeit.default_timer()\n",
    "\n",
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        loss = 0\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            enc_output, enc_hidden = encoder(inp, hidden)\n",
    "            \n",
    "            dec_hidden = enc_hidden\n",
    "            \n",
    "            dec_input = tf.expand_dims([tokenizer.word_index['sos']] * BATCH_SIZE, 1)\n",
    "            \n",
    "            # Teacher forcing - feeding the target as the next input\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                # passing enc_output to the decoder\n",
    "                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "                \n",
    "                loss += loss_function(targ[:, t], predictions)\n",
    "                \n",
    "                # using teacher forcing\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        variables = encoder.variables + decoder.variables\n",
    "        \n",
    "        gradients = tape.gradient(loss, variables)\n",
    "        \n",
    "        optimizer.apply_gradients(zip(gradients, variables))\n",
    "        \n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                        total_loss / N_BATCH))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "stop = timeit.default_timer()\n",
    "print('Time: {} s'.format(round(stop - start,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2word = {idx: word for word, idx in tokenizer.word_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def summarize(text, encoder, decoder, tokenizer, max_encoder_seq_length, max_decoder_seq_length):\n",
    "def summarize(text):\n",
    "    attention_plot = np.zeros((max_decoder_seq_length, max_encoder_seq_length))\n",
    "    \n",
    "#     text = preprocess(text)\n",
    "    inputs = tokenizer.texts_to_sequences([text])\n",
    "    inputs = pad_sequences(inputs, maxlen=max_encoder_seq_length, padding = \"post\")\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "#     print(inputs)\n",
    "#     print(hidden)\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['sos']], 0)\n",
    "\n",
    "    for t in range(max_decoder_seq_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n",
    "        \n",
    "        # storing the attention weights to plot later on\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result += idx2word[predicted_id] + ' '\n",
    "\n",
    "        if idx2word[predicted_id] == 'eos':\n",
    "            return result, text, attention_plot\n",
    "        \n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, text, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3fdcc83e33ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_reviews\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_plot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msummarize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mpred_summaries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summary: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-fa86cde29307>\u001b[0m in \u001b[0;36msummarize\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#     print(inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     print(hidden)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mdec_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    590\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;31m# Eager execution on data tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-467429e9ce95>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x, hidden)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mencoder_dropout\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTimeDistributed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#dropout_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mgru_layer_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCuDNNGRU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                             \u001b[0;34m(\u001b[0m\u001b[0mencoder_dropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0minput_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   6068\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname_with_trailing_slash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m               if self._old_name else name_with_trailing_slash)\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0mname_scope_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6072\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pred_summaries = []\n",
    "for review in test_reviews[0:10]:\n",
    "    summary, review, attention_plot = summarize(review)\n",
    "    pred_summaries.append(summary)\n",
    "    print(\"summary: \", summary, \"\\n\")\n",
    "    print(\"review: \", review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  and and and and and and and and of and and and and and to and and the and the and and and and and and and and  \n",
      "\n",
      "review:  the archetypical american novel features a solitary protagonist undertaking an odyssey in which heshe confronts both physical and moral challenges and through which heshe emerges with a renewed spirit, transformed by the crucible of confrontations with adversaries, both real and imagined  sara gruens engaging water for elephants is an eminently american work, set in the depths of the depression and featuring a brokenhearted young man whose unplanned existential leap of faith catapults him into a chaotic and unknown way of life  jacob jankowski discovers his untapped reservoir of courage, conviction and compassion, and in so doing, he, in every manner of the american definition of the word, emerges as a hero  water for elephants is a triumph    of a writer fully in control of her craft, of a character resolute in his determination to discover life and of a depiction of ideas that truly embody our national characterthe innocent jacob, on the threshold of graduating from cornell veterinary school and returning to the midwest to practice medicine with his father, is shattered by the sudden accidental deaths of his parents  unmoored from his past and unable to focus on his final examination, he abandons his former life the moment he jumps on board a passing train  to his astonishment, he has leaped into the midst of a traveling circus, and, thanks to the flinty care of several roustabouts, manages to gain employment with the tawdry benzini brothers most spectacular show on earth, a circus dominated by a sadistic ringmaster and populated by a cast of characters, admirable and evil in equal proportions  in a manner of weeks, jacob immerses himself as the circus vet, forms abiding friendships, falls is love and faces moral dilemmas that tie his conscience into gordian knotssome critics have likened jacob to his biblical counterpart and praised the novel for its allusions to the bible  in my opinion, jacob mirrors a less sanctified model, huckleberry finn  like huck, jacob flees familiar surroundings, flaunts social norms, befriends outcasts and risks his personal safety in order to adhere to a hardearned sense of conscience and social responsibility  both huck and jacob confront mendacious adversaries, rejoice in life on the lam and reinvent themselves  like twain, sarah gruen writes with inordinate respect about the dispossessed and with painful awareness about the inexplicable capacity for the human heart to do well in the midst of spiritual darkness  as twain has written    and as gruen affirms    the source of humor is paineven the title bespeaks our capacity for misrepresentation  the elderly jacob, shunted away by his family in a assistedliving nursing home, excoriates a new resident who has claimed to have carried water in buckets for elephants when he was a youngster  aware of an elephants enormous drinking capacity, jacob knows a lie when he hears one  and well he should  the entire purpose of the traveling circus is deception, an enterprise where rubes are separated from their money by gaping at mangy animals, staring at sideshow freaks many of whom are frauds or being serviced by exotic dancers  falsehoods, illusions and duplicity are the moral big tent under which circus performers enchant depression men and women hungry for escape  the central conflict of the novel rotates around jacobs involvement with a married woman and the peculiarly disturbing aspects of her husbands mental state  this conflict is rife with secrecy, duplicity and tormented twisting of ideas of right and wrongit is no surprise that this novel is a bestseller  written with extraordinary narrative drive, populated by characters who truly represent our national character and animated by a young man whose journey for selfdiscovery is our own, water for elephants intimately involves us in jacob jankowskis search for meaning\n",
      "summary:  and and this and this and and and the of the and and and and and the of to and and and the and to and and the  \n",
      "\n",
      "review:  so tommy and tuppence were married, she chanted, and lived happily ever afterwards and six years later they were still living together happily ever afterwardssome stories were fairly straightforward and i was able to guess the end which turned out right like the unbreakable alibi  yippeeee the bantering between tommy and tuppence is amusing they are two people who complement each other while tommy is serious and cautious, tuppence is impulsive and hopelessly optimistici can rub my hands together when i am pleased for polton,tommy and tuppence, who were first introduced in the book the secret adversary,are now happily married but tuppence is restless and bored she craves for some adventurei wish something would happenas if her wish is granted, they are visited by mr carter who works for some unnamed secret government agency and is their old friend he proposes that they take over the international detective agency and tommy pose as its owner mr theodore blunt they are to intercept secret messages from russia meanwhile they are to run the detective agency as their own and solve any cases that come their waythe book contains short accounts of different cases that come their way with the secret russian angle running in the background in all the storiesagatha christie wrote these stories as parodies on the famous detectives of 1920s like sherlock holmes, john thorndyke, father brown, tommy mccarty and even poirot the stories present some much unexpected turns like in the case of the missing lady, i could never have guessed the end solution it will be too marvelouswe will hunt down murderers, and discover the missing family jewels, and find people who have disappeared and detect embezzlersi always like your cheery optimism, tuppence you seem to have no doubt whatever that you have talent to exercisewell i have read every detective novel that has been published in the last ten yearsthey are a fun loving couple who enjoy their life as well as the cases leaving the reader feeling giddy and delightedi especially liked their efforts in imitating their favorite detective inspirations likea nice little syringe and a bottle labelled cocaine for sherlock holmesand it was hilarious when they sort of overdid the sherlock holmes or polton or father brown mannerism copyinghe picked up a violin which lay on the table, and drew the bow once or twice across the strings tuppence ground her teeth and even the explorer blanched  p dalbert, first introduced in the secret adversary, is another great character who apart from being a domestic help at home is also appointed by tommy and tuppence beresford as their assistant at their agency  he is efficient, yet eccentric and likes to change his behavior, mannerisms to suit a situation based on movies he has watched likea tall lad of fifteen who seemed undecided as to whether he was a footman or a page boy inquired in a truly magnificent mannerare you at home, madam the front door bell has just rungi wish albert would not go to the pictureshe is copying a long island butler now thank goodness i have cured him of asking for peoples cards and bringing them to me on a salver or albert relinquished the role of a long island butler, and took  up that of office boy, a part which he played to perfection a paper bag of sweets, inky hands, and a tousled head was his conception of the charactermy words are inconsequential when it comes to commenting on agatha christie she is aptly described as the queen of crime and she does not let us down in this book toothe book seems like a casual, nostress venture from her and therefore is a perfect light, entertaining readthe short stories though dealing with different cases are connected and should be read in the order they are presented in the bookalthough not the best from tommy and tuppence, the stories are simple in style but very expertly written and exercise our little grey cells vigorously i give it a 38 out of 5 and urge all to read and fall in love with tommy and tuppence, a great pair of detectives who are courageous, impulsive, loyal and not afraid to risk their lives following the slogan any case solved in 24 hours they will and can solve any case no matter how bizarre that comes their wayprepare to test all your mental faculties in this beautiful short story collection with these great partners in crime this review is also available on njkinnyblogspotin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  and to this of of the and the the of and and and the the the and of and to and and and and of and and the  \n",
      "\n",
      "review:  flash jumps around a bib in this adventure, but in the west his manner improved form deplorable to tolerably despicablei do not think it would be amused with the oh show tune , i want to be an indian too , would not you  like a sioux but he honest shows a slice of what the encroaching society effect were on the fictional charterers and there action and reaction to the new world  as far as the locals, heck it was their street corner so i will not make any judgment\n",
      "summary:  and and the and and and and the and and the and of and the this the and and and and and the and and the and and  \n",
      "\n",
      "review:  a song of fire and ice is one of the few fantasy series which becomes more enjoyable with each novel released  a dance with dragons was a long time coming but the wait was worth itabout half of the main characters were missing from a feast for crows, so i thought that dance would show the same period of time from different viewpoints instead of advancing the story this is not the caseabout 14th of dance overlaps with the previous novel  after that the overall plot advances and takes some interesting turns characters absent from feast such as jon snow and tyrion tell the majority of dance, but jaime, arya, and others also have a few chaptersmartin has not dropped the ball a dance with dragons is just as good as game of thrones, and even more fun to read\n",
      "summary:  and and the of of and and and and and the and and this to and and and and of the and and of and and and and  \n",
      "\n",
      "review:  this collection of quality photos taken during the christmas season shows americans celebrating in many different ways santa claus appears in many places and in many contexts, emerging from the hawaiian surf to the traditional handing off of a wrapped gift there are also photos of christmas performances, from concerts and ballets to nativity scenes most striking are the beautiful photos of natures winter splendor winter weather can be horrific, but there is nothing prettier that a fresh snowfall in cold crisp weathera celebration of the christmas season with all the splendor and shopping stress, this collection of photos truly captures americans in all aspects of the christmas spirit\n",
      "summary:  and and the and and the and this and and the and and and the and and and and of and the and the to and and the  \n",
      "\n",
      "review:  i have been reading clive cussler novels for many years and i can call myself a true fan of cusslers pop fiction of course, cussler does not aspire to the status of belles lettres highclass literature worthy of study for its aesthetic value, but this author excels in his chosen genre of adventure action novel and of books with plots that relate at least in part to seafaring exploits or maritime curiosities sahara is among the best of cusslers novels, among my very favourite of his exciting epics only the mediterranean caper, iceberg, and raise the titantic are as thrilling as sahara is there are few films based on cussler novels, the only two of which i know being cinematic treatments of sahara, a great boxoffice success, and raise the titanic a novel, hence the film too, whose plausibility suffers in retrospect only due to the discovery of the titanic wreck well after cussler had written his novel and after the film industry made a cinematic treatment of it i read sahara many years before the film came out both the novel and the film are supercussler researches his subjects exceedingly well the tuaregs in sahara are true to the life, religious beliefs and practices, and lore of this peculiar muslim sect in mali eg, whose men, rather than their women, wear an allencompassing veil cusslers experience at sea, especially in exploring wrecks and naval mysteries, shows in all of his novels having been in the us navy myself even having consorted for a few months with the navy seals during the kennedy presidency in the early 1960s, i can appreciate the authenticity of cusslers naval and maritime lore as he depicts it in sahara it takes the form of subplots that entail some expert manoeuvering under dangerous conditions of a small river craft vessel as well as the discovery of a marooned confederacy warshipthe reader cannot go wrong with most of cusslers novels especially those which were published before the present 21st century began sahara makes a good point of departure in exploring the dirk pitt pop classics of cusslers famously enjoyable 20th century output if you liked the film treatment, you probably will enjoy the novel even more, if you are an avid reader of action fiction\n",
      "summary:  and and the and the and the and and and the and and the and and to of and and and and and and and and and and  \n",
      "\n",
      "review:  i love books like this i saw this one, browsed through it a bit and bought it right away because it looked very good  it was i can confidently say that this is one of the best books of its type the author explains, in very clear language, the nature of probability and its use in understanding some of the many areas in everyday life that could otherwise remain very obscure or misunderstood the topics covered include gambling games and methods, the ways casinos operate this may be very surprising to many, card games and strategies, preelection polls, certain game shows, the war against spam, weather prediction, and many other areas where probability plays a key role the author, an expert in the field, writes very well and in such an engaging and often humorous style that the book is almost impossible to put down those who read this book will understand a bit more about how the world that we live in actually works i highly recommended this book to everyone\n",
      "summary:  the and the and and the and the the the and the the the and and this and and the and and and and the the and and  \n",
      "\n",
      "review:  i started reading this series about a year ago i was enraptured by the first three books especially a storm of swords, and i came off a bit disappointed by a feast for crows, not just because it had a slow pace, but because my favorite characters were left outbut this only got me more excited for a dance with dragons i thought it could be like the movie twins, where grrm would put all of the recessive genes into feast while putting the dominant genes into dance the book started off well, with some interesting revelations taking place early on, but it quickly returned to the pace of a feast for crowssomething that disappointed me was how few bran chapters were included i felt he had the most interesting chapters in the entire book and not to spoil anything, but it actually gets somewhere even if the rest of the book were just segments of bran training that did not go anywhere, the skinchanging segments are some of the most fascinating passages ever written that it would still be satisfyingbut no, instead of bran, we get nameless character after nameless character that does not do anything only a few of these characters i felt were absolutely necessary, and the ones that were had enough chapters to warrant getting an actual name, instead of being giving 4 or 5 names throughout the bookthe book came off as 1000 pages of buildup to the winds of winter, with events happening at the end of the book that get you really excited for what is to come, only to be cut short i honestly cannot wait for twow, but not because i was so enthralled by this book it is because this book plodded along, introduced some exciting things, but cut us shortto me, george r r martin has become like another george, the one named lucas both creating some of the best pieces of fiction ever george lucas obviously created star wars, and grrm created a song of ice and fire when lucas introduced some questionable ideas with the prequels, who at that point would dare question the man who made star wars it is the same with grrm now his series is commonly referred to as one of the greatest fantasy series of all time what editor would have the guts to tell him that his novel was sloppy, meandering, and pointless up till the last few chapters, especially when under a tight deadline the editor could force him to refine the novel, but push back the ridiculously close july 12th deadline, or the editor could put a first draft into print and make a bunch of moneyso, in conclusion, a dance with dragons was disappointing but hopefully, grrm has purged himself of all the tedious buildup and will deliver us an actionpacked, eventful winds of winter of course, i thought a feast of crows was going to be a purge, too well, i guess i will have to wait and see, and there will be a lot of waiting\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary:  and and the and the the of this and the and to and to of the and to and and the this to and the and to the  \n",
      "\n",
      "review:  this is a remarkable book that will introduce you to the process of science and a fascinating aspect of the emergence of life  trilobites are among the best fossils for children to get to know because they are very distinct the tri lobed shells and very different from anything currently living the horseshoe crab on american atlantic beaches is comparable in unique appearance and attracts children with similar fascinationfor those who want a better system of american science education, fortey gives some powerful hints  consider his language quotthe fever of discovery was upon me i found a trilobitethe textbook came alivethis was my first discovery of the animals that would change my life p18quot  he continues, i knew, by some principle which i could not articulate, that the wider end was the head of the animal  and of course upon the head there were the eyes  despite the unfamiliar conformation of the fossil i knew that eyes must always belong on heads  so despite the exoticism of the fossil there was already a common bond between me and the trilobite  we both had our heads screwed on the right wayquotp19again and again fortey reminds us that scientists grow from discovery, mystery, romance, intrigue, while the memorization comes later  he reminds us that there is an enormous amount we still do not know and in the process introduces us to a world we have never considered  quoti want to invest the trilobite with all the glamour of the dinosaur and twice its endurance  i want you to see the world through the eyes of trilobites, to help you make a journey back through hundreds of millions of yearsthis will be an unabashedly trilobitecentric view of the world,quotp19for anyone who wants to take a few hours to think different thoughts, to consider how brief our history has been and how successful other organisms have been, to contemplate the various catastrophic extinctions and their dramatic impact on life, to ask about life beyond the rush hour traffic and the monthly report, forteys work is a little gem of an introduction to a fascinating part of the world  i highly recommend it\n",
      "summary:  the and and and of and of the and the and to the and and the of and and and and and and and and the the and  \n",
      "\n",
      "review:  this was a very well written book  i also appreciated the comments made by the author on her research and the way she brought the stories she uncovered into the book\n"
     ]
    }
   ],
   "source": [
    "pred_summaries = []\n",
    "for review in test_reviews[0:10]:\n",
    "    summary, review, attention_plot = summarize(review)\n",
    "    pred_summaries.append(summary)\n",
    "    print(\"summary: \", summary, \"\\n\")\n",
    "    print(\"review: \", review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/56/90178929712ce427ebad179f8dc46c8deef4e89d4c853092bee1efd57d05/nltk-3.4.1.zip (3.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.1MB 24.9MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from nltk) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/97/8a/10/d646015f33c525688e91986c4544c68019b19a473cb33d3b55\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.4.1\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py-rouge in /usr/local/lib/python3.5/dist-packages (1.1)\n",
      "\u001b[33mYou are using pip version 19.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install py-rouge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation with Avg\n",
      "\trouge-1:\tP:  3.93\tR:  8.60\tF1:  5.30\n",
      "\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-l:\tP:  6.17\tR: 11.81\tF1:  8.00\n",
      "\trouge-w:\tP:  3.54\tR:  4.71\tF1:  3.94\n",
      "\n",
      "Evaluation with Best\n",
      "\trouge-1:\tP:  3.93\tR:  8.60\tF1:  5.30\n",
      "\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\trouge-l:\tP:  6.17\tR: 11.81\tF1:  8.00\n",
      "\trouge-w:\tP:  3.54\tR:  4.71\tF1:  3.94\n",
      "\n",
      "Evaluation with Individual\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 16.67\tF1: 10.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 20.00\tF1: 10.53\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 16.67\tF1: 10.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR:  8.33\tF1:  7.69\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-1:\tP:  3.57\tR: 10.00\tF1:  5.26\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-1:\tP:  7.14\tR: 14.29\tF1:  9.52\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-1:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-2:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-3:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-4:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 22.47\tF1: 14.85\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 26.15\tF1: 15.57\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 22.47\tF1: 14.85\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 12.61\tF1: 11.80\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-l:\tP:  6.22\tR: 14.68\tF1:  8.74\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-l:\tP: 11.09\tR: 19.76\tF1: 14.21\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-l:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\tHypothesis #0 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #1 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  9.03\tF1:  7.47\n",
      "\tHypothesis #2 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #3 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR: 11.24\tF1:  8.13\n",
      "\tHypothesis #4 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  9.03\tF1:  7.47\n",
      "\tHypothesis #5 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  3.93\tF1:  4.86\n",
      "\tHypothesis #6 & Reference #0: \n",
      "\t\trouge-w:\tP:  3.57\tR:  6.31\tF1:  4.56\n",
      "\tHypothesis #7 & Reference #0: \n",
      "\t\trouge-w:\tP:  6.36\tR:  7.51\tF1:  6.89\n",
      "\tHypothesis #8 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\tHypothesis #9 & Reference #0: \n",
      "\t\trouge-w:\tP:  0.00\tR:  0.00\tF1:  0.00\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pip install py-rouge\n",
    "import rouge\n",
    "\n",
    "def prepare_results(p, r, f):\n",
    "    return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "\n",
    "for aggregator in ['Avg', 'Best']: #, 'Individual'\n",
    "    print('Evaluation with {}'.format(aggregator))\n",
    "    apply_avg = aggregator == 'Avg'\n",
    "    apply_best = aggregator == 'Best'\n",
    "\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=4,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=apply_avg,\n",
    "                           apply_best=apply_best,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "\n",
    "    prediction = pred_summaries\n",
    "    gold = test_summaries[0:10]\n",
    "\n",
    "    scores = evaluator.get_scores(prediction, gold)\n",
    "\n",
    "    for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "#         if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "#             for hypothesis_id, results_per_ref in enumerate(results):\n",
    "#                 nb_references = len(results_per_ref['p'])\n",
    "#                 for reference_id in range(nb_references):\n",
    "#                     print('\\Pred_summary #{} & Gold_summary #{}: '.format(hypothesis_id, reference_id))\n",
    "#                     print('\\t' + prepare_results(results_per_ref['p'][reference_id], results_per_ref['r'][reference_id], results_per_ref['f'][reference_id]))\n",
    "#             print()\n",
    "        if apply_avg or apply_best:\n",
    "            print(prepare_results(results['p'], results['r'], results['f']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, text, predicted_summary):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "    \n",
    "    fontdict = {'fontsize': 14}\n",
    "    \n",
    "    ax.set_xticklabels([''] + text, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_summary, fontdict=fontdict)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_result(text, encoder, decoder, tokenizer, max_encoder_seq_length, max_decoder_seq_length):\n",
    "def print_result(tex):\n",
    "    result, text, attention_plot = summarize(text, encoder, decoder, tokenizer, max_encoder_seq_length, max_decoder_seq_length)\n",
    "        \n",
    "    print('Input: {}'.format(text))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(text.split(' '))]\n",
    "    plot_attention(attention_plot, text.split(' '), result.split(' '))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f5dc88ecef0>"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "aaaa = '''\n",
    "we have many of the old, old issue but the number had depleted there were not  enough books to allow us to use them regularly with the additional supply the books will be used more often  they arre a good old standby for gospel singing\n",
    "'''\n",
    "print_result(aaaa)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(history.history['loss'])\n",
    "# plt.plot(history.history['val_loss'])\n",
    "# plt.legend(['train','validation'])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss = model.history['loss']\n",
    "# epoch = [i for i in range(epochs)]\n",
    "\n",
    "# plt.plot(epoch, loss) #, label=str(batch_size)\n",
    "    \n",
    "# plt.legend()\n",
    "# # plt.title('different batch size');\n",
    "# plt.xlabel('epoch'); \n",
    "# plt.ylabel('loss')\n",
    "# plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_6B_100d_file_path_name = \"../glove.6B/glove.6B.100d.txt\"\n",
    "\n",
    "# embeddings_index = dict()\n",
    "\n",
    "# f = open(glove_6B_100d_file_path_name)\n",
    "\n",
    "# for line in f:\n",
    "#     values = line.split()\n",
    "#     word = values[0]\n",
    "#     coefs = np.asarray(values[1:], dtype='float32')\n",
    "#     embeddings_index[word] = coefs\n",
    "    \n",
    "# f.close()\n",
    "# print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "# # Tokenize\n",
    "# vocabulary_size = len(all_glove_words)\n",
    "# tokenizer = Tokenizer() #num_words= vocabulary_size\n",
    "# tokenizer.fit_on_texts(all_glove_words) \n",
    "\n",
    "# # create a weight matrix for words in training docs\n",
    "# embedding_matrix = np.zeros((vocabulary_size, 100)) \n",
    "# for word, index in tokenizer.word_index.items():\n",
    "#     if index > vocabulary_size - 1:\n",
    "#         break\n",
    "#     else:\n",
    "#         embedding_vector = embeddings_index.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(data_path + \"\\model-40.hdf5\")\n",
    "# dummy_iters = 40\n",
    "# example_training_generator = KerasBatchGenerator(train_data, num_steps, 1, vocabulary,\n",
    "#                                                      skip_step=1)\n",
    "# print(\"Training data:\")\n",
    "# for i in range(dummy_iters):\n",
    "#     dummy = next(example_training_generator.generate())\n",
    "# num_predict = 10\n",
    "# true_print_out = \"Actual words: \"\n",
    "# pred_print_out = \"Predicted words: \"\n",
    "# for i in range(num_predict):\n",
    "#     data = next(example_training_generator.generate())\n",
    "#     prediction = model.predict(data[0])\n",
    "#     predict_word = np.argmax(prediction[:, num_steps-1, :])\n",
    "#     true_print_out += reversed_dictionary[train_data[num_steps + dummy_iters + i]] + \" \"\n",
    "#     pred_print_out += reversed_dictionary[predict_word] + \" \"\n",
    "# print(true_print_out)\n",
    "# print(pred_print_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def test_summary_generation(reviews):\n",
    " \n",
    "#      # clean inputs\n",
    "#     cleaned = cleaning_data(reviews) \n",
    "#     # tokenize\n",
    "#     tokenized = tokenizer.texts_to_sequences([cleaned]) \n",
    "#     # padding\n",
    "#     sequence = pad_sequences(tokenized, maxlen = maxlen)  \n",
    "   \n",
    "#     # encode\n",
    "#     state = encoder_model.predict(sequence)\n",
    "\n",
    "\n",
    "#     # collect predictions\n",
    "#     output = list()\n",
    "#     for t in [answer_word2index['_B_'], answer_word2index['_U_']]:\n",
    "#         # predict next sequence\n",
    "#         target_seq = np.eye(n_class)[[t]]\n",
    "#         target_seq = target_seq[newaxis,:, : ]\n",
    "#         yhat, h, c = decoder_model.predict([target_seq] + state)\n",
    "#         # save first prediction\n",
    "#         output.append(yhat[0,0,:])\n",
    "#         # update state\n",
    "#         state = [h, c]\n",
    "#         # update target sequence\n",
    "#         target_seq = yhat\n",
    "    \n",
    "#     # select max probability words and decode\n",
    "#     output_sequence = [np.argmax(vector) for vector in np.array(output)]\n",
    "#     decoded = [answer_index2word[i] for i in output_sequence]\n",
    "\n",
    "#     # Remove anything after '_E_'        \n",
    "#     if \"_E_\" in decoded:\n",
    "#         end = decoded.index('_E_')\n",
    "#         answer = ' '.join(decoded[:end])\n",
    "#     else :\n",
    "#         answer = ' '.join(decoded[:])    \n",
    "#     # if no answer return choose random answer    \n",
    "#     if answer:\n",
    "#         result = answer\n",
    "#     else: \n",
    "#         result = np.random.random_integers(100)\n",
    "#     return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
